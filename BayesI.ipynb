{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian inference is the process of fitting a probability model to a set of data and summarizing the result by a probability distribution on the parameters of the model. \n",
    "\n",
    "\\section{Probability and inference}\n",
    "Bayesian methods: explicit use of probability\n",
    "\n",
    "Bayesian data analysis three steps\n",
    "\\begin{enumerate}\n",
    "\\item Full probability model: joint probability for all observable and unobservable quantities\n",
    "\\item Calculate posterior distribution\n",
    "\\item Evaluate fit of the model\n",
    "\\end{enumerate}\n",
    "\n",
    "Second step aided by computational methodology\n",
    "\n",
    "Pragmatic advantages of the Bayesian framework, flexibility and generality\n",
    "\n",
    "\\subsection{General notation for statistical inference}\n",
    "Statistical inference: drawing conclusions from numerical data about quantities that are not observed. \n",
    "\n",
    "Population, sample\n",
    "\n",
    "Two kinds of estimands: unobserved and observed\n",
    "\n",
    "$\\theta$ unobservable vector quantities or population parameters, $y$ observed data and $\\tilde{y}$ unknown but potentially observable quantities. In general multivariate quantities\n",
    "\n",
    "Vectors are columns: $u^T u$ and $u u^T$\n",
    "\n",
    "Data set: $y = (y_1, ..., y_n)$. Exchangeable! We model data from an exchangeable distribution as independent and identically distributed \n",
    "\n",
    "Explanatory variables or covariates $x$: $n$ units, $k$ explanatory variabels, $X (n \\times k)$. \n",
    "\n",
    "Hierarchical modeling: also called multilevel models, information is available on several different levels of observational units. For example, suppose two medical treatments are applied, in separate randomized experiments, to patients in several different cities. Then, if no other information were available, it would be reasonable to treat the patients within each city as exchangeable and also treat the results from different cities as themselves exchangeable. In practice it would make sense to include, as explanatory variables at the city level, whatever relevant information we have on each city, as well as the explanatory variables mentioned before at the individual level, and then the conditional distributions given these explanatory variables would be exchangeable.\n",
    "\n",
    "\\subsection{Bayesian inference}\n",
    "Bayesian statistical conclusions about a parameter $\\theta$ or unobserved data $\\tilde{y}$ are made in terms of probability statements: $p(\\theta|y)$ and $p(\\tilde{y} | y)$\n",
    "\n",
    "Different notations: \n",
    "$\\theta \\sim N(\\mu, \\sigma^2)$, $p(\\theta) = N(\\theta | \\mu, \\sigma^2)$\n",
    "\n",
    "\\subsubsection{Bayes' rule}:\n",
    "We begin by providing a joint probability distribution for $\\theta$ and $y$. The joint probability density function is the product of the sampling distribution and the prior distribution ($p(\\theta, y) = p(\\theta)p(y | \\theta)$). Then the posterior distribution is:\n",
    "\\[\n",
    "p(\\theta | y) = \\frac{p(\\theta, y)}{p(y)} = \\frac{p(\\theta)p(y | \\theta)}{p(y)}\n",
    "\\]\n",
    "where $p(y) = \\sum_{\\theta}p(\\theta)p(y |\\theta)$ or $p(y) = \\int p(\\theta)p(y |\\theta) d\\theta$\n",
    "\n",
    "Unnormalized posterior density:\n",
    "\\[\n",
    "p(\\theta | y) \\propto p(\\theta)p(y|\\theta)\n",
    "\\]\n",
    "\\subsubsection{Prediction}\n",
    "Before the data $y$ are considered, the distribution of the unknown but observable $y$ is\n",
    "\\[\n",
    "p(y) = \\int p(y, \\theta) d\\theta = \\int p(\\theta) p(y | \\theta) d \\theta\n",
    "\\]\n",
    "This ($p(y)$) is called the marginal distribution of $y$ or prior predictive distribution. \n",
    "\n",
    "After the data $y$ has been observed,  we can predict an unknown observable, $\\tilde{y}$, from the same process.\n",
    "\n",
    "Posterior predictive distribution: the distribution of $\\tilde{y}$\n",
    "\\[\n",
    "p(\\tilde{y} | y) = \\int p(\\tilde{y} | \\theta) p(\\theta | y) d\\theta\n",
    "\\]\n",
    "\\subsubsection{Likelihood}\n",
    "The data $y$ affect the posterior inference only through $p(y | \\theta)$, which is called the likelihood function. \n",
    "\n",
    "\\subsubsection{Likelihood and odds ratios}\n",
    "The ratio of the posterior density $p(\\theta |y)$ evaluated at the points $\\theta_1$ and $\\theta_2$ under a given model is called the posterior odds for $\\theta_1$ compared to $\\theta_2$. \n",
    "\\[\n",
    "\\frac{p(\\theta_1|y)}{p(\\theta_2|y)}\n",
    "\\]\n",
    "\\subsection{Discrete example}\n",
    "The woman is either a carrier of the gene $(\\theta = 1)$ or not $(\\theta = 0)$. The prior distribution for the unknown $\\theta$ can be expressed simply as $\\textrm{Pr}(\\theta = 1) = \\textrm{Pr}(\\theta = 0) = 1/2$. \n",
    "\n",
    "The woman has two sons, neither of whom is affected. We generate the likelihood functions:\n",
    "\\[\n",
    "\\textrm{Pr}(y_1 = 0, y_2 = 0 | \\theta = 1) = 0.5 \\cdot 0.5 = 0.25\n",
    "\\]\n",
    "\\[\n",
    "\\textrm{Pr}(y_1 = 0, y_2 = 0 | \\theta = 0) = 1 \\cdot 1 = 1\n",
    "\\]\n",
    "The posterior distribution:\n",
    "\\[\n",
    "\\textrm{Pr}(\\theta = 1|y) = \\frac{p(y|\\theta = 1)\\textrm{Pr}(\\theta = 1)}{p(y|\\theta = 1)\\textrm{Pr}(\\theta = 1) + p(y | \\theta = 0)\\textrm{Pr}(\\theta = 0)}\n",
    "\\]\n",
    "\\subsection{Probability as a measure of uncertainty}\n",
    "Within the Bayesian paradigm, it is equally legitimate to discuss the probability of ‘rain tomorrow’ or of a Brazilian victory in the soccer World Cup as it is to discuss the probability that a coin toss will land heads.\n",
    " \n",
    "Two justifications for numerical measure of uncertainty: symmetry or exchangeability argument, frequency argument. \n",
    " \n",
    " \\subsection{Some useful results from probability theory}\n",
    "Probability background: manipulation of joint densities, the definition of simple moments, the transformation of variables, and methods of simulation\n",
    "\n",
    "We generally represent joint distributions by their joint probability density functions. It is also often useful to factor a joint density as a product of marginal and conditional densities; for example, \n",
    "\\[\n",
    " p(u, v, w) = p(u|v, w) p(v|w)p(w)\n",
    " \\]\n",
    "\n",
    "The expextation and variance are defined as follows:\n",
    "\n",
    "\\[\n",
    "E(u) = \\int up(u)du\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "var(u) = \\int (u-E(u))^2p(u)du\n",
    "\\]\n",
    "If the parameter is a vector, the covariance matrix is defined as \n",
    "\\[\n",
    "var(u) = \\int (u-E(u))(u-E(u))^Tp(u)du\n",
    "\\]\n",
    "where $u$ is considered a column vector. \n",
    "\n",
    "\\subsubsection{Means and variances of conditional distributions}\n",
    "It is often useful to express the mean and variance of a random variable $u$ in terms of the conditional mean and variance given some related quantity $v$. The mean of $u$ can be obtained by averaging the conditional mean over the marginal distribution of $v$, \n",
    "\\[\n",
    "E(u) = E(E(u|v))\n",
    "\\]\n",
    "where the inner expectation averages over $u$, conditional on $v$, and the outer expectation averages over $v$. \n",
    "\\[\n",
    "E(u) = \\int \\int up(u, v)dudv = \\int \\int u p(u | v) du p(v) dv = \\int E(u|v) p(v) dv\n",
    "\\]The corresponding result for the variance includes two terms, the mean of the conditional variance and the variance of the conditional mean:\n",
    "\\[\n",
    "var(u) = E(var(u|v)) + var(E(u|v))\n",
    "\\]\\subsubsection{Transformation of variables}\n",
    "Suppose $p_u(u)$ is the density of the vector $u$, and we transform to $v=f(u)$, where $v$ has the same number of components as $u$. \n",
    "\n",
    "If $p_u$ is a discrete distribution, and $f$ is a one-to-one function, then the density of $v$ is given by\n",
    "\\[\n",
    "p_v(v) = p_u(f^{-1}(v))\n",
    "\\]\n",
    "If $p_u$ is a continuous distribution, and $v=f(u)$ is a one-to-one transformation, then the joint density of the transformed vector is \n",
    "\\[\n",
    "p_v(v) = |J|p_u(f^{-1}(v))\n",
    "\\]\\subsection{Bayesian inference in applied statistics}\n",
    "Rationale for the use of Bayesian methods: incorporation of multiple levels of randomness and the resultant ability to combine information from different sources, while incorporating all reasonable sources of uncertainty in inferential summaries. \n",
    "\n",
    "\\subsection{Code}\n",
    "\\subsubsection{Sample prior distribution and visualize density plot}\n",
    "\\begin{minted}[breaklines]{R}\n",
    "# Sample 10000 draws from Beta(45,55) prior\n",
    "prior_A <- rbeta(n = 10000, shape1 = 45, shape2 = 55)\n",
    "\n",
    "# Store the results in a data frame\n",
    "prior_sim <- data.frame(prior_A)\n",
    "\n",
    "# Construct a density plot of the prior sample\n",
    "ggplot(prior_sim, aes(x = prior_A)) + \n",
    "    geom_density()\n",
    "\\end{minted}\n",
    "\n",
    "\\subsubsection{Sample three different prior distributions and visualize as separate  density functions in the same plot}\n",
    "\\begin{minted}[breaklines]{R}\n",
    "# Sample 10000 draws from the Beta(1,1) prior\n",
    "prior_B <- rbeta(n = 10000, shape1 = 1, shape2 = 1)    \n",
    "\n",
    "# Sample 10000 draws from the Beta(100,100) prior\n",
    "prior_C <- rbeta(n = 10000, shape1 = 100, shape2 = 100)\n",
    "\n",
    "# Combine the results in a single data frame\n",
    "prior_sim <- data.frame(samples = c(prior_A, prior_B, prior_C),\n",
    "        priors = rep(c(\"A\",\"B\",\"C\"), each = 10000))\n",
    "\n",
    "# Plot the 3 priors\n",
    "ggplot(prior_sim, aes(x = samples, fill = priors)) + \n",
    "    geom_density(alpha = 0.5)\n",
    "\\end{minted}\n",
    "\n",
    "\\subsubsection{Likelihood simulation}\n",
    "Here we define a discrete array of parameter values and simulate a sample from the likelihood function using each parameter value. \n",
    "\n",
    "\\begin{minted}{R}\n",
    "# Define a vector of 1000 p values    \n",
    "p_grid <- seq(from = 0, to = 1, length.out = 1000)\n",
    "\n",
    "# Simulate 1 poll result for each p in p_grid   \n",
    "poll_result <- rbinom(n=1000, size=10, prob=p_grid)\n",
    "poll_result\n",
    "# Create likelihood_sim data frame\n",
    "likelihood_sim <- data.frame(p_grid, poll_result)    \n",
    "\n",
    "# Density plots of p_grid grouped by poll_result\n",
    "ggplot(likelihood_sim, aes(x = p_grid, y = poll_result, group = poll_result)) + \n",
    "    geom_density_ridges()\n",
    "\\end{minted}\n",
    "\n",
    "\\subsubsection{Approximating likelihood}\n",
    " \\begin{minted}{R}\n",
    " # Density plots of p_grid grouped by poll_result\n",
    "ggplot(likelihood_sim, aes(x = p_grid, y = poll_result, group = poll_result, fill = poll_result == 6)) + \n",
    "    geom_density_ridges()\n",
    " \\end{minted}\n",
    "\n",
    "\\section{Single-parameter models}\n",
    "Statistical models where only a single scalar parameter is to be estimated. \n",
    "\n",
    "\\subsection{Estimating a probability from binomial data}\n",
    "Results of Bernoulli trials: that is, data $y_1, \\dots, y_2$, each of which is either 0 or 1. \n",
    "\n",
    "The binomial distribution provides a natural model for data that arise from a sequence of $n$ exchangeable trials or draws from a large population where each trial gives rise to one of two possible outcomes, conventionally labeled \"success\" and \"failure\". Total number of successes $y$. The binomial sampling model is\n",
    "\\[\n",
    "p(y|\\theta) = \\textrm{Bin}(y|n, \\theta) = {n \\choose y} \\theta^y (1-\\theta)^{n-y}\n",
    "\\]To perform Bayesian inference in the binomial model, we must specify a prior distribution for $\\theta$. For simplicity, we assume that the prior distribution for $\\theta$ is uniform on the interval $[0,1]$. \n",
    "\n",
    "Application of Bayes' rule gives the posterior density for $\\theta$ as\n",
    "\\[\n",
    "p(\\theta | y) \\propto \\theta^y(1-\\theta)^{n-y}\n",
    "\\]\n",
    "The factor ${n \\choose y}$ can be treated as a constant and left out. We can recognize the posterior as the unnormalized form of the beta distribution\n",
    "\\[\n",
    "\\theta | y \\sim \\textrm{Beta}(y + 1, n - y + 1)\n",
    "\\]\\subsection{Posterior as compromise between data and prior information}\n",
    "We might expect that, because the posterior distribution incorporates the information from the data, it will be less variable than the prior distribution. The variance formula is more interesting because it says that the posterior variance is on average smaller than the prior variance, by an amount that depends on the variation in posterior means over the distribution of possible data. \n",
    "\n",
    "\\subsection{Summarizing posterior inference}\n",
    "Ideally one might report the entire posterior distribution $p(\\theta|y)$. For multiparameter problems we use contour plots and scatterplots. \n",
    "\n",
    "Mean, median, mode, standard deviation, the interquartile range, and other quantiles. \n",
    "\n",
    "\\subsection{Posterior quantiles and intervals}\n",
    "Posterior intervals: the range of values above and below which lies exactly $100(\\alpha/2)\\%$ of the posterior probability.   \n",
    "\n",
    "\\subsection{Informative prior distribution}\n",
    "In the binomial example, we have so far considered only the uniform prior distribution for $\\theta$. \n",
    "\n",
    "We consider two basic interpreations that can be given to prior distributions. In the population interpreation, the prior distribution represents a population of possible parameter values, from which the $\\theta$ of current interest has been drawn. In the more subjective state of knowledge interpretation, we express our knowledge and uncertainty about $\\theta$. \n",
    "\n",
    "\\subsubsection{Binomial example with different prior distributions}\n",
    "If the prior density is of the same form, with its own values $a$ and $b$, then the posterior density will also be of this form. We will parameterize such a prior density as \n",
    "\\[\n",
    "p(\\theta) \\propto \\theta^{\\alpha - 1}(1 - \\theta)^{\\beta - 1}\n",
    "\\]\n",
    "Hyperparameters: the parameters of the prior distribution\n",
    "\n",
    "The posterior density for $\\theta$ is \n",
    "\\[\n",
    "p(\\theta | y) \\propto \\theta^y (1 - \\theta)^{n-y}\\theta^{\\alpha - 1}(1 - \\theta)^{\\beta - 1}\n",
    "\\]\n",
    "\\[\n",
    "= \\theta^{y + \\alpha - 1}(1 - \\theta)^{n - y + \\beta -1}\n",
    "\\]\n",
    "Conjugacy: the posterior distribution follows the same parametric form as the prior distribution\n",
    "\n",
    "In the limit, the parameters of the prior distribution have no influence on the posterior distribution. The central limit theorem of probability theory can be put in a Bayesian context to show:\n",
    "\\[\n",
    "\\Bigg( \\frac{\\theta - E(\\theta | y)}{\\sqrt{\\textrm{var}(\\theta | y)}} \\Bigg| y \\Bigg) \\to N(0,1)\n",
    "\\]\n",
    "This result is often used to justify approximating the posterior distribution with a normal distribution. \n",
    "\n",
    "\\subsubsection{Conjugate prior distributions}\n",
    "Formal definition of conjugacy: if $F$ is a class of sampling distributions $p(y|\\theta)$, and $P$ is a class of prior distributions for $\\theta$, then the class $P$ is conjugate for $F$ if\n",
    "\\[\n",
    "p(\\theta | y) \\in P \\textrm{ for all } p(\\cdot|\\theta) \\in F \\textrm{ and } p(\\cdot) \\in P\n",
    "\\]\n",
    "\\subsubsection{Nonconjugate prior distributions}\n",
    "The basic justification for the use of conjugate prior distributions is similar to that for using standard models: it is easy to understand the results. \n",
    "\n",
    "\\subsubsection{Conjugate prior distributions, exponential families and sufficient statistics}\n",
    "Probability distributions that belong to an exponential family have natural conjugate prior distributions. \n",
    "\n",
    "The class $F$ is an exponential family if all its members have the form\n",
    "\\[\n",
    "p(y_i | \\theta) = f(y_i)g(\\theta)e^{\\phi(\\theta)^Tu(y_i)}\n",
    "\\]\\subsection{Normal distribution with known variance}\n",
    "The normal distribution is fundamental to most statistical modelling. We derive results first for a single data point and then for the general case of many data points.\n",
    "\n",
    "\\subsubsection{Likelihood of one data point}\n",
    "The sampling distribution is\n",
    "\\[\n",
    "p(y|\\theta) = \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2\\sigma^2}(y-\\theta)^2}\n",
    "\\]\n",
    "\\subsubsection{Conjugate prior and posterior distributions}\n",
    "Considered as a function of $\\theta$, the likelihood is an exponential of a quadratic form in $\\theta$, so the family of conjugate prior densities looks like\n",
    "\\[\n",
    "p(\\theta) = e^{A\\theta^2 + B\\theta + C}\n",
    "\\]\n",
    "We parameterize this family as \n",
    "\\[\n",
    "p(\\theta) \\propto \\exp\\Bigg( -\\frac{1}{2\\tau_0^2}(\\theta - \\mu_0)^2\\Bigg)\n",
    "\\]that is $\\theta \\sim N(\\mu_0, \\tau_0^2)$ with hyperparameters $\\mu_0$ and $\\tau_0^2$. \n",
    "\n",
    "The posterior is then:\n",
    "\\[\n",
    "p(\\theta | y) \\propto \\exp\\Bigg( - \\frac{1}{2}\\Bigg(\\frac{(y - \\theta)^2}{\\sigma^2} + \\frac{(\\theta - \\mu_0)^2}{\\tau_0^2}\\Bigg) \\Bigg)\n",
    "\\]which can be simplified into \n",
    "\\[\n",
    "p(\\theta | y) \\propto \\exp\\Bigg( -\\frac{1}{2\\tau_1^2}(\\theta - \\mu_1)^2\\Bigg)\n",
    "\\]that is, $\\theta | y \\sim N(\\mu_1, \\tau_1^2)$, where \n",
    "\\[\n",
    "\\mu_1 = \\frac{\\frac{1}{\\tau_0^2}\\mu_0 + \\frac{1}{\\sigma^2}y}{\\frac{1}{\\tau_0^2} + \\frac{1}{\\sigma^2}} \\textrm{ and } \\frac{1}{\\tau_1^2} = \\frac{1}{\\tau_0^2} + \\frac{1}{\\sigma^2}\n",
    "\\]\\subsubsection{Precision of the prior and posterior distributions}\n",
    "Precision: the inverse of the variance\n",
    "\n",
    "The posterior precision equals the prior precision plus the data precision\n",
    "\n",
    "There are several different ways of interpreting the form of the posterior mean, $\\mu_1$. The posterior mean is expresed as a weighted average of the prior mean and the observed value $y$, with weights proportional to the precisions. \n",
    "\n",
    "\\subsubsection{Posterior predictive distribution}\n",
    "The posterior predictive distribution of a future observation, $\\tilde{y}, p(\\tilde{y}|y)$, can be calculated directly by integration\n",
    "\\[\n",
    "p(\\tilde{y}|y) = \\int p(\\tilde{y}|\\theta) p(\\theta |y) d \\theta\n",
    "\\]\n",
    "\\[\n",
    "\\propto \\int \\exp \\Bigg( -\\frac{1}{2\\sigma^2} (\\tilde{y} - \\theta)^2 \\Bigg) \\exp \\Bigg(-\\frac{1}{2\\tau_1^2}(\\theta - \\mu_1)^2 \\Bigg) d\\theta\n",
    "\\]\\subsubsection{Normal model with multiple observations}\n",
    "The normal model with a single observation can be easily extended to the more realistic situation where a sample of independent and identically distributed observations $y = (y_1, \\dots, y_n)$ is available. The posterior density is then\n",
    "\\[\n",
    "p(\\theta | y) \\propto p(\\theta) p(y | \\theta)\n",
    "\\]\n",
    "\\[\n",
    "= p(\\theta) \\prod_{i=1}^n p(y_i | \\theta) \n",
    "\\]\n",
    "\\[\n",
    "\\propto \\exp\\Bigg(-\\frac{1}{2\\tau_0^2} (\\theta - \\mu_0)^2 \\Bigg) \\prod_{i=1}^n \\exp \\Bigg(- \\frac{1}{2\\sigma^2}(y_i - \\theta)^2 \\Bigg)\n",
    "\\]\n",
    "\\[\n",
    "\\exp \\Bigg( -\\frac{1}{2} \\Bigg(\\frac{1}{\\tau_0^2}(\\theta - \\mu_0)^2 + \\frac{1}{\\sigma^2} \\sum_{i=1}^n (y_i - \\theta)^2 \\Bigg) \\Bigg)\n",
    "\\]Simplification of the expression reveals that the posterior distribution depends on $y$ only through the sample mean, i.e. $\\overline{y}$ is a sufficient statistic in this model. Furthermore, we get that\n",
    "\\[\n",
    "p(\\theta | y_1, \\dots, y_n) = p(\\theta | \\overline{y}) = N(\\theta | \\mu_n, \\tau_n^2)\n",
    "\\]\n",
    "where \n",
    "\\[\n",
    "\\mu_1 = \\frac{\\frac{1}{\\tau_0^2}\\mu_0 + \\frac{n}{\\sigma^2}\\overline{y}}{\\frac{1}{\\tau_0^2} + \\frac{n}{\\sigma^2}} \\textrm{ and } \\frac{1}{\\tau_n^2} = \\frac{1}{\\tau_0^2} + \\frac{n}{\\sigma^2}\n",
    "\\]\n",
    "\\subsection{Other standard single-parameter models}\n",
    "In general, the posterior density has no closed-form expression; the normalizing constant is often especially difficult to compute due to the integral. Much formal Bayesian analysis concentrates on situations where closed forms are available; such models are sometimes unrealistic, but they provide a useful starting point. \n",
    "\n",
    "Standard distributions: binomial, normal, Poisson, exponential\n",
    "\n",
    "Each of these standard models has an associated family of conjugate prior distributions, which we discuss in turn.\n",
    "\n",
    "\\subsubsection{Normal distribution with known mean but unknown variance}\n",
    "Useful building block for more complicated models. The likelihood for a vector $y$ of $n$ independent and identically distributed observations is\n",
    "\\[\n",
    "p(y | \\sigma^2) \\propto \\sigma^{-n} \\exp \\Bigg( - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - \\theta)^2 \\Bigg)\n",
    "\\]\n",
    "\\[\n",
    "= (\\sigma^2)^{-n/2} \\exp\\Bigg( -\\frac{n}{2\\sigma^2}v \\Bigg)\n",
    "\\]\n",
    "The sufficient statistic is \n",
    "\\[\n",
    "v = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\theta)^2\n",
    "\\]The corresponding conjugate prior density is the inverse-gamma\n",
    "\\[\n",
    "p(\\sigma^2) \\propto (\\sigma^2)^{-(\\alpha + 1)}e^{-\\beta/\\sigma^2}\n",
    "\\]\n",
    "which has hyperparameters $(\\alpha, \\beta)$. \n",
    "\n",
    "The resulting posterior density for $\\sigma^2$ is\n",
    "\\[\n",
    "p(\\sigma^2 | y) \\propto p(\\sigma^2)p(y | \\sigma^2)\n",
    "\\]\n",
    "\\[\n",
    "\\propto \\Bigg( \\frac{\\sigma_0^2}{\\sigma^2} \\Bigg)^{v_0/2+1} \\exp \\Bigg( -\\frac{v_0\\sigma_0^2}{2\\sigma^2} \\Bigg) (\\sigma^2)^{-n/2} \\exp \\Bigg( - \\frac{n}{2}\\frac{v}{\\sigma^2} \\Bigg)\n",
    "\\]\\[\n",
    "\\propto (\\sigma^2)^{-((n+v_0)/2+1)} \\exp \\Bigg( -\\frac{1}{2\\sigma^2} (v_0 \\sigma_0^2 + nv) \\Bigg)\n",
    "\\]Thus \n",
    "\\[\n",
    "\\sigma^2 | y \\sim \\textrm{Inv-}\\chi^2 \\Bigg(v_0 + n, \\frac{v_0\\sigma_0^2 + nv}{v_0 + n} \\Bigg)\n",
    "\\]\\subsubsection{Poisson model}\n",
    "The Poisson distribution arises naturally in the study of data taking the form of counts:\n",
    "\n",
    "If a data point $y$ follows the Poisson distribution with rate $\\theta$, then the probability distribution of a single observation $y$ is \n",
    "\\[\n",
    "p(y|\\theta) = \\frac{\\theta^y e^{-\\theta}}{y!}\\textrm{ for } y = 0, 1, 2, \\dots\n",
    "\\]\n",
    "and for a vector $y = (y_1, \\dots, y_n)$ of independent and identically distributed observations, the likelihood is \n",
    "\\[\n",
    "p(y|\\theta) = \\prod_{i=1}^n \\frac{1}{y_i!}\\theta^{y_i}e^{-\\theta}\n",
    "\\]\n",
    "\\[\n",
    "\\propto \\theta^{t(y)}e^{-n\\theta}\n",
    "\\]\n",
    "where $t(y) = \\sum_{i=1}^n$ is the sufficient statistic. We can rewrite the likelihood in exponential family form as \n",
    "\\[\n",
    "p(y|\\theta) \\propto e^{-n\\theta}e^{t(y)\\log\\theta}\n",
    "\\]\n",
    "Then the natural conjugate prior distribution  is\n",
    "\\[\n",
    "p(\\theta) \\propto (e^{-\\theta})^{\\eta}e^{v\\log{\\theta}}\n",
    "\\]\n",
    "With the conjugate prior distribution, the posterior distribution is\n",
    "\\[\n",
    "\\theta | y \\sim \\textrm{Gamma}(\\alpha + n\\overline{y}, \\beta + n)\n",
    "\\]\n",
    "\\subsubsection{The negative binomial distribution}\n",
    "The binomial distribution is a mixture of Poisson distributions with rates, $\\theta$, that follow the gamma distribution:\n",
    "\\[\n",
    "\\textrm{Neg-bin}(y|\\alpha, \\beta) = \\int \\textrm{Poisson}(y|\\theta) \\textrm{Gamma}(\\theta|\\alpha, \\beta) d\\theta\n",
    "\\]\n",
    "\\subsubsection{Exponential model}\n",
    "The sampling distribution of an outcome $y$, given parameter $\\theta$, is\n",
    "\\[\n",
    "p(y|\\theta) = \\theta \\exp(-y\\theta)\\textrm{ for }y > 0\n",
    "\\]\n",
    "The sampling distribution of $n$ independent exponential observations $y=(y_1, \\dots, y_n)$ with constant rate $\\theta$ is\n",
    "\\[\n",
    "p(y|\\theta) = \\theta^n \\exp(-n\\overline{y}\\theta)\n",
    "\\]\n",
    "\\subsection{Non-informative prior distributions}\n",
    "When prior distributions have no population basis, they can be difficult to construct, and there has long been a desire for prior distributions that can be guaranteed to play a minimal role in the posterior distribution. \n",
    "\n",
    "\\subsubsection{Proper and improper prior distributions}\n",
    "Proper prior: the prior density $p(\\theta)$ does not depend on data and integrates to 1. \n",
    "\n",
    "Posterior distributions obtained from improper prior distributions must be interpreted with great care - one must always check that the posterior distribution has a finite integral and a sensible form\n",
    "\n",
    "\\subsubsection{Jeffreys' invariance principle}\n",
    "Defining noninformative prior distributions, based transformations of the parameter $\\phi = h(\\theta)$. Jeffreys' general principe is that any rule for determining the prior density $p(\\theta)$ should yield an equivalent result if applied to the transformed parameter. \n",
    "\n",
    "Jeffreys' principle leads to defining the noninformative prior density as $p(\\theta) \\propto [J(\\theta)]^{1/2}$, where $J(\\theta)$ is the Fisher information for $\\theta$: \n",
    "\\[\n",
    "J(\\theta) = E\\Bigg( \\Bigg( \\frac{d\\log p(y|\\theta)}{d\\theta} \\Bigg)^2 \\Bigg| \\theta \\Bigg) = -E\\Bigg(\\frac{d^2 \\log p(y|\\theta)}{d\\theta^2} \\Bigg| \\theta \\Bigg)\n",
    "\\]\n",
    "\\subsubsection{Pivotal quantities}\n",
    "If the density $y$ is such that $p(y-\\theta | \\theta)$ is a function that is free of $\\theta$ and $y$, say, $f(u)$, where $u = y - \\theta$, then $y - \\theta$ is a pivotal quantity, and $\\theta$ is called a pure location parameter. \n",
    "\n",
    "If the density of $y$ is such that $p(\\frac{y}{\\theta} | \\theta)$ is a function that is free of $\\theta$ and $y$ - say, $g(u)$, where $u = \\frac{y}{\\theta}$, then $u = \\frac{y}{\\theta}$ is a pivotal quantity and $\\theta$ is called a pure scale parameter. \n",
    "\n",
    "\\subsubsection{Difficulties with noninformative prior distributions}\n",
    "The search for noninformative priors has several problems, including:\n",
    "\n",
    "1. Establishing a particular specification as the reference prior distribution seems to encourage its automatic, and possibly inappropriate, use. \n",
    "\n",
    "2. For many problems, there is no clear choice for a vague prior distribution, since a density that is flat or uniform in one parameterization will not be in another. \n",
    "\n",
    "3. Further difficulties arise when averaging over a set of computing models that have improper prior distributions\n",
    "\n",
    "\\subsection{Weakly informative prior distributions}\n",
    "Weakly informative prior: proper prior but it is set up so that the information it does provide is intentionally weaker than whatever actual prior knowledge is available. \n",
    "\n",
    "Going at the problem from two different directions:\n",
    "Start with some version of a noninformative prior distribution and then add enough information so that inferences are constrained to be reasonable\n",
    "\n",
    "Start with a strong, highly informative prior and broaden it to account for uncertainty in one's prior beliefs and in the applicability of any historically based prior distribution to new data. \n",
    "\n",
    "\\section{Multiparameter models}\n",
    "Virtually every practical problem in statistics involves more than one unknown or unobservable quantity. \n",
    "\n",
    "Conclusions will often be drawn about one, or only a few, parameters at a time. In this case, the ultimate aim of a Bayesian analysis is to obtain the marginal posterior distribution of the parameters of interest. We first require the joint posterior distribution of all unknowns, and then we integrate the distribution over the unknowns that are not of immediate interest to obtain the desired marginal distribution. \n",
    "\n",
    "Nuisance parameters: no interest in making inferences\n",
    "\n",
    "\\subsection{Averaging over 'nuisance parameters'}\n",
    "To express the ideas of joint and marginal posterior distributions mathematically, suppose $\\theta$ has two parts, each of which can be a vector, $\\theta = (\\theta_1, \\theta_2)$, and further suppose that we are only interested in inference for $\\theta_1$, so $\\theta_2$ may be considered a 'nuisance' parameter. For example\n",
    "\\[\n",
    "y | \\mu, \\sigma^2 \\sim N(\\mu, \\sigma^2) \n",
    "\\]\n",
    "We seek the conditional distribution of the parameter of interest given the observed data; in this case $p(\\theta_1|y)$. This is derived from the joint posterior density\n",
    "\\[\n",
    "p(\\theta_1, \\theta_2|y) \\propto p(y|\\theta_1, \\theta_2)p(\\theta_1, \\theta_2)\n",
    "\\]\n",
    "by averaging over $\\theta_2$: \n",
    "\\[\n",
    "p(\\theta_1 | y) = \\int p(\\theta_1, \\theta_2 |y) d\\theta_2\n",
    "\\]\n",
    "which can be factored to yield\n",
    "\\[\n",
    "p(\\theta_1 | y) = \\int p(\\theta_1|\\theta_2, y) p(\\theta_2|y) d\\theta_2\n",
    "\\]\n",
    "Posterior distributions can be computed by marginal and conditional simulation, first drawing $\\theta_2$ from its marginal posterior distribution and then $\\theta_1$ from its conditional posterior distribution, given the drawn value $\\theta_2$. \n",
    "\n",
    "\\subsection{Normal data with a noninformative prior distribution}\n",
    "A vector $y$ of $n$ independent observations from a univariate normal distribution, $N(\\mu, \\sigma^2)$\n",
    "\n",
    "\\subsubsection{A noninformative prior distribution}\n",
    "A vague prior density for $\\mu$ and $\\sigma$, is uniform on $(\\mu, \\log\\sigma)$, or equivalently,\n",
    "\\[\n",
    "p(\\mu, \\sigma^2) \\propto (\\sigma^2)^{-1}\n",
    "\\]\n",
    "\\subsubsection{The joint posterior distribution}\n",
    "Under this prior density, the joint posterior distributions is\n",
    "\\[\n",
    "p(\\mu, \\sigma^2 | y) \\propto \\sigma^{-n-2}\\exp \\Bigg(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (y_i - \\mu)^2 \\Bigg)\n",
    "\\]\n",
    "\\[\n",
    "= \\sigma^{-n-2}\\exp\\Bigg(-\\frac{1}{2\\sigma^2}\\Bigg[\\sum_{i=1}^n(y_i - \\overline{y})^2 + n(\\overline{y} - \\mu)^2 \\Bigg] \\Bigg)\n",
    "\\]\\[\n",
    "= \\sigma^{-n-2} \\exp \\Bigg(-\\frac{1}{2\\sigma^2} [(n-1)s^2 + n(\\overline{y}-\\mu)^2]\\Bigg)\n",
    "\\]\n",
    "where \n",
    "\\[\n",
    "s^2 = \\frac{1}{n-1}\\sum_{i=1}^n (y_i - \\overline{y})^2\n",
    "\\]\n",
    "\\subsubsection{The conditional posterior distribution, $p(\\mu|\\sigma^2, y)$ }\n",
    "To determine the posterior distribution of $\\mu$, given $\\sigma^2$, we simply use the result derived in Section 2.5:\n",
    "\\[\n",
    "\\mu | \\sigma^2, y \\sim N(\\overline{y}, \\sigma^2/n)\n",
    "\\]\n",
    "\\subsubsection{The marginal posterior distribution, $p(\\sigma^2|y)$}\n",
    "We average the joint distribution over $\\mu$: \n",
    "\\[\n",
    "p(\\sigma^2 | y) \\propto \\int \\sigma^{-n-2}\\exp \\Bigg(-\\frac{1}{2\\sigma^2}[(n-1)s^2 + n(\\overline{y} - \\mu)^2] \\Bigg) d\\mu\n",
    "\\]\n",
    "\\[\n",
    "p(\\sigma^2 | y) \\propto \\sigma^{-n-2} \\exp \\Bigg( - \\frac{1}{2\\sigma^2}(n-1)s^2 \\Bigg) \\sqrt{2 \\pi \\sigma^2/n}\n",
    "\\]\n",
    "\\[\n",
    "\\propto (\\sigma^2)^{-(n+1)/2} \\exp \\Bigg( -\\frac{(n-1)s^2}{2\\sigma^2} \\Bigg)\n",
    "\\]\n",
    "which is a scaled inverse-$\\chi^2$ density:\n",
    "\\[\n",
    "\\sigma^2|y \\sim \\textrm{Inv}-\\chi^2(n - 1, s^2)\n",
    "\\]\n",
    "\\subsubsection{Sampling from the joint posterior distribution}\n",
    "First draw $\\sigma^2$, then draw $\\mu$ \n",
    "\n",
    "\\subsubsection{Analytic form of the marginal posterior distribution of $\\mu$ }\n",
    "Few cases where possible: We can derive the marginal posterior density for $\\mu$ by integrating the joint posterior density over $\\sigma^2$: \n",
    "\\[\n",
    "p(\\mu | y) = \\int_{0}^{\\infty}p(\\mu, \\sigma^2 | y)d\\sigma^2\n",
    "\\]\n",
    "\\[\n",
    "p(\\mu | y) \\propto A^{-n/2}\\int_{0}^{\\infty}z^{(n-2)/2}\\exp(-z)dz\n",
    "\\]\n",
    "\\[\n",
    "\\propto [(n-1)s^2 + n(\\mu - \\overline{y})^2]^{-n/2}\n",
    "\\]\n",
    "\\[\n",
    "\\propto \\Bigg[1 + \\frac{n(\\mu - \\overline{y})^2}{(n-1)s^2} \\Bigg]^{-n/2}\n",
    "\\]\n",
    "\\subsubsection{Posterior predictive distribution for a future observation}\n",
    "The posterior predictive distribution for a future observation, $\\tilde{y}$, can be written as a mixture\n",
    "\\[\n",
    "p(\\tilde{y}|y) = \\iint p(\\tilde{y}|\\mu, \\sigma^2, y)p(\\mu, \\sigma^2 |y) d\\mu d\\sigma^2\n",
    "\\]\n",
    "\n",
    "To draw from the posterior predicitve distribution, first draw $\\mu, \\sigma^2$ from their joint posterior distribution and then simulate $\\tilde{y} \\sim N(\\mu, \\sigma^2)$ \n",
    "\n",
    "\\subsection{Normal data with a conjugate prior distribution}\n",
    "\\subsubsection{A family of conjugate prior distributions}\n",
    "A convenient parameterization is given by the following specification:\n",
    "\\[\n",
    "\\mu | \\sigma^2 \\sim N(\\mu_0, \\sigma^2/\\kappa_0)\n",
    "\\]\n",
    "\\[\n",
    "\\sigma^2 \\sim \\textrm{Inv-}\\chi^2(v_0, \\sigma_0^2)\n",
    "\\]which corresponds to the joint prior density\n",
    "\\[\n",
    "p(\\mu, \\sigma^2) \\propto \\sigma^{-1}(\\sigma^2)^{-(v_0/2+1)}\\exp\\Bigg( -\\frac{1}{2\\sigma^2}[v_0\\sigma_0^2 + \\kappa_0(\\mu_0 - \\mu)^2]\\Bigg)\n",
    "\\]\n",
    "\\subsubsection{The joint posterior distribution, $p(\\mu, \\sigma^2|y)$}\n",
    "Multiplying the prior density by the normal likelihood yields the posterior density\n",
    "\\[\n",
    "p(\\mu, \\sigma^2|y) \\propto \\sigma^{-1}(\\sigma^2)^{-(v_0/2+1)}\\exp \\Bigg( -\\frac{1}{2\\sigma^2}[v_0\\sigma_0^2 + \\kappa_0(\\mu - \\mu_0)^2] \\Bigg) \\cdot\n",
    "\\]\n",
    "\\[\n",
    "\\cdot (\\sigma^2)^{-n/2} \\exp \\Bigg( -\\frac{1}{2\\sigma^2}[(n-1)s^2 + n(\\overline{y} - \\mu)^2] \\Bigg)\n",
    "\\]\n",
    "\\[\n",
    "= \\textrm{N-Inv-}\\chi^2(\\mu, \\sigma^2 | \\mu_n, \\sigma_n^2/\\kappa_n; v_n, \\sigma_n^2)\n",
    "\\]\n",
    "where after some algebra it can be shown that\n",
    "\\[\n",
    "\\mu_n = \\frac{\\kappa_0}{\\kappa_0 + n}\\mu_0 + \\frac{n}{\\kappa_0 + n}\\overline{y}\n",
    "\\]\n",
    "\\[\n",
    "\\kappa_n = \\kappa_0 + n\n",
    "\\]\n",
    "\\[\n",
    "v_n = v_0 + n\n",
    "\\]\n",
    "\\[\n",
    "v_n\\sigma_n^2 = v_0\\sigma_n^2 + (n-1)s^2 + \\frac{\\kappa_0n}{\\kappa_0 + n}(\\overline{y} - \\mu_0)^2\n",
    "\\]\\subsubsection{The conditional posterior distribution, $p(\\mu|\\sigma^2, y)$}\n",
    "The conditional posterior density is proportional to the joint posterior density with $\\sigma^2$ held constant\n",
    "\\[\n",
    "\\mu | \\sigma^2, y \\sim N(\\mu_n, \\sigma^2/\\kappa_n)\n",
    "\\]\n",
    "\\[\n",
    "N\\Bigg( \\frac{\\frac{\\kappa_0}{\\sigma^2}\\mu_0 + \\frac{n}{\\sigma^2}\\overline{y}}{\\frac{\\kappa_0}{\\sigma^2} + \\frac{n}{\\sigma^2}} , \\frac{1}{\\frac{\\kappa_0}{\\sigma^2} + \\frac{n}{\\sigma^2}} \\Bigg)\n",
    "\\]\n",
    "\\subsubsection{The marginal posterior distribution, $p(\\sigma^2|y)$}\n",
    "The marginal posterior density is scaled inverse-$\\chi^2$: \n",
    "\\[\n",
    "\\sigma^2 | y \\sim \\textrm{Inv-}\\chi^2(v_n, \\sigma_n^2)\n",
    "\\]\n",
    "\\subsubsection{Sampling from the joint posterior distribution}\n",
    "We first draw $\\sigma^2$ from its marginal posterior distribution, then draw $\\mu$ from its normal conditional posterior distribution\n",
    "\n",
    "\\subsubsection{Analytic form of the marginal posterior distribution $\\mu$ }\n",
    "\\[\n",
    "p(\\mu | y) \\propto \\Bigg(1 + \\frac{\\kappa_n(\\mu - \\mu_n)^2}{v_n\\sigma_n^2} \\Bigg)^{-(v_n+1)/2}\n",
    "\\]\n",
    "\\[\n",
    "= t_{v_n}(\\mu | \\mu_n, \\sigma_n^2/\\kappa_n)\n",
    "\\]\n",
    "\\subsection{Multinomial model for categorical data}\n",
    "The multinomial sampling distribution is used to describe data for which each observation is one of $k$ possible outcomes. If $y$ is the vector of counts of the number of of observations of each outcome, then\n",
    "\\[\n",
    "p(y | \\theta) \\propto \\prod_{j=1}^k \\theta_j^{y_j}\n",
    "\\]where the sum of the probabilities $\\sum_{j=1}^k\\theta_j$ is 1. The conjugate prior distribution is a multivariate generalization of the beta distribution known as the Dirichlet,\n",
    "\\[\n",
    "p(\\theta | \\alpha) \\propto \\prod_{j=1}^k \\theta_j^{\\alpha_j - 1}\n",
    "\\]\n",
    "The resulting posterior distribution for the $\\theta_j$'s is Dirichlet with parameters $\\alpha_j + y_j$\n",
    "\n",
    "\\subsection{Multivariate normal model with known variance}\n",
    "\\subsubsection{Multivariate normal likelihood}\n",
    "An observable vector $y$ of $d$ components, with the multivariate normal distribution\n",
    "\\[\n",
    "y | \\mu, \\Sigma \\sim N(\\mu, \\Sigma)\n",
    "\\]\n",
    "where $\\mu$ is a column vector of length $d$ and $\\Sigma$ is a $d\\times d$ variance matrix. The likelihood function for a single observation is\n",
    "\\[\n",
    "p(y | \\mu, \\Sigma) \\propto |\\Sigma|^{-1/2} \\exp \\Bigg( -\\frac{1}{2}(y - \\mu)^T \\Sigma^{-1} (y - \\mu) \\Bigg)\n",
    "\\]\\[\n",
    "= |\\Sigma|^{-n/2} \\exp\\Bigg( -\\frac{1}{2}\\textrm{tr}(\\Sigma^{-1}S_0) \\Bigg)\n",
    "\\]\n",
    "where $S_0$ is the matrix of sums of squares relative to $\\mu$,\n",
    "\\[\n",
    "S_0 = \\sum_{i=1}^n(y_i - \\mu)(y_i - \\mu)^T\n",
    "\\]\\subsubsection{Conjugate prior distribution for $\\mu$ with known $\\Sigma$}\n",
    "The conjugate prior distribution for $\\mu$ is the multivariate normal distribution, which we parameterize\n",
    "\\[\n",
    "\\mu \\sim N(\\mu_0, \\Lambda_0)\n",
    "\\]\n",
    "\\subsubsection{Posterior distribution for $\\mu$ with known $\\Sigma$ }\n",
    "The posterior distribution of $\\mu$ is\n",
    "\\[\n",
    "p(\\mu|y, \\Sigma) \\propto \\exp \\Bigg( -\\frac{1}{2} \\Bigg((\\mu - \\mu_0)^T \\Lambda_0^{-1}(\\mu - \\mu_0) + \\sum_{i=1}^n (y_i - \\lambda)^T \\Sigma^{-1}(y_i - \\mu) \\Bigg) \\Bigg)\n",
    "\\]\n",
    "\\[\n",
    "p(\\mu | y, \\Sigma) \\propto \\exp\\Bigg(-\\frac{1}{2}(\\mu - \\mu_n)^T \\Lambda_n^{-1}(\\mu - \\mu_n) \\Bigg)\n",
    "\\]\\[\n",
    "= N(\\mu | \\mu_n, \\Lambda_n)\n",
    "\\]\n",
    "where \n",
    "\\[\n",
    "\\mu_n = (\\Lambda_0^{-1} + n\\Sigma^{-1})^{-1}(\\Lambda_0^{-1}\\mu_0 + n\\Sigma^{-1}\\overline{y})\n",
    "\\]\\[\n",
    "\\Lambda_n^{-1} = \\Lambda_0^{-1} + n\\Sigma^{-1}\n",
    "\\]\n",
    "\\subsubsection{Posterior conditional and marginal distributions of subvectors of $\\mu$ with known $\\Sigma$}\n",
    "The conditional posterior distribution of a subset $\\mu^{(1)}$ given the values of a second subset $\\mu^{(2)}$ is multivariate normal. If we write superscripts in parentheses to indicate appropriate subvectors and submatrices, then\n",
    "\\[\n",
    "\\mu^{(1)} | \\mu^{(2)}, y \\sim N \\Bigg( \\mu_n^{(1)} + \\beta^{1|2}(\\mu^{(2)} - \\mu_n^{(2)}), \\Lambda^{1|2} \\Bigg)\n",
    "\\]\n",
    "where the regression coefficients $\\beta^{1|2}$ and conditional variance matrix $\\Lambda^{1|2}$ are defined by\n",
    "\\[\n",
    "\\beta^{1|2} = \\Lambda_n^{(12)} \\Bigg( \\Lambda_n^{(22)} \\Bigg)^{-1}\n",
    "\\]\n",
    "\\[\n",
    "\\Lambda^{1|2} = \\Lambda_n^{(11)} - \\Lambda_n^{(12)} \\Bigg(\\Lambda_n^{(22)} \\Bigg)^{-1} \\Lambda_n^{(21)}\n",
    "\\]\n",
    "\n",
    "\\subsubsection{Posterior predictive distribution for new data}\n",
    "The marginal posterior distribution of $\\tilde{y}$ is multivariate normal. in this case,\n",
    "\\[\n",
    "\\textrm{E}(\\tilde{y}|y) = \\textrm{E}(\\textrm{E}(\\tilde{y}|\\mu, y)|y) = \\textrm{E}(\\mu | y) = \\mu_n\n",
    "\\]\n",
    "and \n",
    "\\[\n",
    "\\textrm{Var}(\\tilde{y}|y) = \\textrm{E}(\\textrm{Var}\\tilde{y}|\\mu, y) | y) + \\textrm{Var}(\\textrm{E}(\\tilde{y}|\\mu, y)|y)\n",
    "\\]\\[\n",
    "= \\textrm{E}(\\Sigma | y) + \\textrm{Var}(\\mu | y) = \\Sigma + \\Lambda_n\n",
    "\\]\n",
    "\\subsection{Multivariate normal with unknown mean and variance}\n",
    "\\subsubsection{Conjugate inverse-Wishart family of prior distributions}\n",
    "The conjugate distribution for the univariate normal with unknown mean and variance is the normal-inverse-$chi^2$ distribution. We can use the inverse-Wishart distribution, a multivariate generalization of the scaled inverse-$\\chi^2$, to describe the prior distribution of the matrix $\\Sigma$. \n",
    "\\[\n",
    "\\Sigma \\sim \\textrm{Inv-Wishart}_{v_0}(\\Lambda_0^{-1})\n",
    "\\]\n",
    "\\[\n",
    "\\mu | \\Sigma \\sim N(\\mu_0, \\Sigma/\\kappa_0)\n",
    "\\]\n",
    "which corresponds to the joint prior density\n",
    "\\[\n",
    "p(\\mu, \\Sigma) \\propto |\\Sigma|^{-((v_0+d)/2+1)}\\exp \\Bigg( -\\frac{1}{2}\\textrm{tr}(\\Lambda_0 \\Sigma^{-1}) - \\frac{\\kappa_0}{2}(\\mu - \\mu_0)^T \\Sigma^{-1} (\\mu - \\mu_0) \\Bigg)\n",
    "\\]\n",
    "Multiplying the prior density by the normal likelihood results in a posterior density of the same family with parameters\n",
    "\\[\n",
    "\\mu_n = \\frac{\\kappa_0}{\\kappa_0 + n} + \\frac{n}{\\kappa_0 + n}\\overline{y}\n",
    "\\]\\[\n",
    "\\kappa_n = \\kappa_0 + n\n",
    "\\]\n",
    "\\[\n",
    "v_n = v_0 + n\n",
    "\\]\n",
    "\\[\n",
    "\\Lambda_n = \\Lambda_0 + S + \\frac{\\kappa_0n}{\\kappa_0 + n}(\\overline{y} - \\mu_0)(\\overline{y} - \\mu_0)^T\n",
    "\\]\n",
    "where $S$ is the sum of squares matrix about the sample mean\n",
    "\\[\n",
    "S = \\sum_{i=1}^n (y_i - \\overline{y})(y_i - \\overline{y})^T\n",
    "\\]\n",
    "\\subsection{Example: analysis of a bioassay experiment}\n",
    "Few multiparameter sampling models allow simple explicit calculation of posterior distributions. Data analysis for such models is possible using the computational methods. \n",
    "\n",
    "Various dose levels of the compound given to animals. The animals' responses are characterized by a dichotomous outcome: alive or dead, tumor or no tumor. \n",
    "\\[\n",
    "(x_i, n_i, y_i); i = 1, \\dots, k\n",
    "\\]\n",
    "where $x_i$ represents the $i$th of $k$ dose levels given to $n_i$ animals, of which $y_i$ subsequently respond with positive outcome. \n",
    "\n",
    "The data points $y_i$ are binomially distributed:\n",
    "\\[\n",
    "y_i | \\theta_i \\sim \\textrm{Bin}(n_i, \\theta_i)\n",
    "\\]\n",
    "where $\\theta_i$ is the probability of death for animals given dose $x_i$. \n",
    "\n",
    "Dose-response relation: $\\textrm{logit}(\\theta_i) = \\alpha + \\beta x_i$\n",
    "\n",
    "The likelihood for each group $i$ :\n",
    "\\[\n",
    "p(y_i | \\alpha, \\beta, n_i, x_i) \\propto [\\textrm{logit}^{-1}(\\alpha + \\beta x_i)]^{y_i}[1 - \\textrm{logit}^{-1}(\\alpha + \\beta x_i)]^{n_i - y_i}\n",
    "\\]\n",
    "The joint posterior distribution is\n",
    "\\[\n",
    "p(\\alpha, \\beta | y, n, x) \\propto p(\\alpha, \\beta | n, x) p(y | \\alpha, \\beta, n, x)\n",
    "\\]\n",
    "\\[\n",
    "\\propto p(\\alpha, \\beta) \\prod_{i=1}^k p(y_i | \\alpha, \\beta, n_i, x_i)\n",
    "\\]\n",
    "\\subsection{Summary of elementary modeling and computation}\n",
    "Lack of multiparameter models permitting easy calculation of posterior distributions is not a problem: 1. Simple simulation methods 2. Soophisticated models can be represented in a hierarchical or conditional manner 3. We can often apply a normal approximation to the posterior distribution. \n",
    "\n",
    "\\begin{enumerate}\n",
    "\\item Write the likelihood $p(y|\\theta)$\n",
    "\\item Posterior density $p(\\theta | y) \\propto p(\\theta)p(y | \\theta)$ \n",
    "\\item Create a crude estimation of the parameters\n",
    "\\item Draw simulations $\\theta^1, \\dots, \\theta^S$ from the posterior distribution.\n",
    "\\item If any predictive quantities $\\tilde{y}$ are of interest, simulate $\\tilde{y}^1, \\dots, \\tilde{y}^S$ by drawing each $\\tilde{y}^S$ from the sampling distribution conditional on the drawn value $\\theta^S$, $p(\\tilde{y}|\\theta^S)$ \n",
    "\\end{enumerate}\n",
    "\n",
    "\\section{Asymptotics and connections to non-Bayesian approaches}\n",
    "Many simple Bayesian analyses based on noninformative prior distributions give similar results to standard non-Bayesian approaches. \n",
    "\n",
    "\\subsection{Normal approximation to the posterior distribution}\n",
    "\\subsubsection{Normal approximation to the joint posterior distribution}\n",
    "If the posterior distribution $p(\\theta | y)$ is unimodal and roughly symmetric, it can be convenient to approximate it by a normal distribution. \n",
    "\n",
    "Posterior mode $\\hat{\\theta}$\n",
    "\\[\n",
    "p(\\theta | y) \\approx N(\\hat{\\theta}, [I(\\hat{\\theta}]^{-1})\n",
    "\\]\n",
    "where $I(\\theta)$ is the observed information\n",
    "\\[\n",
    "I(\\theta) = - \\frac{d^2}{d\\theta^2}\\log p(\\theta | y)\n",
    "\\]\n",
    "\\subsubsection{Interpretation of the posterior density function relative to its maximum}\n",
    "The multivariate normal distribution provides a benchmark for interpreting the posterior density function and contour plots\n",
    "\n",
    "\\subsubsection{Data reduction and summary statistics}\n",
    "Under the normal approximation, the posterior distribution is summarized by its mode $\\hat{\\theta}$ and the curvature of the posterior density $I(\\hat{\\theta})$; that is, asymptotically these are sufficient statistics. \n",
    "\n",
    "\\subsubsection{Lower-dimensional normal approximations}\n",
    "For a finite sample size $n$, the normal approximation is typically more accurate for conditional and marginal distributions of components of $\\theta$ than for the full joint distribution. \n",
    "\n",
    "\\subsection{Large-sample theory}\n",
    "We review some theory of how the posterior distribution behaves as the amount of data, from some fixed sampling distribution, increases. \n",
    "\n",
    "\\subsubsection{Notation and mathematical setup}\n",
    "Asymptotic normality of the posterior distribution\n",
    "\n",
    "Kullback-Leibler divergence\n",
    "\n",
    "\\subsubsection{Asymptotic normality and consistency}\n",
    "As $n \\to \\infty$, the posterior distribution of $\\theta$ approaches normality with mean $\\theta_0$ and variance $(nJ(\\theta_0))^{-1}$\n",
    "\n",
    "In the limit of large $n$, in the context of a specified family of models, the posterior mode, $\\hat{\\theta}$, approaches $\\theta_0$, and the curvature approaches $nJ(\\hat{\\theta})$ or $nJ(\\theta_0)$. \n",
    "\n",
    "\\subsubsection{Likelihood dominating the prior distribution}\n",
    "The asymptotic results formalize the notion that the importance of the prior distribution diminshes as the sample size increases\n",
    "\n",
    "\\subsection{Counterexamples to the theorems}\n",
    "A good way to understand the limitations of the large-sample results is to consider cases in which the theorems fail\n",
    "\n",
    "\\subsubsection{Underidentified models and nonidentified parameters}\n",
    "Underidentified: if the likelihood $p(\\theta|y)$ is equal for a range of values of $\\theta$. \n",
    "\n",
    "Nonidentified: the data supply no information about $\\rho$, so the posterior distribution of $\\rho$ is the same as its prior distribution, no matter how large the dataset is\n",
    "\n",
    "\\subsubsection{Number of parameters increasing with sample size}\n",
    "Large number of parameters, and then we need to distinguish between different types of asymptotics. For example, sometimes a parameter is assigned for each sampling unit in a study; for example, $y_i \\sim N(\\theta_i, \\sigma^2)$. The parameters $\\theta_i$ generally cannot be estimated consistently unless the amount of data collected from each sampling unit increases along with the number of units. \n",
    "\n",
    "\\subsubsection{Aliasing}\n",
    "Aliasing is a special case of underidentified parameters in which the same likelihood function repeats at a discrete set of points. The problem of aliasing is eliminated by restricting the parameter space so that no duplication appears. \n",
    "\n",
    "\\subsubsection{Unbounded likelihoods}\n",
    "If the likelihood function is unbounded, then there might be no posterior mode within the parameter space, invalidating both the consistency results and the normal approximation\n",
    "\n",
    "\\subsubsection{Improper posterior distributions}\n",
    "An improper posterior distribution cannot occur except with an improper prior distribution\n",
    "\n",
    "\\subsubsection{Prior distributions that exclude the point of convergence}\n",
    "If $p(\\theta_0) = 0$ for a discrete parameter space, or if $p(\\theta) = 0$ in a neighborhood about $\\theta_0$ for a continuous parameter space, then the convergence results, which are based on the likelihood dominating the prior distribution, do not hold. \n",
    "\n",
    "\\subsubsection{Convergence to the edge of parameter space}\n",
    "If $\\theta_0$ is on the boundary of the parameter space, then the Taylor series expansion must be truncated in some directions, and the normal distribution will not necessarily be approprate, even in the limit. \n",
    "\n",
    "\\subsubsection{Tails of the distribution}\n",
    "The normal approximation can hold for essentially all the mass of the posterior distribution but still not be accurate in the tails\n",
    "\n",
    "\\subsection{Frequency evaluations of Bayesian inference}\n",
    "The methods of frequentist statistics provide a useful approach for evaluating the properties of Bayesian inferences\n",
    "\n",
    "\\subsubsection{Large-sample correspondence}\n",
    "The posterior distribution derived from Bayesian theory is asymptotically the same as the repeated sampling distribution. \n",
    "\n",
    "\\subsubsection{Point estimation, consistency and efficiency}\n",
    "A point estimate and its estimated standard error are adequate to summarize a posterior inference, but we interpret the estimate as an inferential summary, not as the solution to a decision problem. \n",
    "\n",
    "When the truth is included in the family of models being fitted, the posterior mode $\\hat{\\theta}$, and also the posterior mean and median, are consistent and asymptotically unbiased under mild regularity conditions.\n",
    "\n",
    "Under  mild regularity conditions, the center of the posterior distribution (defined, for example, by the posterior mean, median, or mode) is asymptotically efficient. \n",
    "\n",
    "\\subsubsection{Confidence coverage}\n",
    "Asymptotically a $100(1 - \\alpha)\\%$ central posterior interval for $\\theta$ has the property that, in repeated samples of $y$, $100(1 - \\alpha)\\%$ of the intervals include the value $\\theta_0$. \n",
    "\n",
    "\\subsection{Bayesian interpretations of other statistical methods}\n",
    "We briefly consider several statistical concepts - point and interval estimation, likelihood inference, unbiased estimation, frequency coverage of confidence intervals, hypothesis testing, multiple comparisons, nonparametric methods, and the jackknife and bootstrap - and discuss their relation to Bayesian methods.\n",
    "\n",
    "\\subsubsection{Maximum likelihood and other point estimates}\n",
    "We can often interpret classical point estimates as exact or approximate posterior summaries based in some implicit full probability model. \n",
    "\n",
    "In the limit (assuming regularity conditions), the maximum likelihood estimate, $\\hat{\\theta}$, is a sufficient statistic - and so is the posterior mode, mean, or median. That is, for large enough $n$, the maximum likelihood estimate (or any other summaries) supplies essentially all the information about $\\theta$ available from the data. \n",
    "\n",
    "\\subsubsection{Unbiased estimates}\n",
    "From a Bayesian perspective, the principle of unbiasedness is reasonable in the limit of large samples, but otherwse is potentially misleading.\n",
    "\n",
    "\\subsubsection{Hypothesis testing}\n",
    "In order for a Bayesian analysis to yield a nonzero probability for a point null hypothesis, it must begin with a nonzero prior probability for that hypothesis. For a continuous parameter $\\theta$, the question \"Does $\\theta$ equal 0?\" can generally be rephrased more usefully as \"What is the posterior distribution for $\\theta$?\"\n",
    "\n",
    "We do find a form of hypothesis test to be useful when assessing the goodness of fit of a probability model.\n",
    "\n",
    "\\subsubsection{Multiple comparisons and multilevel modeling}\n",
    "Several competing multiple comparisons procedures have been derived in classical statistics, with rules about when various $\\theta_j$'s can be declared significantly different. \n",
    "\n",
    "We prefer to handle multiple comparisons problems using hierarchical models. As a result, this Bayesian procedure automatically addresses the key concern of classical multiple comparisons analysis. \n",
    "\n",
    "\\subsubsection{Nonparametric methods, permutation tests, jackknife, bootstrap}\n",
    "Hypothesis tests for comparing medians based on ranks do not have direct counterparts in Bayesian inference; therefore it is hard to interpret the resulting estimates and $p$-values from a Bayesian point of view. \n",
    "\n",
    "\\section{Hierarchical models}\n",
    "It is natural to model such a problem hierarchically, with observable outcomes modeled conditionally on certain parameters, which themselves are given a probabilistic specification in terms of further parameters, known as hyperparameters. \n",
    "\n",
    "Simple nonhierarchical models are usually inapproprate for hierarchical data\n",
    "\n",
    "\\subsection{Constructing a parameterized prior distribution}\n",
    "\\subsubsection{Analyzing a single experiment in the context of historical data}\n",
    "We consider the problem of estimating a parameter $\\theta$ using data from a small experiment and a prior distribution constructed from similar previous (or historical) experiments. \n",
    "\n",
    "\\subsubsection{Analysis with a fixed prior distribution }\n",
    "Assuming a $\\textrm{Beta}(\\alpha, \\beta)$ prior distribution for $\\theta$ yields a $\\textrm{Beta}(\\alpha + 4, \\beta + 10)$ posterior distribution for $\\theta$. \n",
    "\n",
    "\\subsubsection{Approximate estimate of the population distribution using the historical data}\n",
    "Several logical and practical problems with the approach of directly estimating a prior distribution from existing data:\n",
    "\n",
    "If we wanted to use the estimated prior distribution for inference about the first 70 experiments, then the data would be used twice: \n",
    "\n",
    "The point estimate for $\\alpha$ and $\\beta$ seems arbitrary, and using any point estimate for $\\alpha$ and $\\beta$ necessarily ignores some posterior uncertainty.\n",
    "\n",
    "\\subsubsection{Logic of combining information}\n",
    "It clearly makes more sense to try to estimate the population distribution from all the data, and thereby to help estimate each $\\theta_j$, than to estimate all 71 values $\\theta_j$ separately. \n",
    "\n",
    "\\subsection{Exchangeability and hierarcical models}\n",
    "In order to create a joint probability model for all the parameters $\\theta$, we use the crucial idea of exchangability\n",
    "\n",
    "\\subsubsection{Exchangeability}\n",
    "If no information - other than the data $y$ - is avaiable to distinguish any of the $\\theta_j$'s from any of the others, and no ordering or grouping of the parameters can be made, one must assume symmetry among the parameters in their prior distribution. \n",
    "\n",
    "The simplest form of an exchangeable distribution has each of the parameters $\\theta_j$ as an independent sample from a prior (or population) distribution governed by some unknown parameter vector $\\phi$; thus\n",
    "\\[\n",
    "p(\\theta | \\phi) = \\prod_{j=1}^J p(\\theta_j | \\phi)\n",
    "\\]\n",
    "In general, $\\phi$ is unknown, so our distribution for $\\theta$ must average over our uncertainty in $\\phi$: \n",
    "\\[\n",
    "p(\\theta) = \\int \\Bigg( \\prod_{j=1}^J p(\\theta_j | \\phi) \\Bigg) p(\\phi) d\\phi\n",
    "\\]\n",
    "\\subsubsection{Exchangeability when additional information is available on the units}\n",
    "If the observations can be grouped, we may make a hierarchical model, where each group has its own submodel, but the group properties are unknwon. If we assume that group properties are exchangeable, we can use a common prior distribution for the group properties.\n",
    "\n",
    "If $y_i$ has additional information $x_i$ so that $y_i$ are not exchangeable but $(y_i, x_i)$ still are exchangeable, then we can make a joint model for $(y_i, x_i)$ or a conditional model for $y_i | x_i$ \n",
    "\n",
    "In general, the usual way to model exchangeability with covariates is through conditional independence: $p(\\theta_1, \\dots, \\theta_j | x_1, \\dots, x_j) = \\int[\\prod_{j=1}^J p(\\theta_j|\\phi, x_j)]p(\\phi|x)d\\phi$ with $x = (x_1, \\dots, x_J)$. \n",
    "\n",
    "\\subsubsection{Objections to exchangeable models}\n",
    "With no information available to distinguish them, we have no logical choice but to model the $\\theta_j$'s exchangeability. \n",
    "\n",
    "\\subsubsection{The full Bayesian treatment of the hierarchical model}\n",
    "The appropriate Bayesian posterior distribution is of the vector $(\\phi, \\theta)$. The joint prior distribution is\n",
    "\\[\n",
    "p(\\phi, \\theta) = p(\\phi)p(\\theta|\\phi)\n",
    "\\]\n",
    "and the joint posterior distribution is\n",
    "\\[\n",
    "p(\\phi, \\theta | y) \\propto p(\\phi, \\theta)p(y | \\phi, \\theta)\n",
    "\\]\n",
    "\\[\n",
    "= p(\\phi, \\theta)p(y|\\theta)\n",
    "\\]\n",
    "\\subsubsection{The hyperprior distribution}\n",
    "In order to create a joint probability distribution for $(\\phi, \\theta)$, we must assign a prior distribution to $\\phi$. \n",
    "\n",
    "\\subsubsection{Posterior predictive distributions}\n",
    "Two posterior predictive distributions: 1. the distribution of future observations $\\tilde{y}$ correspodning to an existing $\\theta_j$ 2. The distribution of observations $\\tilde{y}$ corresponding to future $\\theta_j$'s drawn from the same superpopulation\n",
    "\n",
    "\\subsection{Bayesian analysis of conjugate hierarchical models}\n",
    "We present an approach that combines analytical and numerical methods to obtain simulations from the joint posterior distribution $p(\\theta, \\phi|y)$ for the beta-binomial model, for which the population distribution $p(\\theta|\\phi)$ is conjugate to the likelihood $p(y|\\theta)$. \n",
    "\n",
    "\\subsubsection{Analytic derivation of conditional and marginal distributions}\n",
    "Three steps analytically\n",
    "\\begin{enumerate}\n",
    "\\item Write the joint posterior density $p(\\theta, \\phi |y)$ in unnormalized form as the product of $p(\\phi)$, $p(\\theta|\\phi)$ and $p(y|\\theta)$\n",
    "\\item Determine $p(\\theta | \\phi, y)$\n",
    "\\item Obtain the marginal posterior distribution $p(\\phi | y)$ \n",
    "\\end{enumerate}\n",
    "\n",
    "For many standard models, the marginal posterior distribution of $\\phi$ can be computed algebraically\n",
    "\n",
    "\\[\n",
    "p(\\phi | y) = \\frac{p(\\theta, \\phi | y}{p(\\theta | \\phi, y)}\n",
    "\\]\n",
    "\\subsubsection{Drawing simulations from the posterior distribution}\n",
    "Draw from the joint posterior distribution $p(\\theta, \\phi|y)$ \n",
    "\\begin{enumerate}\n",
    "\\item Draw the vector or hyperparameters from its marginal posterior distribution $p(\\phi|y)$\n",
    "\\item Draw the parameter vector $\\theta$ from its conditional posterior distribution, $p(\\theta | \\phi, y)$ \n",
    "\\item If desired, draw predictive values $\\tilde{y}$ from the posterior predictive distribution given the drawn $\\theta$. \n",
    "\\end{enumerate}\n",
    "\n",
    "\\subsubsection{Application to the model of rat tumors}\n",
    "\\[\n",
    "y_j \\sim \\textrm{Bin}(n_j, \\theta_j)\n",
    "\\]\n",
    "\\[\n",
    "\\theta_j \\sim \\textrm{Beta}(\\alpha, \\beta)\n",
    "\\]\n",
    "Joint, conditional and marginal posterior distributions\n",
    "\\[\n",
    "p(\\theta, \\alpha, \\beta | y) \\propto p(\\alpha, \\beta) p(\\theta | \\alpha, \\beta) p(y|\\theta, \\alpha, \\beta)\n",
    "\\]\n",
    "\\[\n",
    "\\propto p(\\alpha, \\beta) \\prod_{j=1}^J \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\theta_j^{\\alpha - 1}(1 - \\theta_j)^{\\beta - 1}\\prod_{j=1}^J \\theta_j^{y_j}(1 - \\theta_j)^{n_j - y_j}\n",
    "\\]\n",
    "The joint density is\n",
    "\\[\n",
    "p(\\theta | \\alpha, \\beta, y) = \\prod_{j=1}^J \\frac{\\Gamma(\\alpha + \\beta + n_j)}{\\Gamma(\\alpha + y_j)\\Gamma(\\beta + n_j - y_j)}\\theta_j^{\\alpha + y_j -1}(1 - \\theta_j)^{\\beta + n_j - y_j - 1}\n",
    "\\]\n",
    "The marginal posterior distribution\n",
    "\\[\n",
    "p(\\alpha, \\beta | y) \\propto p(\\alpha, \\beta) \\prod_{j=1}^J \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)}\\frac{\\Gamma(\\alpha + y_j) \\Gamma(\\beta + n_j - y_j)}{\\Gamma (\\alpha + \\beta + n_j)}\n",
    "\\]\n",
    "\\subsection{Normal model with exchangeable parameters}\n",
    "Simple hierarchical model based on the normal distribution, in which observed data are normally distributed with a different mean for each \"group\" or \"experiment\", with known observation variance, and a normal population distribution for the group means. One-way normal random-effects model with known data variance \n",
    "\n",
    "\\subsubsection{The data structure}\n",
    "$J$ independent experiments, with experiment $j$ estimating the parameter $\\theta_j$ from $n_j$ independent normally distributed data points, $y_{ij}$ each with knwon error variance $\\sigma^2$; that is\n",
    "\\[\n",
    "y_{ij} | \\theta_j \\sim N(\\theta_j, \\sigma^2), i=1, \\dots, n_j; j = 1, \\dots, J\n",
    "\\]\n",
    "Using standard notation from the analysis of variance, we label the sample mean of each group $j$ as\n",
    "\\[\n",
    "\\overline{y}_{\\cdot j} = \\frac{1}{n_j}\\sum_{i=1}^{n_j}y_{ij}\n",
    "\\]\n",
    "with sampling variance\n",
    "\\[\n",
    "\\sigma_j^2 = \\sigma^2/n_j\n",
    "\\]\n",
    "We can then write the likelihood for each $\\theta_j$ using the sufficient statistics $\\overline{y}_{\\cdot j}$ \n",
    "\\[\n",
    "\\overline{y}_{\\cdot j} | \\theta_j \\sim N(\\theta_j, \\sigma_j^2) \n",
    "\\]\n",
    "\\subsubsection{The hierarchical model}\n",
    "We assume that the parameters $\\theta_j$ are drawn from a normal distribution with hyperparameters $(\\mu, \\tau)$:\n",
    "\\[\n",
    "p(\\theta_1, \\dots, \\theta_j | \\mu, \\tau) = \\prod_{j=1}^J N(\\theta_j | \\mu, \\tau^2)\n",
    "\\]\n",
    "\\[\n",
    "p(\\theta_1, \\dots, \\theta_J) = \\int \\prod_{j=1}^J [N(\\theta_j | \\mu, \\tau^2)]p(\\mu, \\tau) d(\\mu, \\tau)\n",
    "\\]\n",
    "We assign a noninformative uniform hyperprior distribution to $\\mu$ given $\\tau$\n",
    "\\[\n",
    "p(\\mu, \\tau) = p(\\mu | \\tau)p(\\tau) \\propto p(\\tau)\n",
    "\\]\n",
    "\\subsubsection{The joint posterior distribution}\n",
    "\\[\n",
    "p(\\theta, \\mu, \\tau | y) \\propto p(\\mu, \\tau)p(\\theta | \\mu, \\tau)p(y|\\theta)\n",
    "\\]\n",
    "\\[\n",
    "\\propto p(\\mu, \\tau) \\prod_{j=1}^J N(\\theta_j | \\mu, \\tau^2) \\prod_{j=1}^J N(\\overline{y}_{\\cdot j} | \\theta_j, \\sigma_j^2)\n",
    "\\]\n",
    "\\subsubsection{The conditional posterior distribution of the normal means, given the hyperparameters}\n",
    "The conditional posterior distributions for the $\\theta_j$'s are independent, and\n",
    "\\[\n",
    "\\theta_j | \\mu, \\tau, y \\sim N(\\hat{\\theta}_j, V_j)\n",
    "\\]\n",
    "where \n",
    "\\[\n",
    "\\hat{\\theta}_j = \\frac{\\frac{1}{\\sigma_j^2}\\overline{y}_{\\cdot j} + \\frac{1}{\\tau^2}\\mu}{\\frac{1}{\\sigma_j^2} + \\frac{1}{\\tau^2}}\\textrm{ and }V_j = \\frac{1}{\\frac{1}{\\sigma_j^2} + \\frac{1}{\\tau^2}}\n",
    "\\]\n",
    "\\subsubsection{The marginal posterior distribution of the hyperparameters}\n",
    "For the normal distribution, the marginal likelihood has a particularly simple form. The marginal distributions of the group means $\\overline{y}_{\\cdot j}$, averaging over $\\theta$, are independent normal:\n",
    "\\[\n",
    "\\overline{y}_{\\cdot j} | \\mu, \\tau \\sim N(\\mu, \\sigma_j^2 + \\tau^2)\n",
    "\\]\n",
    "Thus we can write the marginal posterior density as \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
