{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\section{Decision trees}\n",
    "Tree.based methods for regressio  and classification. These involve stratifying or segmenting the predictor space into a number of simple regions. Since the set of splitting rules used to segment the predictor space can be summarized in a tree, these types of approaches are known as decision tree methods\n",
    "\n",
    "Bagging, random forests, and boosting. Each of these approaches involves producing multiple trees which are then combined to yield a single consensus prediction. \n",
    "\n",
    "\\subsection{The basics of decision trees}\n",
    "Decision trees can be applied to both regression and classification problems. \n",
    "\n",
    "\\subsubsection{Regression trees}\n",
    "Example: the tree stratifies or segments the players into three regions of predictor space $R_1, R_2$ and $R_3$. The regions are known as terminal nodes or leaves of the tree. The points along the tree where the predictor space is split are referred to as internal nodes.\n",
    "\n",
    "There are tow steps in building a regression tree;\n",
    "\\begin{enumerate}\n",
    "    \\item We divide the predictor space - that is the set of possible values for $X_1, X_2, \\dots, X_p$ - into $J$ distinct and non-overlapping regions, $R_1, R_2, \\dots, R_J$,\n",
    "    \\item For every observation that falls into the region $R_j$, we make the same prediciton, which is simply the mean of the response values for the training observations in $R_j$. \n",
    "\\end{enumerate}\n",
    "\n",
    "We now elaborate on the first step above. How do we construct the regions $R_1, \\dots, R_J$? We choose to divide the predictor space into high-dimensional rectangles, or boxes. The goal is to find boxes $R_1, \\dots, R_J$ that minimize the RSS, given by\n",
    "\\[\n",
    "\\sum_{j=1}^J \\sum_{i \\in R_{j}} (y_i - \\hat{y}_{R_j})^2\n",
    "\\]\n",
    "where $\\hat{y}_{R_j}$ is the mean response for the training observations within the $j$:th box. We take a top-down, greedy approach that is known as recursive binary splitting. \n",
    "\n",
    "We consider all predictors $X_1, \\dots, X_p$, and all possible values of the cutpoit $s$ for each of the predictors, and then choose the predictor and cutpoint such that the resulting tree has the lowest RSS. In other words, for any $j$ and $s$, we define the pair of half-planes\n",
    "\\[\n",
    "R_1(j,s) = \\{X|X_j < s\\}, R_2(j,s) = \\{X|X_j \\geq s\\}\n",
    "\\]\n",
    "and we seek the value of $j$ and $s$ that minimize the equation\n",
    "\\[\n",
    "\\sum_{i: x_i \\in R_1(j, s)}(y_i - \\hat{y}_{R_1})^2 + \\sum_{i: x_i \\in R_2(j, s)}(y_i - \\hat{y}_{R_2})^2\n",
    "\\]\n",
    "where $\\hat{y}_{R_1}$ is the mean response for the training observation in $R_1(j, s)$, and $\\hat{y}_{R_2}$ is the mean response for the training observations in $R_2(j, s)$. Next we repeat the process, looking for the best predictor and best cutpoint in order to split the data further so as to minimize the RSS within each of the resulting regions. \n",
    "\n",
    "\\textbf{Tree pruning}\n",
    "\n",
    "A strategy is to grow a very large tree $T_0$, and then prune it back in order to obtain a subtree. How do we determine the best way to prune the tree? Our goal is to select a subtree that leads to the lowest test error rate. We need a way to select a small set of subtrees for consideration. \n",
    "\n",
    "Cost complexity pruning or weakest link pruning gives us a way to do just this. We consider a sequence of trees indexed by a nonnegative tuning parameter $\\alpha$.\n",
    "\n",
    "Building a regression tree:\n",
    "\\begin{enumerate}\n",
    "    \\item Use recursive binary splitting to grow a large tree on the training data, stopping only when each terminal node has fewer than some minimum number of observations.\n",
    "    \\item Apply cost complexity pruning to the large tree in order to obtain a sequence of best subtrees, as a function of $\\alpha$\n",
    "    \\item Use K-fold cross-validation to choose $\\alpha$. That is, divide the training observations into $K$ folds. For each $k = 1, \\dots, K$:\n",
    "        \\item Repeat steps 1 and 2 on all but the kth fold of the training data.\n",
    "        \\item Evaluate the mean squared prediction error on the data in the left-out kth fold, as a function of $\\alpha$. Average the results for each value of $\\alpha$, and pick $\\alpha$ to minimize the average error.\n",
    "    \\item Return the subtree from step 2 that corresponds to the chosen value of $\\alpha$.\n",
    "\\end{enumerate}\n",
    "\n",
    "For each value of $\\alpha$ there corresponds a subtree $T \\subset T_0$ such that\n",
    "\\[\n",
    "\\sum_{m=1}^{|T|}\\sum_{i: x_i \\in R_m}(y_i - \\hat{y}_{R_m})^2 + \\alpha|T|\n",
    "\\]\n",
    "is as small as possible. Here $|T|$ indicates the number of terminal nodes of the tree $T$, $R_m$ is the rectangle (i.e. the subset of predictor space) corresponding to the mth terminal node, and $\\hat{y}_{R_m}$ is the predicted response associated with $R_m$. \n",
    "\n",
    "\\subsubsection{Classification trees}\n",
    "A classification tree is very similar to a regression tree, except that it is used to predict a qualitative response rather than a quantitative one. For a classification tree, we predict that each observation belongs to the most commonly occurring class of training observations in the region to which it belongs. \n",
    "\n",
    "Just as in the regression setting, we use recursive binary splitting to grow a classification tree. A natural alternative to RSS is the classification error rate. The classification error rate is simply the fraction of the training observations in that region that do not belong to the most common class:\n",
    "\\[\n",
    "E = 1 - max_k(\\hat{p}_{mk})\n",
    "\\]\n",
    "Here $\\hat{p}_{mk}$ represents the proportion of training observations in the mth region that are fromm the $k$th class. However, it rusn out that classification error is not sufficently sensitive fro tree-growing, and in practice two other measures are preferable.\n",
    "\n",
    "The Gini index is defined by\n",
    "\\[\n",
    "G = \\sum_{k=1}^K \\hat{p}_{mk}(1 - \\hat{p}_{mk})\n",
    "\\]\n",
    "a measure of total variance across the $K$ classes. An alternative to the Gini index is entropy, given by\n",
    "\\[\n",
    "D = - \\sum_{k=1}^K \\hat{p}_{mk}\\log \\hat{p}_{mk}\n",
    "\\]\n",
    "\n",
    "When building a classification tree, either the Gini index or the entropy are typically used to evaluate the quality of a particular split, since these two approaches are more sensitive to node purity than is the classification error rate. Any of these approaches might be used when pruning the tree, but the classification error rate is preferable if prediction accuracy of the final prunde tree is the goal. \n",
    "\n",
    "\\subsubsection{Trees versus linear models}\n",
    "If the relationship between the features and the response is well approximated by a linear model, then an approach such as linear regression will likely work well, and will outperform a regression tree. \n",
    "\n",
    "\\subsubsection{Advantages and disadvantages of trees}\n",
    "\\begin{enumerate}\n",
    "    \\item Trees are very easy to explain,\n",
    "    \\item Decision trees might mirror human decision-making more closely\n",
    "    \\item Trees can be displayed graphically\n",
    "    \\item Trees can easily handle qualitative predictors\n",
    "    \\item Unfortunately, trees generally do not have the same level of predictive accuracy as some of the other regression and classification approaches.\n",
    "    \\item Additionally, trees can be very non-robust \n",
    "\\end{enumerate}\n",
    "\n",
    "By aggregating many decision trees, using methods like bagging, random forests, and boosting, the predictive performance of trees can be substantially improved.\n",
    "\n",
    "\\subsection{Bagging}\n",
    "Bootstrap aggregation, or bagging, is a general-purpose procedure for reducing the variance of a statistical learning method; we introduce it here because it is particularly useful and frequently used in the context of decision trees. \n",
    "\n",
    "In theory, we could calculate $\\hat{f}^1(x), \\hat{f}^2(x), \\dots, \\hat{f}^B(x)$ using $B$ separate training sets, and average them in order to obtain a single low-variance statistical learning model, given by\n",
    "\\[\n",
    "\\hat{f}_{avg}(x) = \\frac{1}{B}\\sum_{b=1}^B \\hat{f}^b(x)\n",
    "\\]\n",
    "Of course, this is not practical because we generally do not have access to multiple training sets. Instead, we can bootstrap, by taking repeated samples from the (single) training data set. In this approach we generate $B$ different bootstrapped training data sets. We then train our method on the bth bootstrapped training set in order to get $\\hat{f}^{*b}(x)$, and finally average all the predictions, to obtain\n",
    "\\[\n",
    "\\hat{f}_{bag}(x) = \\frac{1}{B}\\sum_{b=1}^B \\hat{f}^{*b}(x)\n",
    "\\]\n",
    "This is called bagging.\n",
    "\n",
    "How can bagging be extended to a classification problem where $Y$ is qualitative? For a given observation, we can record the class predicted by each of the $B$ trees, and take a majority vote: the overall prediction is the most commonly occurring class among the $B$ predictions. \n",
    "\n",
    "It turns out there is a very straightforward way to estimate the test error of a bagged model, without the need to perform cross-validation or the validation set approach. It can be shown that on average, each bagged tree makes use of around two-thirds of the observations. The remaining one-third are called out-of-bag observations. The resulting OOB error is a valid estimate of the test error for the bagged model. \n",
    "\n",
    "\\textbf{Variable importance:} One can obtain an overall summary of the importance of each predicotr using RSS (for bagging regression trees) or the Gini index (for bagging classification trees). We can record the total amount that the RSS is decreased due to splits over a given predictor. \n",
    "\n",
    "\\subsection{Random forests}\n",
    "Random forests provide an improvement over bagged trees by way of a small tweak that decorrelates the trees. As in bagging, we build a number of decision trees on bootstrapped training samples. Each time a split in atree is considered, a rondom sample of $m$ predictors is chosen as split candidates from the full set of $p$ predictors. In bagging, most or all of the trees will use the strong predictor in the top split. Therefore, all of the bagged trees will look quite similiar. \n",
    "\n",
    "Using a small value of $m$ in building a random forest will typically be helpful when we have a large number of correlated predictors. \n",
    "\n",
    "\\subsection{Boosting}\n",
    "Like bagging, boosting is a general approach that can be applied to many statistical learning methods for regression or classification. Bagging involves creating multiple copies of the original training data set using the bootstrap, fitting a separate decision tree to each copy, and then combining all of the trees in order to create a single predictive model. Boosting works in a similar way, except that the trees are grwon sequentially: each tree is grown using information from previously grown trees. \n",
    "\n",
    "Boosting for regression trees:\n",
    "\\begin{enumerate}\n",
    "    \\item Set $\\hat{f}(x) = 0$ and $r_i = y_i$ for all $i$ in the training set.\n",
    "    \\item For $b = 1,2, \\dots, B$ repeat:\n",
    "        \\item Fit a tree $\\hat{f}^b$ with $d$ splits (d+1 terminal nodes) to the training data $(X, r)$.\n",
    "        \\item Update $\\hat{f}$ by adding in a shrunken version of the new tree:\n",
    "        \\[\n",
    "        \\hat{f}(x) + \\lambda \\hat{f}^b(x) \\to \\hat{f}(x)\n",
    "        \\]\n",
    "        \\item Update the residuals\n",
    "        \\[\n",
    "        r_i - \\lambda \\hat{f}^b(x_i) \\to r_i\n",
    "        \\]\n",
    "    \\item Output the boosted model\n",
    "    \\[\n",
    "    \\hat{f}(x) = \\sum_{b=1}^B \\lambda \\hat{f}^b(x)\n",
    "    \\]\n",
    "\\end{enumerate}\n",
    "\n",
    "Boosting has three tuning parameters:\n",
    "\\begin{enumerate}\n",
    "    \\item The number of tree $B$. We use cross-validation to select $B$.\n",
    "    \\item The shrinkage parameter $\\lambda$. This controls the rate at which boosting learns.\n",
    "    \\item The number $d$ of splits in each tree, which controls the complexity of the boosted ensemble. \n",
    "\\end{enumerate}\n",
    "\n",
    "\\section{Support vector machines}\n",
    "Support vector machines (SVM), an approach for classification that was developed in the computer science community in the 1990s and that has grown in popularity since then. \n",
    "\n",
    "SUpport vector machines are intended for the binary classification setting in which there are two classes.\n",
    "\n",
    "People often loosely refer to the maximal margin classfier, the support vector classfiier, and the support vector machine as \"support vector machines\".\n",
    "\n",
    "\\subsection{Maximal margin classfier}\n",
    "\\subsubsection{What is a hyperplane?}\n",
    "In a p-dimensional space, a hyperplane is a flat affine subspace of dimension $p-1$. \n",
    "\\[\n",
    "\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_pX_p = 0\n",
    "\\]\n",
    "If \n",
    "\\[\n",
    "\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_pX_p > 0\n",
    "\\]\n",
    "then this tells us that $X$ lies to one side of the hyperplane. On the other hand, if\n",
    "\\[\n",
    "\\beta_pX_p < 0\n",
    "\\]\n",
    "then $X$ lies on the other side of the hyperplane. \n",
    "\n",
    "\\subsubsection{Classification using a separating hyperplane}\n",
    "A separating hyperplane has the property that \\[\n",
    "\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip} > 0, \\textrm{if} y_i = 1\n",
    "\\]\n",
    "and \n",
    "\\[\n",
    "\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip} < 0, \\textrm{if} y_i = -1\n",
    "\\]\n",
    "If a separating hyperplane exists, we can use it to construct a very natural classifier: a test observation is assigned a class depending on which side of the hyperplane it is located. \n",
    "\n",
    "A classifier that is based on a separating hyperplane leads to a linear decision boundary.\n",
    "\n",
    "\\subsubsection{The maximal margin classfier}\n",
    "In order to construct a classifier based upon a separating hyperplane, we must have a reasonable way to decide which of the infinite possible separating hyperplanes to use. \n",
    "\n",
    "Maximal margin hyperplane: the separating hyperplane that is farthest from the training observations.\n",
    "\n",
    "Maximal margin classifier: classifying a test observation based on which side of the maximal margin hyperplane it lies. \n",
    "\n",
    "Support vectors: vectors in p-dimensional space corresponding to training observations closest to the maximal margin hyperplane\n",
    "\n",
    "\\subsubsection{The non-separable case}\n",
    "The maximal margin classifier is a very natural way to perform classification, if a separating hyperplane exists. We can extend the concept of a separating hyperplane in order to develop a hyperplane that almost separates the classes, using a so-called soft margin. The generalization of the maximal margin classifier to the non-separable case is known as the support vector classifier.\n",
    "\n",
    "\\subsection{Support vector classifiers}\n",
    "\\subsubsection{Overview of the supprt vector classifier}\n",
    "Rather than seeking the largest possible margin so that every observation is not only on the correct side of the hyperplane but also on the correct side of the margin, we instead allow some observations to be on the incorrect side of the margin, or even the incorrect side of the hyperplane. \n",
    "\n",
    "\\subsubsection{Details of the support vector classifier}\n",
    "The tuning parameter $C$. $C$ determines the number and severity of the violations to the margin (and to the hyperplane) that we will tolerate. We can think of $C$ as abudget for the amount that the margin can be violated by the $n$ observations. \n",
    "\n",
    "$C$ is chosen by cross-validation. \n",
    "\n",
    "Observations that lie directly on the margin, or on the wrong side of the margin for their class, are known as support vectors. These observations do affect the support vector classifier.\n",
    "\n",
    "\\subsection{Support vector machines}\n",
    "We discuss a general mechanism for converting a linear classifier into one that produces non-linear decision boundaries.\n",
    "\n",
    "\\subsubsection{Classification with non-linear decision boundaries}\n",
    "Rather than fitting a support vector classifier using $p$ features, \n",
    "\\[\n",
    "X_1, X_2, \\dots, X_p\n",
    "\\]\n",
    "we could instead fit a support vector classifier using $2p$ features\n",
    "\\[\n",
    "X_1, X_1^2, X_2, X_2^2, \\dots, X_p, X_p^2\n",
    "\\]\n",
    "In the enlarged feature space, the decision boundary that results is in fact linear. But in the original feature space, the solution is generally non-linear. \n",
    "\n",
    "The support vector machine allows us to enlarge the feature space used by the support vector classifier in a way that leads to efficient computations.\n",
    "\n",
    "\\subsubsection{The support vector machine}\n",
    "The support vector machine (SVM) is an extension of the support vector classifier that results from enlarging the feature space in a spefici way, using kernels.\n",
    "\n",
    "The lienar support vector classifier can be represented as \n",
    "\\[\n",
    "f(x) = \\beta_0 + \\sum_{i=1}^n \\alpha_i \\langle x, x_i \\rangle\n",
    "\\]\n",
    "where there are $n$ parameters $\\alpha_i, i = 1, \\dots, n$, one per training observation.\n",
    "\n",
    "To estimate the parameters $\\alpha_1, \\dots, \\alpha_n$ and $\\beta_0$, all we need are the ${n \\choose 2}$ inner products $\\langle x_i, x_i' \\rangle$ between all pairs of training observations. \n",
    "\n",
    "If $S$ is the collection of indices of these support points, we can rewrite any solution function as\n",
    "\\[\n",
    "f(x) = \\beta_0 + \\sum_{i \\in S}\\alpha_i \\langle x, x_i \\rangle\n",
    "\\]\n",
    "which typically involves far fewer terms.\n",
    "\n",
    "Nor suppose that every tome the inner product appears in the representation, we can replace it woth a generalization of the inner product of the form\n",
    "\\[\n",
    "K(x_i, x_i')\n",
    "\\]\n",
    "where $K$ is some function that we will refer to as a kernel. For instance, one could replace every instance of $\\sum_{j=1}^px_{ij}x_{i'j}$ with the quantity\n",
    "\\[\n",
    "K(x_i, x_i') = (1 + \\sum_{j=1}^p x_{ij}x_{i'j})^d\n",
    "\\]\n",
    "This is known as a polynomial kernel of degree $d$. \n",
    "\n",
    "When the support vector classifier is combined with a non-linear kernel, the resulting classifier is known as a support vector machine. \n",
    "\n",
    "The polynomial kernel is one example of a possible non-linear kernel, but alternatives abound. Another popular choice is the radial kernel, which takes the form\n",
    "\\[\n",
    "K(x_i, x_i') = \\exp(-\\gamma \\sum_{J=1}^p (x_{ij} - x_{i'j})^2)\n",
    "\\]\n",
    "\n",
    "\\subsection{SVMs with more than two classes}\n",
    "How can we extend SVMs to the more general case where we have some arbitrary number of classes? The two most popular approaches are one-versus-one and one-versus-all.\n",
    "\n",
    "\\subsubsection{One-versus-one classification}\n",
    "We construct ${K \\choose 2}$ SVMs, each of which compares a pair of classes. E.g., one such SVM might compare the kth class, coded as 1, to the k'th class, coded as -1. We classify a test observation using each o the ${K \\choose 2}$ classifiers, and we tally the number of times that the test observation is assigned to each of the $K$ classes. The final classification is performed by assigning the test observation to the class to which it was most frequently assigned in these pairwise classifications.\n",
    "\n",
    "\\subsubsection{One-versus-all classification}\n",
    "We fir $K$ SVMs, each time comparing one of the $K$ classes to the remaining $K-1$ classes. Let $\\beta_{0k}, \\beta_{1k}, \\dots, \\beta_{pk}$ denote the parameters that result from fitting an SV; comparing the kth class (1) to the others (-1). We assign the observation to the class for which $\\beta_{0k} + \\beta_{1k}x_1^* + \\beta_{2k}x_2^* + \\dots + \\beta_{pk}x_p^*$ is largest, as this amounts to a high level of confidence that the test observation belongs to the kth class rather than to any of the other classes.\n",
    "\n",
    "\\subsection{Relationship to logistic regression}\n",
    "Logistic regression and the support vector classifier often give very similar results. When the classes are well separated, SVMs tend to behave better than logistic regression; in more overlapping regimes, logistic regression is often preferred. \n",
    "\n",
    "For hisotricla reasons, the use of non-linear kernels is much more widespread in the context of SV;s than in the context of logistic regression or other methods. \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
