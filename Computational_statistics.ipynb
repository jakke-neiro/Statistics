{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\section{Numerical mathematics}\n",
    "All numerical and statistical computations are performed on computers\n",
    "\n",
    "The numbers represented by a computer do not have all the same properties as real numbers!\n",
    "\n",
    "\\subsection{Floating point numbers}\n",
    "\n",
    "\\textbf{Floating point numbers}\n",
    "\\[\n",
    "x_{FP} = \\pm c 2^q\n",
    "\\]\n",
    "Both the coefficient $c$ and exponent $q$ are integers\n",
    "\n",
    "Underflow: there are real numbers for which the closest floating point number is 0\n",
    "\n",
    "Overflow: a result that is larger than the largest floating number\n",
    "\n",
    "IEEE Standard for Floating-Point Arithmetic (IEEE 754). 64 bit double precision numbers and 32 bit single precision numbers\n",
    "\n",
    "\\begin{minted}{Python}\n",
    "import numpy as np\n",
    "#imports the numpy package\n",
    "\n",
    "#class numpy.finfo(dtype)\n",
    "#Machine limits for floating point types.\n",
    "\n",
    "print(np.finfo(np.float32))\n",
    "#Prints machine parameters for float32\n",
    "#E.g. precision = 6 and max = 3.4028235e+38\n",
    "\\end{minted}\n",
    "\n",
    "Eps returned above is the smallest number of the given type for which $1 + eps \\neq 1$ \n",
    "\n",
    "\\subsubsection{Special values}\n",
    "Infinity and not-a-number are special values.\n",
    "\\begin{minted}{Python}\n",
    "import numpy as np\n",
    "print('exp(1000) =', np.exp(1000))\n",
    "\n",
    "## exp(1000) = inf\n",
    "## \n",
    "## /usr/bin/python3:1: RuntimeWarning: overflow encountered in exp\n",
    "\\end{minted}\n",
    "\n",
    "All arithmetic operations including NaN result in NaN!\n",
    "\\begin{minted}{R}\n",
    "np.isnan()\n",
    "#Testing if a given number is NaN\n",
    "\\end{minted}\n",
    "\n",
    "\\subsection{Some important properties of floating numbers}\n",
    "Uneven distribution of numbers, more dense near zero\n",
    "\n",
    "Using $\\log p$ instead of $p$ can help with probabilities\n",
    "\n",
    "Avoid computing $x-y$ for $x, y >> 0$ or $x, y << 0$\n",
    "\n",
    "\\subsection{Computing with probabilities}\n",
    "It is often numerically advantageous to store their logarithms $\\log p$ instead of raw values $p$.\n",
    "\n",
    "\\subsubsection{Logsumexp}\n",
    "Consider a vector $\\pi = (\\pi_i) = (\\log p_i)$, where $p_i > 0$. \n",
    "\\[\n",
    "\\log S  = \\log \\sum_{i \\in I} \\exp(\\pi_i)\n",
    "\\]\n",
    "The overflow can be avoided by finding \n",
    "\\[\n",
    "M = {\\textrm{max} \\pi_i}\n",
    "\\]\n",
    "for ${i \\in I}$ and computing\n",
    "\\[\n",
    "\\log S = M + \\log \\sum_{i \\in I} \\exp(\\pi_i - M)\n",
    "\\]\n",
    "\n",
    "\\subsection{Best practices and hints}\n",
    "You should not do equality comparisons for floating point numbers because of possible rounding errors\n",
    "\n",
    "Equality tests for special cases: \n",
    "\\begin{minted}{Python}\n",
    "    np.isfinite()\n",
    "    np.isinf()\n",
    "    np.isnan()\n",
    "\\end{minted}\n",
    "\n",
    "\\subsection{Lecture 01 exercises}\n",
    "\n",
    "\\subsubsection{1. Computing with floating point numbers}\n",
    "Write a program to increment x = 0.0 by 0.1 100 times. Compute x - 10. How do you interpret the result?\n",
    "\n",
    "\\begin{minted}[breaklines]{Python}\n",
    "x = 0.0\n",
    "#Defining start value\n",
    "\n",
    "for i in range(100):\n",
    "    x += 0.1\n",
    "    #Adding, x = x + 0.1\n",
    "print(x - 10)\n",
    "\n",
    "##-1.9539925233402755e-14\n",
    "\\end{minted}\n",
    "\n",
    "\\subsubsection{2. Computing probabilities}\n",
    "Compute $\\log(p)$ instead of $p$.\n",
    "\n",
    "1. Probability of randomly drawing the 8191-letter HIV-1 genome\n",
    "\n",
    "2. Probability that you need at least 5000 throws of a regular 6-sided die to get the first 6\n",
    "\n",
    "3. Probability that $x = 200$ when $x \\sim \\textrm{Poisson(1)}$ \n",
    "\n",
    "\\begin{minted}[breaklines]{Python}\n",
    "import numpy as np\n",
    "#1\n",
    "# Probability of drawing one letter from 4-letter alphabet is 1/4\n",
    "# Assuming probalities are independent we get Pr(genome) = 0.25^8191\n",
    "logp_hiv = 8191*np.log10(0.25)\n",
    "print(logp_hiv)\n",
    "\n",
    "#2\n",
    "# Probality for 4999 throws before first 6 is given \n",
    "#by geometric distribution with p = 1/6\n",
    "logp_dice = 4999*np.log10(5/6)+np.log10(1/6)\n",
    "print(logp_dice)\n",
    "\n",
    "#3\n",
    "# Probabitity for x=200 when x ~ #Poisson(1) is given by exp(-1)/200!.\n",
    "# Logarithm of n! can be computed as the sum_i=1^n (log(i))\n",
    "logp_poi = -np.log10(np.exp(1)) - sum([np.log10(i+1) for i in range(200)])\n",
    "print(logp_poi)\n",
    "\\end{minted}\n",
    "\n",
    "\\subsubsection{3. Numerical algorithms}\n",
    "Playing with sums and variances in Python\n",
    "\n",
    "\\begin{minted}[breaklines]{Python}\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "\n",
    "#1\n",
    "def two_pass(x):\n",
    "    n = len(x)\n",
    "    mean = 0\n",
    "    for x_i in x:\n",
    "        mean += x_i/n\n",
    "    variance = 0\n",
    "    for x_i in x:\n",
    "        variance += 1/(n-1)*(x_i-mean)**2\n",
    "    return variance\n",
    "\n",
    "#2\n",
    "def one_pass(x):\n",
    "    n = len(x)\n",
    "    square_sum_x = 0\n",
    "    sum_x = 0\n",
    "    for x_i in x:\n",
    "        square_sum_x += x_i**2\n",
    "        sum_x += x_i\n",
    "    return (square_sum_x-sum_x**2/n)/(n-1)\n",
    "\n",
    "#3\n",
    "sample = npr.normal(1e9, 1, size=1000)\n",
    "print(two_pass(sample)) \n",
    "# variance of sample computed using two-pass approach\n",
    "print(one_pass(sample))  \n",
    "# variance of sample computed using one-pass approach\n",
    "\n",
    "#4\n",
    "#enumerate allows us to loop over something and have an automatic counter.\n",
    "\n",
    "def welfords(x):\n",
    "    m = 0\n",
    "    s = 0\n",
    "    for k, x_i in enumerate(x):\n",
    "        oldm = m\n",
    "        m += (x_i-m)/(k+1)\n",
    "        s+= (x_i-m)*(x_i-oldm)\n",
    "    return s/k \n",
    "# note that indexing of array in python starts from 0 and ends to length(array)-1\n",
    "# so at the end of for loop k=len(x)-1\n",
    "\n",
    "print(welfords(sample))\n",
    "\\end{minted}\n",
    "\n",
    "\\subsubsection{4. Useful special functions}\n",
    "\\begin{minted}{Python}\n",
    "#2\n",
    "from scipy.special import gammaln\n",
    "logp_poi = (-1 - gammaln(201))*np.log10(np.exp(1))\n",
    "print(logp_poi)\n",
    "\\end{minted}\n",
    "\n",
    "\\section{Simulating pseudorandom numbers}\n",
    "Computers are deterministic devices. Most practical applications use pseudorandom numbers - deterministic sequences of numbers that appear to be random.\n",
    "\n",
    "Pseudorandom number generation is based on random number generators: random integers to uniform distribution, unifomr distribution into other distributions.\n",
    "\n",
    "\\subsection{Uniform pseudorandom number generation}\n",
    "Uniform pseudorandom number generators (PRNGs) form the foundation for any computation requiring randomness.\n",
    "\n",
    "Properties of a good random number generator:\n",
    "\\begin{enumerate}\n",
    "    \\item good statistical properties\n",
    "    \\item long period before the number repeat\n",
    "    \\item efficient and compact implementation\n",
    "    \\item usability in parallel computation\n",
    "\\end{enumerate}\n",
    "\n",
    "Mersenne Twister is the most widely used PRNG, but it cannot be used in parallel computation applications\n",
    "\n",
    "\\subsection{Transformations for non-uniform pseudorandom number generation}\n",
    "The simplest way to generate random numbers following a non-uniform distribution is to transform the generated numbers.\n",
    "\n",
    "Affine transformations $A(x) = ax + b$ can adjust the bounds and scale of a distribution\n",
    "\n",
    "Shape of distribution: non-linear transformations\n",
    "\n",
    "\\begin{Theorem}\n",
    "If $p$ is a random variable with the absolutely continuous cumulative density function $\\Theta_p (x)$ and $x \\sim p$, then\n",
    "\\[\n",
    "\\Theta_p (x) \\sim Uniform(0,1)\n",
    "\\]\n",
    "\\end{Theorem}\n",
    "Applying the inverse transform $\\Theta_p^{-1}(y)$ yields the following corollary\n",
    "\n",
    "\\begin{Theorem}\n",
    "Under the assumptions of the previous theorem, if $y \\sim Uniform(0,1)$, then\n",
    "\\[\n",
    "\\Theta_p^{-1}(y) \\sim p\n",
    "\\]\n",
    "\\end{Theorem}\n",
    "\n",
    "Taking an exponentially distributed variable $x\\sim~ Exponential(\\lambda)$ as an example, we have the CDF $\\Theta(x) = 1 - exp(-\\lambda x)$ whose inverse is \n",
    "\\[\n",
    "\\Theta^{-1}(y) = - \\frac{log(1 - y)}{\\lambda}\n",
    "\\]\n",
    "\\subsubsection{Generating normally distributed random draws}\n",
    "Normally distributed random variables can be generated using the Box-Muller transformation based on the polar coordinate representation of the bivariate normal distribution. \n",
    "\n",
    "\\subsection{Rejection sampling}\n",
    "Rejection sampling provides a generic alternative for sampling complicated targets for which a transformation is not available.\n",
    "\n",
    "The density function is graphed onto a rectangular board and darts are thrown at it. The x-positions of these darts will be distributed according to the random variable's density. \n",
    "\n",
    "Rejection sampling can be used in higher dimensions.\n",
    "\n",
    "Target distribution $f(x)$, dominated by the tractable envelope $M g(x)$. Sample for each proposal $x_i$ an additional $u \\sim Uniform(0,1)$ and accept $x_i$ if $u M g(x_i) < f(x_i)$. Before applying rejection sampling, it is critical to check that $f(x) < M g(x)$.\n",
    "\n",
    "\\begin{figure}\n",
    "  \\centering\n",
    "    \\includegraphics[width=1\\textwidth]{Pictures/figure2_RejectionSampling.png}\n",
    "\\end{figure}\n",
    "\n",
    "\\begin{minted}[breaklines]{Python}\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "\n",
    "#f_pdf is the target distribution\n",
    "#g_pdf is the proposal\n",
    "\n",
    "#The rejection sampler\n",
    "def RejectionSampler(f_pdf, g_pdf, g_sample, M, N):\n",
    "    # Returns N samples following f_pdf() using proposal g-pdf()\n",
    "    # Requirement: f_pdf(x) <= M*g_pdf(x) for all x\n",
    "    i = 0\n",
    "    #Initialize index\n",
    "    x = np.zeros(N)\n",
    "    #Initialize vector: vector with N zeroes\n",
    "    while i < N:\n",
    "        x_prop = g_sample()\n",
    "        #We sample a x from the proposal\n",
    "        u = npr.uniform(0, 1)\n",
    "        if (u * M * g_pdf(x_prop)) < f_pdf(x_prop):\n",
    "            # Accept the sample and record it\n",
    "            x[i] = x_prop\n",
    "            i += 1\n",
    "    return x\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "# Set the random seed\n",
    "npr.seed(42)\n",
    " \n",
    "# Define normal pdf\n",
    "def normpdf(x, mu, sigma):\n",
    "    return 1/np.sqrt(2*np.pi*sigma**2) * np.exp(-(x-mu)**2 / (2*sigma**2))\n",
    " \n",
    "# Define target pdf as a mixture of two normals\n",
    "def target_pdf(x):\n",
    "    return 0.6*normpdf(x, -2, 0.8) + 0.4*normpdf(x, 2, 1)\n",
    " \n",
    "# Define the proposal pdf and a function to sample from it\n",
    "def proposal_pdf(x):\n",
    "    return normpdf(x, 0, 2)\n",
    " \n",
    "def sample_proposal():\n",
    "    #randn returns a sample (or samples) from the “standard normal” distribution.\n",
    "    #For random samples from N(mu, sigma^2), use: sigma * np.random.randn(...) + mu\n",
    "    return 2*npr.randn()\n",
    " \n",
    "# Define M\n",
    "M = 3\n",
    "N = 3000\n",
    "mysample = RejectionSampler(target_pdf, proposal_pdf, sample_proposal, M, N)\n",
    "\n",
    "#Two subplots, 1 row and 2 columns\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "t = np.linspace(-6, 6, 100)\n",
    "t2 = np.linspace(-10, 10, 100)\n",
    " \n",
    "# Plot f(x) / (M * g(x)) to verify M is valid (the line should be below 1)\n",
    "#First figure\n",
    "ax[0].plot(t2, target_pdf(t2) / (M*proposal_pdf(t2)))\n",
    "ax[0].set_title('$f(x) / (M \\cdot g(x))$')\n",
    "#Second figure with histogram and plot\n",
    "ax[1].hist(mysample, 100, normed=True)\n",
    "ax[1].plot(t, target_pdf(t), 'g')\n",
    "ax[1].set_title('samples')\n",
    "plt.show()\n",
    "\\end{minted}\n",
    "\n",
    "\\subsection{Inexact methods for non-uniform pseudorandom number generation}\n",
    "Markov chain Monte Carlo can be used to approximately draw samples from a given distribution.\n",
    "\n",
    "\\subsection{Best practices}\n",
    "\\begin{enumerate}\n",
    "    \\item Use a good PRNG\n",
    "    \\item Be careful when using PRNGs in parallel\n",
    "    \\item Set and save your random seed\n",
    "    \\item Validate the output of your samplers carefully by testing on a known distribution\n",
    "\\end{enumerate}\n",
    "\n",
    "\\subsection{Lecture 2 exercises}\n",
    "\\subsubsection{1. Linear congruental random number generator}\n",
    "Linear congruental RNGs are a simple class of algorithms that used to be very popular. As an example, we can consider the following example that is suggested in the POSIX.1-2001 standard as a possible implementation of the C language rand() function:\n",
    "seed = seed * 1103515245 + 12345  \n",
    "return (seed // 65536) \\% 32768\n",
    "\n",
    "1. Implement the above algorithm\n",
    "\n",
    "2. Guess how many random number do you need to generate until you see a repeated number. Test your guess!\n",
    "\n",
    "\\begin{minted}[breaklines]{Python}\n",
    "import numpy as np\n",
    "\n",
    "def crand(seed):\n",
    "    seed = seed * 1103515245 + 12345  \n",
    "    value = (seed // 65536) % 32768\n",
    "    return seed, value\n",
    "\n",
    "seed = np.array([42], np.int64)\n",
    "#Use a 64 bit integer to store the seed used in the iteration\n",
    "print(seed)\n",
    "print(crand(seed))\n",
    "# create an array for storing the output sequence\n",
    "values = np.zeros(2000, np.int64)\n",
    "for i in range(2000):\n",
    "    seed, val = crand(seed)\n",
    "    if val in values:\n",
    "        print(\"Repeat at step\", i)\n",
    "    values[i] = val\n",
    "\\end{minted}\n",
    "\n",
    "\\subsubsection{2. Testing random number generation with the cdf transformation}\n",
    "Using Python tools for cdf and random number generation of the normal distribution, let's test the properties of the cdf transformation and uniform distribution.\n",
    "\n",
    "$x \\sim p$, $\\Theta_p (x) \\sim Uniform(0,1)$\n",
    "\n",
    "\\begin{minted}[breaklines]{Python}\n",
    "# Initialise plotting in the notebook. These commands only need to be run once in each session.\n",
    "\n",
    "import matplotlib.pyplot as plt  \n",
    "# import plotting commands with conventional alias 'plt'\n",
    "import numpy as np              \n",
    "# import numpy with conventional alias 'np'\n",
    "import numpy.random as npr      \n",
    "# import numpy random number generators with conventional alias 'npr'\n",
    "from scipy.stats import norm     \n",
    "# import normal distribution functions from scipy.stats\n",
    "\n",
    "x = npr.normal(size=10000)\n",
    "z = norm.cdf(x)\n",
    "plt.hist(z)\n",
    "\n",
    "#2.\n",
    "#First we generate random vector a with 10 000 values sampled from the uniform distribution, Uniform(0,1)\n",
    "a = npr.uniform(size = 10000)\n",
    "#The inverse cumulative density function is the percentile function ppf\n",
    "b = norm.ppf(a)\n",
    "#We calculate the cumulative density function Omega_p^-1(y)\n",
    "plt.hist(b)\n",
    "#We see that the the cumulative density function Omega_p^-1(y) converges toward the cumulative density \n",
    "#function Omega_p(x), in this case the normal distribution, as stated in corollary 2.1.\n",
    "\n",
    "\\end{minted}\n",
    "\n",
    "\\subsubsection{3. Mersenne Twister}\n",
    "The most popular random number generator is the Mersenne Twister, which is also used by Python (random.random() and numpy.random.randint()). It can be used to generate uniformly distributed integers in a given interval, which can be transformed to floating point numbers in a given interval, as is done by the numpy.random.random().\n",
    "\n",
    "1. Find the documentation of the NumPy random number generators.\n",
    "\n",
    "2. Try the numpy.random.randint() and numpy.random.random() generators while setting the seed to a known value.\n",
    "\n",
    "3. Test that you can recreate the same sequence of numbers by setting the seed to the same value.\n",
    "\n",
    "4. Guess how many different random numbers you need to sample until you start seeing the same number repeated. Test your guess. (Hint: you may run into problems when running out of memory etc. It is advisable to increase the number you test relatively slowly (say by a factor of 2 at a time) to avoid starting runs that completely kill your computer for a long time.)\n",
    "\n",
    "\\begin{minted}[breaklines]{Python}\n",
    "#2\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "npr.seed(123)\n",
    "draws1 = npr.random(10)\n",
    "\n",
    "#3\n",
    "npr.seed(123)\n",
    "draws2 = npr.random(10)\n",
    "print(draws1==draws2)\n",
    "\n",
    "#4\n",
    "for i in range(20,30):\n",
    "    print(\"Testing length\", 2**i)\n",
    "    draws3 = npr.random(2**i)\n",
    "    if len(np.unique(draws3)) != 2**i:\n",
    "        print(\"Clash found at length\", 2**i)\n",
    "        break\n",
    "\n",
    "#The loop stops if an element is repeated\n",
    "#numpy.unique\n",
    "#np.unique([1,1,2,2,3,3])\n",
    "##array([1,2,3])\n",
    "\\end{minted}\n",
    "\n",
    "\\subsubsection{Simulating discrete distributions}\n",
    "Many statistical applications depend on generating random numbers with a specific distribution. We will return to the topic many times with increasingly complex methods, but before that we start simple.\n",
    "\n",
    "Write a program to simulate a fair 6-sided die using a uniform(0,1) RNG such as numpy.random.random()\n",
    "Write a program to simulate a biased with a specified bias coin using a uniform(0,1) RNG.\n",
    "Simulate a coin flip competition between a person using a fair coin and a person using a biased coin 100 times. How large a bias do you need for it to lead to 95\\% probability of the person using the biased coin to obtain more heads than the person using the fair coin? What if you want 99\\% probability?\n",
    "\n",
    "\\begin{minted}[breaklines]{Python}\n",
    "import numpy.random as npr\n",
    "\n",
    "#1\n",
    "npr.seed(123)\n",
    "def dice():\n",
    "    draw = npr.random()\n",
    "    if 0<=draw<1/6:\n",
    "        return 1\n",
    "    elif 1/6<=draw<2/6:\n",
    "        return 2\n",
    "    elif 2/6<=draw<3/6:\n",
    "        return 3\n",
    "    elif 3/6<=draw<4/6:\n",
    "        return 4\n",
    "    elif 4/6<=draw<5/6:\n",
    "        return 5\n",
    "    else: return 6\n",
    "\n",
    "throws = [dice() for i in range(100)]\n",
    "\n",
    "#2\n",
    "def biased_coin(bias, n_draws):\n",
    "    return 1*(npr.random(n_draws)<bias)\n",
    "    \n",
    "#3\n",
    "# Consider that aim at each flip is to get 1\n",
    "bias = 0.51\n",
    "while True:\n",
    "    fair_wins = 0 # Initialize wins with fair coin\n",
    "    biased_wins = 0 # Initialize wins with biased coin\n",
    "    # Consider we redo the experiment 100 times\n",
    "    for i in range(100):\n",
    "        if sum(biased_coin(bias, 100))>sum(biased_coin(0.5, 100)): biased_wins+=1\n",
    "        else: fair_wins+=1\n",
    "    if biased_wins>=95 or bias>=0.99: break\n",
    "    else: bias += 0.01\n",
    "print(bias)\n",
    "\\end{minted}\n",
    "\n",
    "\\subsubsection{Simulating continuous distributions with transformations}\n",
    "1. Derive and implement a method for sampling from the distribution  Exponential($\\lambda$)  using inverse cumulative density transformation (see course notes!).\n",
    "\n",
    "2. Test your method by drawing 1000 samples with the values  $\\lambda$=0.1,1,10 . Compute the mean and standard deviation of the samples for each case. Plot a histogram of the samples together with the density and check if they match.\n",
    "\n",
    "3. Implement a method for sampling from the normal distribution using Box-Muller transformation\n",
    "Test your method and plot the histogram of the samples together with the density and check if they match.\n",
    "\n",
    "4. Check what is the largest value of a normally distributed random number that can be generated like this?\n",
    "\n",
    "\\begin{minted}[breaklines]{Python}\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import matplotlib.pyplot as plt\n",
    "#1 \n",
    "# The Cumulative Density Function (CDF) for exponential distribution is 1-exp(-lambda*x).\n",
    "# Inverse CDF is given by x = -log(1-y)/lambda\n",
    "def exp_rvs(l, n_draws): #lambda is a keyword in Python so we use l to denote the parameter of exp dist.\n",
    "    y = npr.random(n_draws)\n",
    "    draws = -np.log(1-y)/l\n",
    "    return draws\n",
    "\n",
    "#2\n",
    "draw0 = exp_rvs(0.1, 1000)\n",
    "draw1 = exp_rvs(1.0, 1000)\n",
    "draw2 = exp_rvs(10.0, 1000)\n",
    "\n",
    "def exp_pdf(x, l):\n",
    "    return l*np.exp(-l*x)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "x0 = np.linspace(0.0, np.max(draw0), 100)\n",
    "plt.plot(x0, exp_pdf(x0, 0.1))\n",
    "plt.hist(draw0, bins=100, normed=True)\n",
    "\n",
    "x1 = np.linspace(0.0, np.max(draw1), 100)\n",
    "plt.plot(x1, exp_pdf(x1, 1.0))\n",
    "plt.hist(draw1, bins=100, normed=True)\n",
    "\n",
    "x2 = np.linspace(0.0, np.max(draw2), 100)\n",
    "plt.plot(x2, exp_pdf(x2, 10.0))\n",
    "plt.hist(draw2, bins=100, normed=True)\n",
    "\n",
    "#3\n",
    "def normal_rvs(mu, sigma, n_draws):\n",
    "    u1 = npr.random(n_draws)\n",
    "    u2 = npr.random(n_draws)\n",
    "    return sigma*np.sqrt(-2*np.log(u1))*np.cos(2*np.pi*u2)+mu\n",
    "\n",
    "#4\n",
    "def normal_pdf(x, mu, sigma):\n",
    "    return np.exp(-(x-mu)**2/(2*sigma**2))/(np.sqrt(2*np.pi)*sigma)\n",
    "\n",
    "draws = normal_rvs(0.0, 1.0, 1000)\n",
    "x = np.linspace(np.min(draws), np.max(draws), 1000)\n",
    "plt.plot(x, normal_pdf(x, 0.0, 1.0))\n",
    "plt.hist(draws, bins=100, normed=True)\n",
    "\n",
    "#5\n",
    "# double has 53 bits for the fraction, therefore the smallest possible value is 2^-53\n",
    "# the largest value of Box-Muller is obtained when u1 is smallest, which is approximately\n",
    "print(\"5)\", np.sqrt(-2*np.log(2**-53)))\n",
    "\\end{minted}\n",
    "\n",
    "\\subsubsection{Basic rejection sampling}\n",
    "$p(x) = 4x^3$, $x \\in [0,1]$, uniform distribution on the interval [0,1] as the proposal.\n",
    "\n",
    "\\begin{minted}[breaklines]{Python}\n",
    "%matplotlib inline\n",
    "import numpy.random as npr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def RejectionSampler(f_pdf, g_pdf, g_sample, M, N):\n",
    "    # Returns N samples following pdf f_pdf() using proposal g(x)\n",
    "    # with pdf g_pdf() that can be sampled by g_sample()\n",
    "    # Requirement: f_pdf(x) <= M*g_pdf(x) for all x\n",
    "    i = 0\n",
    "    rejects = 0\n",
    "    x = np.zeros(N)\n",
    "    while i < N:\n",
    "        x_prop = g_sample()\n",
    "        u = npr.uniform(0, 1)\n",
    "        if (u * M * g_pdf(x_prop)) < f_pdf(x_prop):\n",
    "            # Accept the sample and record it\n",
    "            x[i] = x_prop\n",
    "            i += 1\n",
    "        else:\n",
    "            rejects += 1\n",
    "    print(\"Acceptance rate:\", N/(N+rejects))\n",
    "    return x\n",
    "\n",
    "samples = RejectionSampler(lambda x: 4*x**3, lambda x: \n",
    "1, npr.random, 4, 10000)\n",
    "plt.hist(samples, 30, normed=True)\n",
    "t = np.linspace(0, 1, 30)\n",
    "plt.plot(t, 4*t**3)\n",
    "\\end{minted}\n",
    "\n",
    "\\subsubsection{Rejection sampling in higher dimensions}\n",
    "\\begin{minted}[breaklines]{Python}\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "\n",
    "npr.seed(123)\n",
    "\n",
    "#Two_dimensional sampler\n",
    "def B2_rvs(n_draws):\n",
    "    n_tot = 0\n",
    "    #Initialise empty n_draws x 2 matrix (n_draws rows, 2 columns)\n",
    "    samples = np.empty([n_draws,2])\n",
    "    i = 0\n",
    "    while True:\n",
    "        n_tot += 1\n",
    "        x, y = 2*npr.random()-1, 2*npr.random()-1\n",
    "        # Because both proposal and target are uniform \n",
    "        #distributions, by setting M = 1/Z of the #target,\n",
    "        # we can accept all proposals that are inside \n",
    "        #the designated region\n",
    "        if x**2+y**2<=1:\n",
    "            samples[i] = [x,y]\n",
    "            i += 1\n",
    "        if i==n_draws: break\n",
    "    accept_ratio = n_draws/n_tot\n",
    "    return samples, accept_ratio\n",
    "\n",
    "#2\n",
    "draws, ar = B2_rvs(1000)\n",
    "plt.scatter(draws[:,0], draws[:,1])\n",
    "#draws[:,0] are all the rows in column 0\n",
    "#draws[:,1] are all the rows in column 1\n",
    "\\end{minted}\n",
    "\n",
    "\\subsubsection{Rejection sampling of Beta(2,3)}\n",
    "$p(x) = 12x(1-x)^2$, $0 \\leq x \\leq 1$. \n",
    "\\begin{minted}[breaklines]{Python}\n",
    "#First we define the rejection sampler\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "\n",
    "#Rejection sampler 1D\n",
    "def RejectionSampler(f_pdf, g_pdf, g_sample, M, N):\n",
    "    # Returns N samples following pdf f_pdf() using proposal g(x)\n",
    "    # with pdf g_pdf() that can be sampled by g_sample()\n",
    "    # Requirement: f_pdf(x) <= M*g_pdf(x) for all x\n",
    "    i = 0\n",
    "    x = np.zeros(N)\n",
    "    while i < N:\n",
    "        x_prop = g_sample()\n",
    "        u = npr.uniform(0, 1)\n",
    "        if (u * M * g_pdf(x_prop)) < f_pdf(x_prop):\n",
    "            # Accept the sample and record it\n",
    "            x[i] = x_prop\n",
    "            i += 1\n",
    "    return x\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Set the random seed\n",
    "npr.seed(42)\n",
    "# Define target pdf\n",
    "def target_pdf(x):\n",
    "    return 12*x*((1-x)**2)\n",
    "# Define the proposal pdf and a function to sample from it\n",
    "def proposal_pdf(x):\n",
    "    return 1\n",
    "def sample_proposal():\n",
    "    return npr.uniform()\n",
    "\n",
    "#We plot p(x)/q(x) to find a value for M\n",
    "x = np.linspace(0,1,100)\n",
    "plt.plot(target_pdf(x)/proposal_pdf(x))\n",
    "\n",
    "#We define m\n",
    "M = 2\n",
    "N = 100000\n",
    "mysample = RejectionSampler(target_pdf, proposal_pdf, sample_proposal, M, N)\n",
    "\n",
    "#We plot the results\n",
    "#subplots one row, two columns\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "t = np.linspace(0, 1, 100)\n",
    "t2 = np.linspace(0, 1, 100)\n",
    "#First subplot\n",
    "ax[0].plot(t2, target_pdf(t2) / (M*proposal_pdf(t2)))\n",
    "ax[0].set_title('$f(x) / (M \\cdot g(x))$')\n",
    "#Second subplot\n",
    "ax[1].hist(mysample, 100, normed=True)\n",
    "ax[1].plot(t, target_pdf(t), 'g')\n",
    "ax[1].set_title('samples')\n",
    "plt.show()\n",
    "\n",
    "#Compute the expectation\n",
    "print(np.mean(mysample**5))\n",
    "#The theoretical value\n",
    "print((2/5)*(3/6)*(4/8)*(5/7)*(6/9))\n",
    "\\end{minted}\n",
    "\n",
    "\\subsubsection{Rejection sampling of a Gaussian with Laplace}\n",
    "$q(x) = 1/2 \\exp(-|x|)$ and $p(x) = \\frac{1}{\\sqrt{2 \\pi}} \\exp(-\\frac{1}{2}x^2)$\n",
    "\n",
    "\\begin{minted}[breaklines]{Python}\n",
    "#1.\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "def laplacesampler(size):\n",
    "    y = np.random.uniform(0,1,size)\n",
    "    x = ()\n",
    "    for i in range(0, len(y)-1):\n",
    "        if y[i] <= 0.5:\n",
    "            x = x + (math.log(2*y[i]),)\n",
    "        if y[i] > 0.5:\n",
    "            x = x + (-math.log(2*(1-y[i])),)\n",
    "    return x\n",
    "mysample = laplacesampler(10000)\n",
    "plt.subplot(2,1,1)\n",
    "plt.hist(mysample, 100, normed=True)\n",
    "t = np.linspace(-10,10, 1000)\n",
    "import scipy.stats as scs\n",
    "plt.plot(t, scs.laplace.pdf(t), \"g\")\n",
    "\n",
    "#2.\n",
    "import numpy.random as npr\n",
    "def RejectionSampler(f_pdf, g_pdf, g_sample, M, N):\n",
    "    # Returns N samples following pdf f_pdf() using proposal g(x)\n",
    "    # with pdf g_pdf() that can be sampled by g_sample()\n",
    "    # Requirement: f_pdf(x) <= M*g_pdf(x) for all x\n",
    "    i = 0\n",
    "    x = np.zeros(N)\n",
    "    while i < N:\n",
    "        x_prop = g_sample()\n",
    "        u = npr.uniform(0, 1)\n",
    "        if (u * M * g_pdf(x_prop)) < f_pdf(x_prop):\n",
    "            # Accept the sample and record it\n",
    "            x[i] = x_prop\n",
    "            i += 1\n",
    "    return x\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Set the random seed\n",
    "npr.seed(42)\n",
    "# Define target pdf\n",
    "def target_pdf(x):\n",
    "    return scs.norm.pdf(x)\n",
    "# Define the proposal pdf and a function to sample from it\n",
    "def proposal_pdf(x):\n",
    "    return scs.laplace.pdf(x)\n",
    "def sample_proposal():\n",
    "    return npr.laplace()\n",
    "\n",
    "#We plot p(x)/q(x) to find a value for M\n",
    "x = np.linspace(-10,10,100)\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(target_pdf(x)/proposal_pdf(x))\n",
    "\n",
    "#We define m\n",
    "M = 2\n",
    "N = 10000\n",
    "mysample = RejectionSampler(target_pdf, proposal_pdf, sample_proposal, M, N)\n",
    "\n",
    "#We plot the results\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "t = np.linspace(-3, 3, 1000)\n",
    "t2 = np.linspace(-3,3, 1000)\n",
    "ax[0].plot(t2, target_pdf(t2) / (M*proposal_pdf(t2)))\n",
    "ax[0].set_title('$f(x) / (M \\cdot g(x))$')\n",
    "ax[1].hist(mysample, 100, normed=True)\n",
    "ax[1].plot(t, target_pdf(t), 'g')\n",
    "ax[1].set_title('samples')\n",
    "plt.show()\n",
    "\\end{minted}\n",
    "\n",
    "\\section{Multivariate normal distributions and numerical linear algebra}\n",
    "The multivariate normal distribution is perhaps the most common distribution in statistics\n",
    "\n",
    "$d$-dimensional multivariate normal $N( \\mu, \\Sigma)$, $d$-dimensional mean vector $\\mu$ and a symmetric positive-definite $d \\times d$ covariance matrix $\\Sigma$\n",
    "\n",
    "\\subsection{Cholesky decomposition}\n",
    "A symmetric positive definite matrix $\\Sigma$ can be represented as\n",
    "\\[\n",
    "\\Sigma = LL^T\n",
    "\\]\n",
    "\n",
    "\\begin{minted}{Python}\n",
    "    scipy.linalg.cholesky\n",
    "    numpy.linalg.cholesky(.., lower=True)\n",
    "\\end{minted}\n",
    "\n",
    "\\subsection{Evaluating the multivariate normal density}\n",
    "\\[\n",
    "\\log p(x) = -\\frac{d}{2}\\log (2\\pi) - \\frac{1}{2}\\log |det \\Sigma| - \\frac{1}{2}(x-\\mu)^{T}\\Sigma^{-1}(x-\\mu)\n",
    "\\]\n",
    "\n",
    "\\subsubsection{Determinant evaluation using the Cholesky decomposition}\n",
    "\\[\n",
    "\\log det \\Sigma = 2 \\sum_{i=1}^s\\log (l_{ii})\n",
    "\\]\n",
    "\n",
    "\\subsubsection{Quadratic form evaluation with Cholesky}\n",
    "\\[\n",
    "(x-\\mu)^T \\Sigma^{-1} (x - \\mu) = \\sum_{i=1}^d z_i^2\n",
    "\\]\n",
    "where $z = (z_1, \\dots, z_d)^T=L^{-1}(x-\\mu)$\n",
    "\n",
    "You should never explicitly compute $\\Sigma^{-1}$!\n",
    "\n",
    "\\begin{minted}{Python}\n",
    "import scipy.linalg as slg\n",
    "z = slg.solve_triangular(L, x-mu, lower=True)\n",
    "\\end{minted}\n",
    "\n",
    "\\subsection{Simulating multivariate normal random draws}\n",
    "\n",
    "\\begin{theorem}\n",
    "Assuming $x \\~ N(0, I_d)$, we can obtain $y \\~ N(\\mu, \\Sigma)$ by using the transformation\n",
    "\\[\n",
    "y = Lx + \\mu\n",
    "\\]\n",
    "where L is the Cholesky decomposition of $\\Sigma$\n",
    "\\end{theorem}\n",
    "\n",
    "\\subsection{Numerical integration}\n",
    "Bayesian statistics provides a theoretically well-founded framework for answering many statistical questions. However, complicated integrals cannot be solved analytically.\n",
    "\n",
    "Numerical integration concerns the approximation of definite integrals through finite sums:\n",
    "\\[\n",
    "\\int_V f(x) dx \\approx \\sum_i w_i f(x_i)\n",
    "\\]\n",
    "\\begin{enumerate}\n",
    "\\item Rectangle rule: piece-wise constant interpolation \n",
    "\\item Trapezoidal rule: piece-wise linear interpolation\n",
    "\\item Simpson's rule: piece-wise quadratic interpolation\n",
    "\\end{enumerate}\n",
    "\n",
    "No efficient general methods for high-dimensional numerical integration are known. Monte Carlo methods are often the best choice.\n",
    "\n",
    "\\subsubsection{Software for numerical integration}\n",
    "\\begin{minted}{Python}\n",
    "scipy.integrate.quad\n",
    "scipy.integrate.trapz\n",
    "scipy.integrate.simps\n",
    "\\end{minted}\n",
    "\n",
    "\\subsection{Some best practices}\n",
    "Always use Cholesky instead of explicit inverse\n",
    "\n",
    "Always use Cholesky instead of explicit determinant\n",
    "\n",
    "\\subsection{Exercises}\n",
    "\\subsubsection{Generating multivariate normal random vectors}\n",
    "We will test generating and visualising multivariate random vectors in 2D.\n",
    "\n",
    "\\begin{minted}[breaklines]{Python}\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rho = 0.5\n",
    "#Covariance matrix\n",
    "S = np.array([[1.0, rho], [rho, 1.0]])\n",
    "A = np.linalg.cholesky(S)\n",
    "x = npr.randn(2, 10000)\n",
    "y = A @ x\n",
    "plt.plot(y[0,:], y[1,:], '.', alpha=0.1)\n",
    "# Set equal axes to make the plot easier to interpret\n",
    "plt.axes().set_aspect('equal', 'datalim')\n",
    "l, V = np.linalg.eig(S)\n",
    "plt.plot(np.array([0, V[0,0]]), np.array([0, V[1, 0]]), 'k')\n",
    "plt.plot(np.array([0, V[0,1]]), np.array([0, V[1, 1]]), 'k')\n",
    "\\end{minted}\n",
    "\n",
    "\\subsubsection{Multivariate normal density}\n",
    "1. Compute $ log|det\\Sigma|$  using the Cholesky decomposition of $\\Sigma$  without using the determinant function. \n",
    "\n",
    "2. Evaluate the quadratic form  \n",
    "\n",
    "3. Repeat the example with changing the value \n",
    "\n",
    "\\begin{minted}[breaklines]{Python}\n",
    "import scipy.linalg\n",
    "\n",
    "#1\n",
    "log_det = 2*sum(np.log(np.abs(np.diag(A))))\n",
    "print(log_det)\n",
    "print(np.linalg.slogdet(S)[1])\n",
    "\n",
    "#2\n",
    "x = np.array([1.0, 2.0])\n",
    "mu = np.zeros(2)\n",
    "A = np.linalg.cholesky(S)\n",
    "res0 = scipy.linalg.solve_triangular(A, x-mu, lower=True)\n",
    "res0 = res0 @ res0\n",
    "print(res0)\n",
    "print((x-mu) @ np.linalg.inv(S) @ (x-mu))\n",
    "\n",
    "#3\n",
    "sigma1 = sigma2 = 1\n",
    "rhos = np.linspace(0.99, 1.0, 10, endpoint=False)\n",
    "\n",
    "\n",
    "for rho in rhos:\n",
    "    S = np.array([[sigma1**2, rho*sigma1*sigma2], [rho*sigma1*sigma2, sigma2**2]])\n",
    "    A = np.linalg.cholesky(S)\n",
    "    res0 = scipy.linalg.solve_triangular(A, x-mu, lower=True)\n",
    "    res0 = res0 @ res0\n",
    "    res1 = (x-mu) @ np.linalg.inv(S) @ (x-mu)\n",
    "    print(np.abs(res0-res1))\n",
    "    \n",
    "\\end{minted}\n",
    "\n",
    "\\subsubsection{High-dimensional normal random variables}\n",
    "1. Distribution of the norms of the random vectors. Generate d-dimensional multivariate normal random variables for d=1, 3, 10, 30, 100, 300, 1000 and plot a histogram of their norms. What do you observe? How do these match the intuition of a low-dimensional bell-shaped curve with most of the probability near the origin.\n",
    "\n",
    "2. Distribution of the angles between pairs of random vectors. Generate pairs of d-dimensional multivariate normal random variables for d=3, 10, 30, 100, 300, 1000 and plot a histogram of the angles between them. What do you observe?\n",
    "\n",
    "\\begin{minted}[breaklines]{Python}\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def gaussnorms(d, n=10000):\n",
    "    x = npr.normal(size=(d, n))\n",
    "    r = np.sqrt(np.sum(x**2, 0))\n",
    "    return r\n",
    "\n",
    "plt.hist(gaussnorms(1000))\n",
    "\n",
    "def gaussprods(d, n=10000):\n",
    "    x = npr.normal(size=(d, n))\n",
    "    y = npr.normal(size=(d, n))\n",
    "    r = np.sum(x*y, 0) / (np.sqrt(np.sum(x**2, 0)) * np.sqrt(np.sum(y**2, 0)))\n",
    "    return r\n",
    "\n",
    "plt.hist(gaussprods(100))\n",
    "\\end{minted}\n",
    "\n",
    "\\subsubsection{Numerical integration by quadrature in 1D}\n",
    "$f(x) = \\cos(||x||_2)p_x(x)$, $p_x (x) = (2 \\pi)^{(-D/2)}\\exp(-||x||_2^2 / 2)$, identity covariance matrix $I_D$\n",
    "\n",
    "\\begin{minted}[breaklines]{Python}\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import scipy.integrate\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def unitgausspdf(x):\n",
    "    D = np.alen(x)\n",
    "    return (2*np.pi)**(-D/2) * np.exp(-0.5*np.linalg.norm(x)**2)\n",
    "\n",
    "def f(x):\n",
    "    return np.cos(np.linalg.norm(x)) * unitgausspdf(x)\n",
    "    \n",
    "t = np.linspace(-10, 10, 100)\n",
    "y = np.zeros(t.shape)\n",
    "for k in range(len(t)):\n",
    "    y[k] = f(t[k])\n",
    "plt.plot(t, y)\n",
    "\n",
    "scipy.integrate.quad(f, 0, 1)\n",
    "scipy.integrate.quad(f, 0, 3)\n",
    "scipy.integrate.quad(f, 0, np.inf)\n",
    "scipy.integrate.quad(f, 0, 10)\n",
    "#scipy.integrate.quad calculates the definite integral\n",
    "\\end{minted}\n",
    "\n",
    "\\subsubsection{Monte Carlo numerical integration}\n",
    "\\begin{minted}[breaklines]{Python}\n",
    "def mc_integral(lower, upper, n):\n",
    "    try:\n",
    "        d = len(lower)\n",
    "        V = np.prod(np.abs(lower-upper))\n",
    "        x = npr.random([n, d])*upper + lower\n",
    "        return np.mean([f(x_i) for x_i in x])*V\n",
    "    except: \n",
    "        V = upper-lower\n",
    "        x = npr.random(n)*V+lower\n",
    "        return np.mean([f(x_i) for x_i in x])*V\n",
    "    \n",
    "for i in range(1,5):\n",
    "    print('I_mc={}, n={}'.format(mc_integral(0.0, 10.0, 10**i), 10**i))\n",
    "print('I_scipy={}'.format(scipy.integrate.quad(f, 0, 10)))\n",
    "\\end{minted}\n",
    "\n",
    "\\subsubsection{Numerical evaluation of multivariate normal probabilities}\n",
    "\\[\n",
    "\\ln p(x; \\mu, \\Sigma) = - \\frac{d}{2} \\ln(2 \\pi) - \\frac{1}{2}\\ln |det\\Sigma| - \\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)\n",
    "\\]\n",
    "\n",
    "\\begin{minted}[breaklines]{Python}\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy.linalg as slg\n",
    "\n",
    "def logpdf(x, mu,  Sigma):\n",
    "    L = np.linalg.cholesky(Sigma)\n",
    "    z = slg.solve_triangular(L, x-mu, lower=True)\n",
    "    value = -(len(x)/2)*math.log(2*math.pi) - 1/2*math.log(abs(np.linalg.det(Sigma))) - 1/2 * np.dot(z,z)\n",
    "    return value\n",
    "#i.\n",
    "print(logpdf(np.array([0,0]), np.array([0,0]), np.matrix( ((4,2*0.8), (2*0.8, 1)) )))\n",
    "#ii.\n",
    "print(logpdf(np.array([0,0]), np.array([0,0]), np.matrix( ((4,2*0.999), (2*0.999, 1)) )))\n",
    "#iii..\n",
    "print(logpdf(np.array([1,1]), np.array([0,0]), np.matrix( ((4,2*0.999), (2*0.999, 1)) )))\n",
    "#iv.\n",
    "print(logpdf(np.array([0,0]), np.array([0,0]), np.matrix( ((4,2*(-0.999)), (2*(-0.999), 1)) )))\n",
    "\\end{minted}\n",
    "\n",
    "\\section{Week 1}\n",
    "\n",
    "\\subsubsection{Input ranges for overflow and underflow}\n",
    "Find the largest integer for which exp() over double precision floating point numbers (float64) return a finite value\n",
    "\n",
    "Wirte a program to determine the smallest integer $x$ for which $\\phi(x) = 1$ when using double precision floating point arithmetic\n",
    "\n",
    "\\begin{minted}[breaklines]{Python}\n",
    "#i.\n",
    "import math\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "#We can cheat and look for the maximum value directly \n",
    "print(np.finfo(np.float64).max)\n",
    "print(math.log(np.finfo(np.float64).max))\n",
    "#709\n",
    "\n",
    "#ii.\n",
    "i = 0\n",
    "while True:\n",
    "    a = 1/(1+math.exp(np.float64(-i)))\n",
    "    if a == 1:\n",
    "        break\n",
    "    i = i + 1\n",
    "print(i)\n",
    "#37\n",
    "\\end{minted}\n",
    "\n",
    "\\subsubsection{Numerical computation of binomial probabilities}\n",
    "\\[\n",
    "f(l, u, n, p) = \\sum_{i=l}^n {n \\choose i} p^i (1-p)^{n-i}\n",
    "\\]\n",
    "\n",
    "\\begin{minted}[breaklines]{Python}\n",
    "import scipy as sp\n",
    "import math \n",
    "def f(l, u, n, p):\n",
    "    summa = 0\n",
    "    for i in range(l, u):\n",
    "        value = (math.factorial(n)/(math.factorial(i)*math.factorial(n-i)))*(p**i)*((1-p)**(n-i))\n",
    "        summa = summa + value\n",
    "    return summa\n",
    "#i.\n",
    "print(f(0,5,10,0.25))\n",
    "#ii.\n",
    "print(f(10,20,20,0.25))\n",
    "#iii.\n",
    "print(f(40,60,100,0.25))\n",
    "#iv\n",
    "print(f(75,100,100,0.25))\n",
    "\\end{minted}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
