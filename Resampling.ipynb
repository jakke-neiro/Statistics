{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\section{Resampling: permutation testing and bootstrap}\n",
    "Frequentist statistics. Resampling methods provide a set of flexible tools for computationally simulated sampling\n",
    "\n",
    "\\subsection{p-values and null hypothesis significance testing}\n",
    "Null hypothesis significance testing (NHST) is one of the most widely used statistical methods.\n",
    "\n",
    "T-test. The test statistic is \n",
    "\\[\n",
    "t = \\frac{\\overline{x_i} - \\overline{x_2}}{\\sqrt{s_1^2/N_1 + s_2^2/N_2}}\n",
    "\\]\n",
    "\n",
    "The general NHST procedure\n",
    "\\begin{enumerate}\n",
    "    \\item Set up a null hypothesis $H_0$ and an alternative hypothesis $H_1$ and select test statistic\n",
    "    \\item Evaluate the test statistic under the observed data $t(X)$\n",
    "    \\item $p = Pr(t is at east as extreme as t(X) | H_0)$\n",
    "    \\item Reject $H_0$ if the p-value is smaller than the pre-selected significance level\n",
    "\\end{enumerate}\n",
    "\n",
    "Permutation testing should only be used if no analytical solution is available\n",
    "\n",
    "\\subsection{Permutation testing}\n",
    "\\[\n",
    "p = P(t_i is at least as extreme as t(X)|H_0) = \\frac{B+1}{M+1}\n",
    "\\]\n",
    "\n",
    "\\begin{minted}[breaklines]{Python}\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "npr.seed(42)\n",
    " \n",
    "# Set the number of permutations\n",
    "N_perm = 1000\n",
    " \n",
    "# Generate two samples\n",
    "x1 = npr.normal(size=20)\n",
    "x2 = npr.normal(size=20) + 0.5\n",
    " \n",
    "# Concatenate to form a whole\n",
    "x = np.concatenate((x1, x2))\n",
    "print(\"mean(x1):\", np.mean(x1))\n",
    "\n",
    "print(\"mean(x2):\", np.mean(x2))\n",
    "\n",
    "truediff = np.abs(np.mean(x1) - np.mean(x2))\n",
    " \n",
    "# Repeatedly randomly permute to mix the groups\n",
    "meandiffs = np.zeros(N_perm)\n",
    "for i in range(N_perm):\n",
    "    z = npr.permutation(x)\n",
    "    meandiffs[i] = np.abs(np.mean(z[0:20]) - np.mean(z[20:]))\n",
    " \n",
    "print('p-value:', (np.sum(truediff <= meandiffs)+1)/(len(meandiffs)+1))\n",
    "\\end{minted}\n",
    "\n",
    "\\subsection{Bootstrap sampling}\n",
    "Translating results from an individual observed sample to the larger population. \n",
    "\n",
    "The Bootstrap principle: approximate $p(x)$ by sampling from the empirical data distribution $p_e(x)$ with replacement. \n",
    "\n",
    "Original data set $X = (x_1, ..., x_N)$ with $N$ samples, $M$ pseudo-data-sets $X_i$. For each pseudo-data-set $X_i$, one obtains an estimate $\\theta_i^*$ of the parameter $\\theta$. Confidence intervals, such as two-sided $1-\\alpha$ confidence interval can be obtained as $[h_{\\alpha/2}, h_{1-\\alpha/2}]$, where $h_{\\alpha}$ denotes the $\\alpha$ quantile of the bootstrap estimates $\\theta_i^*$\n",
    "\n",
    "\\begin{minted}[breaklines]{Python}\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    " \n",
    "npr.seed(42)\n",
    "#Normally distributed data\n",
    "x = npr.normal(size=100)\n",
    " \n",
    "print(np.mean(x))\n",
    "\n",
    "means = np.zeros(1000)\n",
    "for i in range(1000):\n",
    "\t#Indexes are sampled, low=100, size=100\n",
    "    I = npr.randint(100, size=100)\n",
    "    means[i] = np.mean(x[I])\n",
    "print(np.percentile(means, [2.5, 97.5]))\n",
    "\\end{minted}\n",
    "\n",
    "\\begin{figure}\n",
    "    \\centering\n",
    "    \\includegraphics[width=1\\textwidth]{Pictures/ScreenShot144.png}\n",
    "\\end{figure}\n",
    "\n",
    "\\subsection{Exercises}\n",
    "\\subsubsection{Playing with the example from the lecture}\n",
    "Experiment with the below example shown at the lecture. How large does the difference in the means need to be to yield a statistically significant result? How does this change when the number of samples is changed?\n",
    "\n",
    "\\begin{minted}[breaklines]{Python}\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "\n",
    "npr.seed(42)\n",
    "\n",
    "# Set the number of permutations\n",
    "N_perm = 1000\n",
    "\n",
    "# Generate two samples\n",
    "x1 = npr.normal(size=20)\n",
    "x2 = npr.normal(size=20) + 0.5\n",
    "\n",
    "# Concatenate to form a whole\n",
    "x = np.concatenate((x1, x2))\n",
    "print(\"mean(x1):\", np.mean(x1))\n",
    "print(\"mean(x2):\", np.mean(x2))\n",
    "truediff = np.abs(np.mean(x1) - np.mean(x2))\n",
    "\n",
    "# Repeatedly randomly permute to mix the groups\n",
    "meandiffs = np.zeros(N_perm)\n",
    "for i in range(N_perm):\n",
    "    z = npr.permutation(x)\n",
    "    meandiffs[i] = np.abs(np.mean(z[0:20]) - np.mean(z[20:]))\n",
    "print('p-value:', (np.sum(truediff <= meandiffs)+1)/(len(meandiffs)+1))\n",
    "\\end{minted}\n",
    "\n",
    "\\subsubsection{Working with real data in Python}\n",
    "1. Load the data set using the data set using the code below and print the values. Note the column names at the top.\n",
    "\n",
    "2. Plot a histogram of the values of the ages of the data subjects. (Hint: extract the age column and use .values to turn it to a NumPy Array.)\n",
    "\n",
    "3. Compute the correlation coefficient (np.corrcoef) of the age and systolic blood pressure (SBP) values in the data.\n",
    "\n",
    "4. Create index vectors for male and female participants. Plot separate histograms of the cholesterol (CHOL) values of the male and female subjects.\n",
    "\n",
    "\\begin{minted}[breaklines]{Python}\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "\n",
    "# load the data from CSV file using pandas\n",
    "fram = pd.read_csv('http://www.helsinki.fi/~ahonkela/teaching/compstats1/fram.txt', sep='\\t')\n",
    "fram\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(fram['AGE'].values)\n",
    "\n",
    "# note: .values is optional here, most numpy functions work without it too\n",
    "np.corrcoef(fram['AGE'].values, fram['SBP'].values)\n",
    "\n",
    "Imale = (fram['SEX'] == 'male').values\n",
    "Ifemale = (fram['SEX'] == 'female').values\n",
    "plt.hist(fram.iloc[Imale]['CHOL'].values)\n",
    "plt.show()\n",
    "plt.hist(fram.iloc[Ifemale]['CHOL'].values)\n",
    "\\end{minted}\n",
    "\n",
    "\\subsubsection{Simple permutation testing on real data}\n",
    "Permutation testing of AGE of male and female participants, absolute difference of the means as the test statistic\n",
    "\n",
    "1. Implement a permutation test to check if the ages (AGE) of male and female participants in the data set are statistically significantly different using the absolute difference of the means as the test statistic. How do you interpret the result?\n",
    "\n",
    "2. Try a similar test with other variables in the data set. How do you interpret the results?\n",
    "\n",
    "\\begin{minted}[breaklines]{Python}\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "\n",
    "# load the data from CSV file using pandas\n",
    "fram = pd.read_csv('http://www.helsinki.fi/~ahonkela/teaching/compstats1/fram.txt', sep='\\t')\n",
    "\n",
    "# Create index vectors (np.array) for male and female samples\n",
    "Ifemale = (fram['SEX'] == 'female').values\n",
    "Imale = (fram['SEX'] == 'male').values\n",
    "\n",
    "# Compute and print the means for males and females\n",
    "malemean = np.mean(fram.iloc[Imale]['AGE'].values)\n",
    "femalemean = np.mean(fram.iloc[Ifemale]['AGE'].values)\n",
    "print(malemean, femalemean)\n",
    "\n",
    "# Check the number of females for generating the permutations\n",
    "numfemales = sum(Ifemale)\n",
    "\n",
    "# Number of permutations to try\n",
    "numrepeats = 1000\n",
    "# Initialise an array to store the results in\n",
    "mean_differences = np.zeros(numrepeats)\n",
    "npr.seed(100)\n",
    "\n",
    "for i in range(numrepeats):\n",
    "    # Permute the indices\n",
    "    I = npr.permutation(len(Ifemale))\n",
    "    # Split to \"women\" and \"men\"\n",
    "    permmean1 = np.mean(fram['AGE'].values[I[0:numfemales]])\n",
    "    permmean2 = np.mean(fram['AGE'].values[I[numfemales:]])\n",
    "    # Compute the absolute value of the difference\n",
    "    mean_differences[i] = np.abs(permmean1 - permmean2)\n",
    "\n",
    "(np.sum(np.abs(malemean - femalemean) <= mean_differences) + 1)/(len(mean_differences)+1)\n",
    "\n",
    "malemean = np.mean(fram['CHOL'].values[Imale])\n",
    "femalemean = np.mean(fram['CHOL'].values[Ifemale])\n",
    "print(malemean, femalemean)\n",
    "\n",
    "numrepeats = 1000\n",
    "mean_differences = np.zeros(numrepeats)\n",
    "npr.seed(100)\n",
    "\n",
    "for i in range(numrepeats):\n",
    "    I = npr.permutation(len(Ifemale))\n",
    "    permmean1 = np.mean(fram['CHOL'].values[I[0:numfemales]])\n",
    "    permmean2 = np.mean(fram['CHOL'].values[I[numfemales:]])\n",
    "    mean_differences[i] = np.abs(permmean1 - permmean2)\n",
    "\n",
    "max(mean_differences)\n",
    "(np.sum(np.abs(malemean - femalemean) <= mean_differences) + 1)/(len(mean_differences)+1)\n",
    "\\end{minted}\n",
    "\n",
    "\\subsubsection{Bootstrap sampling}\n",
    "1. Compute the bootstrap confidence interval for the mean of the variable 'SBP' in the example data set. Compare that with the theoretical interval (see e.g. http://onlinestatbook.com/2/estimation/mean.html).\n",
    "\n",
    "2. Compute the bootstrap confidence interval for the median of the variable 'SBP' in the example data set.\n",
    "\n",
    "3. Compute the bootstrap confidence interval for the correlation between the variables 'SBP' and 'CHOL' in the example data set.\n",
    "\n",
    "\\begin{minted}[breaklines]{Python}\n",
    "#1\n",
    "npr.seed(42)\n",
    "sbp = fram['SBP']\n",
    "#Number of bootsrap iterations\n",
    "n_bootstrap = 10000\n",
    "#number of elements in data vector\n",
    "n = len(sbp)\n",
    "#Bootsrap means, resample whole sample\n",
    "bootstrap_means = np.array([np.mean(npr.choice(sbp, replace=True, size=n)) for i in range(n_bootstrap)])\n",
    "#Confidence interval\n",
    "print('1. bootstrap interval:', np.percentile(bootstrap_means, [2.5, 97.5]))\n",
    "\n",
    "#Calculating theoretical interval\n",
    "m = np.mean(sbp)\n",
    "sd = np.std(sbp)\n",
    "c_int_theo = [m-1.96*sd/np.sqrt(n), m+1.96*sd/np.sqrt(n)]\n",
    "print('1. theoretical interval:', c_int_theo)\n",
    "\n",
    "#2\n",
    "#Repeat for medians\n",
    "n_bootstrap = 10000\n",
    "bootstrap_medians = np.array([np.median(npr.choice(sbp, replace=True, size=n)) for i in range(n_bootstrap)])\n",
    "print('2. bootstrap interval:', np.percentile(bootstrap_medians, [2.5, 97.5]))\n",
    "\n",
    "#3\n",
    "chol = fram['CHOL']\n",
    "n_bootstrap = 10000\n",
    "bootstrap_correlate = np.zeros(n_bootstrap)\n",
    "for i in range(n_bootstrap):\n",
    "    indices = npr.choice(range(n), replace=True, size=n)\n",
    "    bootstrap_correlate[i] = np.corrcoef(sbp[indices], chol[indices])[0,1]\n",
    "\n",
    "print('3. bootstrap interval:', np.percentile(bootstrap_correlate, [2.5, 97.5]))\n",
    "\\end{minted}\n",
    "\n",
    "\\subsubsection{Using bootstrap to study properties of estimators}\n",
    "1. Simulate data sets of 1000 samples with zero mean and unit variance from the Gaussian and the Laplace distribution.\n",
    "\n",
    "2. Estimate the mean, median and standard deviation and their bootstrap confidence intervals. What do you observe?\n",
    "\n",
    "3. How do the results change when you change the initial data set size?\n",
    "\n",
    "\\begin{minted}[breaklines]{Python}\n",
    "#1\n",
    "#Generate the data sets\n",
    "x = npr.randn(1000)\n",
    "y = npr.laplace(loc=0.0, scale=1./np.sqrt(2.0), size=1000)\n",
    "\n",
    "#bootstrap function\n",
    "def bootstrap(data, n_bootstrap, statistic, conf):\n",
    "    stat = statistic(data)\n",
    "    n = len(data)\n",
    "    bootstrap = np.array([statistic(npr.choice(data, replace=True, size=n)) for i in range(n_bootstrap)])\n",
    "    print(stat, np.percentile(bootstrap, [100*conf, 100*(1-conf)]))\n",
    "\n",
    "statistics = [np.mean, np.median, np.std]\n",
    "data_names = ['Gaussian', 'Laplace']\n",
    "stat_names = ['Mean', 'Median', 'SD']\n",
    "conf = 0.025\n",
    "n_bootstrap=1000\n",
    "#enumerate is a counter\n",
    "for i, data in enumerate([x,y]):\n",
    "    for j, statistic in enumerate(statistics):\n",
    "        print(data_names[i], stat_names[j])\n",
    "        bootstrap(data, n_bootstrap, statistic, conf)\n",
    "\\end{minted}\n",
    "\n",
    "\\subsubsection{More complex permutations}\n",
    "1. Run permutation test on 'DBP' the same way as in Problem 1, i.e. without stratification to low and high weight groups. What is the p-value you obtain? Try to increase the number of permutations until the p-value seems reasonably accurate. (Note: this should not take more than a few seconds to run!)\n",
    "\n",
    "2. Find the four subgroups of the data: males and females with FRW smaller/larger than median. How large are these groups?\n",
    "\n",
    "3. Construct vectors of indices to high/low FRW groups.\n",
    "\n",
    "4. Permute the male/female labels within both FRW groups but do not mix the FRW groups. Repeat to compute the p-value by checking how often in the permutation you obtain results as extreme as those in the real data. Compare your result with that from case 1.\n",
    "\n",
    "\\begin{minted}[breaklines]{Python}\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "\n",
    "# load the data from CSV file using pandas\n",
    "fram = pd.read_csv('http://www.helsinki.fi/~ahonkela/teaching/compstats1/fram.txt', sep='\\t')\n",
    "\n",
    "# Create index vectors (np.array) for male and female samples\n",
    "Ifemale = (fram['SEX'] == 'female').values\n",
    "Imale = (fram['SEX'] == 'male').values\n",
    "\n",
    "malemean = np.mean(fram['DBP'].values[Imale])\n",
    "femalemean = np.mean(fram['DBP'].values[Ifemale])\n",
    "print(malemean, femalemean)\n",
    "\n",
    "numrepeats = 10000\n",
    "mean_differences = np.zeros(numrepeats)\n",
    "npr.seed(100)\n",
    "\n",
    "for i in range(numrepeats):\n",
    "    I = npr.permutation(len(Ifemale))\n",
    "    permmean1 = np.mean(fram['DBP'].values[I[0:numfemales]])\n",
    "    permmean2 = np.mean(fram['DBP'].values[I[numfemales:]])\n",
    "    mean_differences[i] = np.abs(permmean1 - permmean2)\n",
    "\n",
    "(np.sum(np.abs(malemean - femalemean) <= mean_differences) + 1)/(len(mean_differences)+1)\n",
    "\n",
    "medFRW = fram['FRW'].median()\n",
    "Ifemale = (fram['SEX'] == 'female').values\n",
    "Imale = (fram['SEX'] == 'male').values\n",
    "highFRW = (fram['FRW'] > medFRW).values\n",
    "lowFRW = (fram['FRW'] <= medFRW).values\n",
    "\n",
    "malemean = np.mean(fram['DBP'].values[Imale])\n",
    "femalemean = np.mean(fram['DBP'].values[Ifemale])\n",
    "print(malemean, femalemean)\n",
    "\n",
    "IhighFRW = np.where(highFRW)[0]\n",
    "IlowFRW = np.where(lowFRW)[0]\n",
    "\n",
    "numrepeats = 10000\n",
    "mean_differences = np.zeros(numrepeats)\n",
    "npr.seed(100)\n",
    "\n",
    "numhighFRWfemales = np.sum(Ifemale & highFRW)\n",
    "numlowFRWfemales = np.sum(Ifemale & lowFRW)\n",
    "\n",
    "for i in range(numrepeats):\n",
    "    I1 = npr.permutation(IhighFRW)\n",
    "    I2 = npr.permutation(IlowFRW)\n",
    "    permmean1 = np.mean(np.concatenate((fram['DBP'].values[I1[0:numhighFRWfemales]], \n",
    "                                        fram['DBP'].values[I2[0:numlowFRWfemales]])))\n",
    "    permmean2 = np.mean(np.concatenate((fram['DBP'].values[I1[numhighFRWfemales:]],\n",
    "                                        fram['DBP'].values[I2[numlowFRWfemales:]])))\n",
    "    #print(permmean2, permmean1)\n",
    "    mean_differences[i] = np.abs(permmean1 - permmean2)\n",
    "\n",
    "(np.sum(np.abs(malemean - femalemean) <= mean_differences) + 1)/(len(mean_differences)+1)\n",
    "\\end{minted}\n",
    "\n",
    "\\section{Density estimation and cross validation}\n",
    "Parametric models provide a common approach for obtaining a smooth probability model over some data. For example, given a sample $X$, we can fit a probability model $p(x)$ if we assume that $X$ follow the normal distribution. \n",
    "\n",
    "Kernel density estimation: obtaining a smooth probability density from a sample of points without assuming a specific functional form for the density. \n",
    "\n",
    "\\subsection{Kernel density estimation}\n",
    "Given a sample $X = (x_1, x_2, \\dots, x_n)$ and a kernel $K(x)$ that satisfies\n",
    "\\[\n",
    "\\int_{-\\infty}^{\\infty}K(x) dx = 1\n",
    "\\]\n",
    "the kernel density estimate $f_h(x)$ is \n",
    "\\[\n",
    "f_h(x) = \\frac{1}{nh}\\sum_{i=1}^n K \\Bigg( \\frac{x - x_i}{h} \\Bigg)\n",
    "\\]\n",
    "where $h$ denotes the width of the kernel, often called kernel bandwidth.\n",
    "\n",
    "Gaussian kernel\n",
    "\\[\n",
    "K_{gauss}(x) = \\frac{1}{\\sqrt{2 \\pi}} \\exp{\\Bigg( \\frac{-x^2}{2} \\Bigg) }\n",
    "\\]\n",
    "Uniform kernel\n",
    "\\[\n",
    "K_{uniform}(x) = \\begin{cases}\n",
    "1/2, & |x| < 1 \\\\\n",
    "0, & otherwise\n",
    "\\end{cases}\n",
    "\\]\n",
    "Epanechnikov kernel\n",
    "\\[\n",
    "K_{Epanechnikov}(x) = \\begin{cases}\n",
    "3/4(1-x^2), & |x| < 1 \\\\\n",
    "0, & otherwise\n",
    "\\end{cases}\n",
    "\\]\n",
    "\n",
    "Gaussian kernel\n",
    "\\begin{minted}[breaklines]{Python}\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import data as d\n",
    "d = pd.read_csv('http://www.helsinki.fi/~ahonkela/teaching/compstats1\n",
    "/toydata.txt').values\n",
    "\n",
    "#plot the histogram\n",
    "plt.hist(d, 30, normed=True)\n",
    "\n",
    "#The K_gauss formula\n",
    "def K_gauss(x):\n",
    "    return 1/np.sqrt(2*np.pi)*np.exp(-0.5*x**2)\n",
    "    \n",
    "#f_h(x) kernel density formula \n",
    "def kernel_density(t, x, h):\n",
    "    y = np.zeros(len(t))\n",
    "    for i in range(len(t)):\n",
    "        y[i] = np.mean(K_gauss((t[i] - x)/ h)) / h\n",
    "    return y\n",
    " \n",
    "t = np.linspace(-2, 10, 100)\n",
    "plt.plot(t, kernel_density(t, d, 3.0), label='h=3.0')\n",
    "plt.plot(t, kernel_density(t, d, 1.0), label='h=1.0')\n",
    "plt.plot(t, kernel_density(t, d, 0.3), label='h=0.3')\n",
    "plt.plot(t, kernel_density(t, d, 0.1), label='h=0.1')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\\end{minted}\n",
    "\n",
    "\\subsubsection{Multivariate density estimation}\n",
    "$d$-dimensional cases:\n",
    "\\[\n",
    "f_h(x) = \\frac{1}{nh^d}\\sum_{i=1}^n K \\Bigg( \\frac{x - x_i}{h} \\Bigg) \n",
    "\\]\n",
    "\\[\n",
    "K_{gauss}(x) = (2\\pi)^{-d/2}\\exp \\Bigg( \\frac{-||x||^2}{2} \\Bigg)\n",
    "\\]\n",
    "\\[\n",
    "K_{uniform}(x) = \\begin{cases}\n",
    "\\frac{1}{2^d}, & if ||x||_{\\infty} < 1 \\\\\n",
    "0, & otherwise\n",
    "\\end{cases}\n",
    "\\]\n",
    "where $||x||_{\\infty} = max_{i = 1, ..., d} |x_i|$\n",
    "\n",
    "\\subsection{Cross-validation (CV)}\n",
    "Cross-validation: general, simple method for estimating unknown parameters from data\n",
    "\n",
    "\\begin{enumerate}\n",
    "    \\item Randomly partition the data set $X = (x_1, x_2, ..., x_n)$ to a training set $X_{train}$ (n-m samples) and a test set $X_{test}$ (m samples)\n",
    "    \\item Fit the model using $X_{train}$\n",
    "    \\item Test the fitted model by using $X_{test}$\n",
    "    \\item Repeat $t$ times and average the results\n",
    "\\end{enumerate}\n",
    "\n",
    "Some example realisations of the CV procedure\n",
    "\\begin{enumerate}\n",
    "    \\item $k$-fold CV: fixed partition to $k$ equal subsets, testing on every subset once\n",
    "    \\item LOO leave-one-out CV: $m=1$, $t=n$ testing on every sample once\n",
    "\\end{enumerate}\n",
    "\n",
    "\\subsubsection{CV for kernel bandwidth selection}\n",
    "\\begin{minted}[breaklines]{Python}\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    " #cross-validation\n",
    "def cross_validate_density(d):\n",
    "    hs = np.linspace(0.1, 1.0, 10)\n",
    "    logls = np.zeros(len(hs))\n",
    "    for j in range(len(d)):\n",
    "        for i in range(len(hs)):\n",
    "            logls[i] += np.sum(np.log(kernel_density(d[j], \n",
    "            np.delete(d, j), hs[i])))\n",
    "    return (hs, logls)\n",
    " \n",
    "hs, logls = cross_validate_density(d)\n",
    " \n",
    "plt.hist(d, 30, normed=True)\n",
    "h_opt = hs[np.argmax(logls)]\n",
    "print(\"Optimal h:\", h_opt)\n",
    "\n",
    "t = np.linspace(-2, 10, 100)\n",
    "plt.plot(t, kernel_density(t, d, h_opt))\n",
    "plt.show()\n",
    "\\end{minted}\n",
    "\n",
    "\\subsection{Other methods for kernel bandwidth selection}\n",
    "Density estimate $\\hat{f}_h(x)$ and true density $f(x)$.\n",
    "\n",
    "ISE, integrated squared error:\n",
    "\\[\n",
    "ISE(h) = \\int_{- \\infty}^{\\infty} \\Bigg( \\hat{f}_h (x) - f(x) \\Bigg)^2 dx\n",
    "\\]\n",
    "MISE, mean integrated squared error:\n",
    "\\[\n",
    "MISE(h) = E \\Bigg[ \\int_{- \\infty}^{\\infty}(\\hat{f}_h(x) - f(x))^2 dx \\Bigg]\n",
    "\\]\n",
    "\n",
    "\\subsection{Exercises}\n",
    "\\subsubsection{Simple 1D density estimation}\n",
    "Generate a random sample of 1000 samples from the standard zero mean unit variance\n",
    "\\begin{minted}[breaklines]{Python}\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Kernel function\n",
    "def gauss_kernel(x, mu, h):\n",
    "    return 1/np.sqrt(2*np.pi)/h*np.exp(-0.5*(x-mu)**2/h**2)\n",
    "\n",
    "#Single kernel density\n",
    "def single_kernel_density(t, x, h):\n",
    "    return np.mean(gauss_kernel(t, x, h))\n",
    "\n",
    "#Kernel density function\n",
    "def kernel_density_function(t, a, sigma):\n",
    "    y = np.zeros(len(t))\n",
    "    for i in range(len(t)):\n",
    "        y[i] = single_kernel_density(t[i], a, sigma)\n",
    "    return y\n",
    "\n",
    "t = np.linspace(-5, 5, 100)\n",
    "x = npr.randn(1000)\n",
    "plt.hist(x, 30, normed=True, alpha = 0.2)\n",
    "plt.plot(t, gauss_kernel(t, 0.0, 1.0), label='True pdf')\n",
    "\n",
    "y = kernel_density_function(t, x, 0.03)\n",
    "plt.plot(t, y, label='h = 0.01')\n",
    "y = kernel_density_function(t, x, 0.1)\n",
    "plt.plot(t, y, label='h = 0.1')\n",
    "y = kernel_density_function(t, x, 0.3)\n",
    "plt.plot(t, y, label='h = 0.3')\n",
    "y = kernel_density_function(t, x, 1.0)\n",
    "plt.plot(t, y, label='h = 1.0')\n",
    "plt.legend(loc='upper right')\n",
    "\\end{minted}\n",
    "\n",
    "\\subsubsection{Cross-validation to define the bandwidth $h$}\n",
    "Apply cross-validation to find the optimal bandwidth in the previous exercise\n",
    "Plot the estimated density together with the true density and the normed sample histogram.\n",
    "\n",
    "\\begin{minted}[breaklines]{Python}\n",
    "#LOO function\n",
    "def loocv_kernel_density(x):\n",
    "    hs = np.linspace(0.1, 1.0, 100)\n",
    "    logls = np.zeros(len(hs))\n",
    "    for j in range(len(x)):\n",
    "        for i in range(len(hs)):\n",
    "            logls[i] += np.log(kernel_density_function(np.array([x[j]]), np.delete(x, j), hs[i]))\n",
    "    return (hs, logls)\n",
    "\n",
    "hs, logls = loocv_kernel_density(x)\n",
    "myh = hs[np.argmax(logls)]\n",
    "y = kernel_density_function(t, x, myh)\n",
    "plt.plot(t, y, label='h = %.3f'%myh)\n",
    "y_gauss = gauss_kernel(t, 0, 1.0)\n",
    "plt.plot(t, y_gauss, label='True pdf')\n",
    "plt.hist(x, 50, normed=True, alpha = 0.2)\n",
    "plt.legend(loc='upper right')\n",
    "\\end{minted}\n",
    "\n",
    "\\subsubsection{More density estimation}\n",
    "1. Generate a few data sets with different numbers of samples: 30, 100, 300 with the same normal distribution as above and apply the same procedure. Does the optimal bandwidth change and if so, how?\n",
    "\n",
    "2. Generate data from different distributions (at least uniform and Laplace) and apply the same procedure. What do you observe?\n",
    "\n",
    "3. Generate data from the d-dimensional multivariate normal distribution with zero mean and unit covariance and  ��=3,10,20 . \n",
    "\n",
    "4. Estimate the density using the above procedure. Evaluate it at the points  (0,…,0)  and  (1,…,1)  and compare with the ground truth. What do you observe\n",
    "\n",
    "\\begin{minted}{Python}\n",
    "#1\n",
    "\n",
    "n_samples = np.array([30,100,300])\n",
    "for n in n_samples:\n",
    "    x = npr.randn(n)\n",
    "    hs, logls = loocv_kernel_density(x)\n",
    "    myh = hs[np.argmax(logls)]\n",
    "    y = kernel_density_function(t, x, myh)\n",
    "    plt.plot(t, y, label='h = %.3f'%myh)\n",
    "    y_gauss = gauss_kernel(t, 0, 1.0)\n",
    "    plt.plot(t, y_gauss, label='True pdf')\n",
    "    plt.hist(x, 50, normed=True, alpha = 0.2)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "#2\n",
    "from scipy.stats import laplace\n",
    "n_samples = 300\n",
    "x = npr.laplace(size=n_samples, scale=1/np.sqrt(2))\n",
    "hs, logls = loocv_kernel_density(x)\n",
    "myh = hs[np.argmax(logls)]\n",
    "y = kernel_density_function(t, x, myh)\n",
    "plt.plot(t, y, label='h = %.3f'%myh)\n",
    "y_lap = laplace.pdf(t, scale=1./np.sqrt(2))\n",
    "plt.plot(t, y_lap, label='True pdf')\n",
    "plt.hist(x, 50, normed=True, alpha = 0.2)\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Laplace')\n",
    "plt.show()\n",
    "\n",
    "x = npr.uniform(size=n_samples, low=-5., high=5.)\n",
    "hs, logls = loocv_kernel_density(x)\n",
    "myh = hs[np.argmax(logls)]\n",
    "y = kernel_density_function(t, x, myh)\n",
    "plt.plot(t, y, label='h = %.3f'%myh)\n",
    "plt.plot(t, np.ones(len(t))/10, label='True pdf')\n",
    "plt.hist(x, 50, normed=True, alpha = 0.2)\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Uniform')\n",
    "plt.show()\n",
    "\n",
    "#3\n",
    "#Multidimensional normal kernel\n",
    "def dgauss_kernel(x, mu, h):\n",
    "    d = len(x)\n",
    "    return (2*np.pi*h**2)**(-d/2)*np.exp(-0.5*((x-mu)/h**2).dot(x-mu))\n",
    "\n",
    "#Single kernel density\n",
    "def dsingle_kernel_density(t, x, h):\n",
    "    return np.mean([dgauss_kernel(t, e, h) for e in x])\n",
    "\n",
    "#Full kernel density\n",
    "def dkernel_density_function(t, a, sigma):\n",
    "    y = dsingle_kernel_density(t, a, sigma)\n",
    "    return y\n",
    "\n",
    "#LOO cross-validation\n",
    "def dloocv_kernel_density(x):\n",
    "    hs = np.linspace(0.1, 1.0, 100)\n",
    "    logls = np.zeros(len(hs))\n",
    "    for j in range(len(x)):\n",
    "        for i in range(len(hs)):\n",
    "            logls[i] += np.log(dkernel_density_function(x[j], np.delete(x, j), hs[i]))\n",
    "    return hs[np.argmax(logls)]\n",
    "\n",
    "ds = np.array([3,10,20])\n",
    "\n",
    "for d in ds:\n",
    "    x = npr.normal(size=[30, d])\n",
    "    t0 = np.zeros(d)\n",
    "    t1 = np.ones(d)\n",
    "    myh = dloocv_kernel_density(x)\n",
    "    print('d = ',d)\n",
    "    print('True density and estimate at origin', \n",
    "    dkernel_density_function(t0,np.zeros(d),np.ones(d)),\n",
    "    dkernel_density_function(t0,x,myh))\n",
    "    print('True density and estimate at (1,...,1)', \n",
    "    dkernel_density_function(t1,np.zeros(d),np.ones(d)),\n",
    "    dkernel_density_function(t1,x,myh))\n",
    "\n",
    "\\end{minted}\n",
    "\n",
    "\\subsubsection{Variants of cross-validation}\n",
    "1. $k$ -fold cross-validation: partition the data to  $k$  disjoint blocks. Fit the density to  $k−1$  blocks and compute the log-probability of the remaining block. Repeat  $k$ times leaving each block for testing at the time. Implement  $k$-fold cross-validation and test it with different values of $k$ . \n",
    "\n",
    "2.Monte Carlo cross-validation: partition the data to a training set for fitting the density and a test set for evaluating the probaility of desired size. A split of 80\\% training / 20\\% testing is typical, but this can vary depending on data characteristics. Repeat until the desired level of accuracy has been reached. Implement Monte Carlo cross-validation and test it with different parameters.\n",
    "\n",
    "3. Which method seems most useful in getting accurate results quickly?\n",
    "\n",
    "\\begin{minted}[breaklines]{Python}\n",
    "#1\n",
    "\n",
    "def disjoint(x, k):\n",
    "    return np.arange(len(x)) % k\n",
    "\n",
    "def k_foldcv(x, k):\n",
    "    N = len(x)\n",
    "    I = npr.permutation(N)\n",
    "    hs = np.linspace(0.1, 1.0, 100)\n",
    "    logls = np.zeros(len(hs))\n",
    "    for j in range(k):\n",
    "        testI = np.zeros(N, np.bool)\n",
    "        testI[((j*N)//k):(((j+1)*N)//k)] = True\n",
    "        trainI = ~testI\n",
    "        for i in range(len(hs)):\n",
    "            logls[i] += np.sum(np.log(kernel_density_function(x[I[testI]], x[I[trainI]], hs[i])))\n",
    "    return (hs, logls)\n",
    "\n",
    "x = npr.randn(100)\n",
    "\n",
    "for k in [2, 5, 100]:\n",
    "    hs, logls = k_foldcv(x, k)\n",
    "    myh = hs[np.argmax(logls)]\n",
    "    y = kernel_density_function(t, x, myh)\n",
    "    plt.plot(t, y, label='h = %f'%myh)\n",
    "    y_gauss = gauss_kernel(t, 0, 1.0)\n",
    "    plt.plot(t, y_gauss, label='True pdf')\n",
    "    plt.hist(x, 50, normed=True, alpha = 0.2)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "\\end{minted}\n",
    "\n",
    "\\section{Machine learning resampling}\n",
    "Resampling methods are an indispensable tool in modern statistics. They involve repeatedly drawing samples from a training set and refitting a model of interest on each sample in order to obtain additional information about the fitted model. \n",
    "\n",
    "Cross-validation can be used to estimate the test error associated with a given statistical learning method in order to evaluate its performance, or to select the appropriate level of flexibility. \n",
    "\n",
    "Model assessment: evaluating a model's performance \n",
    "\n",
    "Model selection: process of selecting the proper level of flexibility for a model \n",
    "\n",
    "\\section{Cross-validation}\n",
    "Test error: average error that results from using a statistical learning method to predict the response on a new observation.\n",
    "\n",
    "\\subsection{The validation set approach}\n",
    "Divide the set of observations into two parts: training set and test set.\n",
    "\n",
    "The validation estimate of the test error rate can be highly variable, depending on precisely which observations are included in the training set and which observations are included in the validation set.\n",
    "\n",
    "The validation set error rate may tend to overestimate the test error rate for the model fit on the entire data set.\n",
    "\n",
    "\\subsection{Leave-one-out cross-validation}\n",
    "LOOCV. A single observation $(x_1, y_1)$ is used for the validation set, and the remaining observations $\\{(x_2, y_2), \\dots, (x_n, y_n)\\}$ make up the training set. $MSE_1 = (y_1 - \\hat{y})^2$ provides an approximately unbiased estimate for the test error. We can repeat the procedure by selecting for the validation data $n$ times, which produces $n$ squared errors, $MSE_1, \\dots, MSE_n$. The LOOCV estimate for the for the test MSE is the average:\n",
    "\\[\n",
    "CV(n) = \\frac{1}{n}\n",
    "\\]\n",
    "\n",
    "\\subsection{$k$-fold cross-validation}\n",
    "An alternative to LOOCV is $k$-fold CV. This involves randomly dividing the set of observations into $k$ groups, or folds, of approximately equal size. The first fold is treated as a validation set, and the method is fit on the remaining $k-1$ folds. This process results in $k$ estimates of the test error, and the $k$-fold CV estimate is computed by averaging these values\n",
    "\\[\n",
    "CV(k) = \\frac{1}{k}\\sum_{i=1}^k MSE_i\n",
    "\\]\n",
    "\n",
    "When we perform cross-validation, our goal might be to determine how well a given statistical learning procedure can be expected to perform on independent data. But at other times we are interested only in the location of the minimum point in the estimated test MSE curve. \n",
    "\n",
    "\\subsection{Bias-variance trade-off for $k$-fold cross-validation}\n",
    "$k$-fold CV often gives more accurate estimates of the test error rate than does LOOCV. This has to do with a bias-variance trade-off.\n",
    "\n",
    "Typically, given these considerations, one perfroms $k$-fold cross-validation using $k=5$ and $k=10$, as these values have been shown empirically to yield test error rate estimates that suffer neither from excessively high bias nor from very high variance. \n",
    "\n",
    "\\subsection{Cross-validation on classification problems}\n",
    "In this setting, cross-validation works just as described earlier in this chapter, except that rather than using MSE to quantify test error, we instead use the number of misclassified observations. The LOOCV error rate takes the form\n",
    "\\[\n",
    "CV(n) = \\frac{1}{n}\\sum_{i=1}^n Err_i\n",
    "\\]\n",
    "where $Err_i = I(y_i \\neq \\hat{y}_i)$. \n",
    "\n",
    "\\section{The bootstrap}\n",
    "The bootstrap is a widely applicable and extremely powerful statistical tool that can be used to quantify the uncertainty associated with a given estimator or statistical learning method. \n",
    "\n",
    "Rather than repeatedly obtaining independent data sets from the population, we instead obtain distinct data sets by repeatedly sampling observations from the original data set. \n",
    "\n",
    "We randomly select $n$ observations from the data set in order to produce a bootstrap data set, $Z^{*1}$. We can use $Z^{*1}$ to produce a new bootstrap estimate for $\\alpha$, which we call $\\hat{\\alpha}^{*1}$. The standard deviation of these bootstrap estimates is an approximation of the standard error of $\\hat{\\alpha}$.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
