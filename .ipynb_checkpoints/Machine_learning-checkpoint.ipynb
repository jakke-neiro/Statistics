{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\section{Introduction}\n",
    "\n",
    "\\subsection{Concepts}\n",
    "\n",
    "Statistical learning: tools foor understanding data\n",
    "\n",
    "Supervised learning: predicting or estimating an output based on input\n",
    "\n",
    "Unsupervised learning: input but no output\n",
    "\n",
    "Continuous or quantitative output value: regression problem\n",
    "\n",
    "Categorical or qualitative output: classification problem\n",
    "\n",
    "\\subsection{History}\n",
    "1800s: Legendre and Gauss on the method of least squares\n",
    "\n",
    "1940s: logistic regression\n",
    "\n",
    "1970s: generalized linear models\n",
    "\n",
    "1986: generalized additive models, non-linear extensions to generalized linear models\n",
    "\n",
    "\\subsection{Notation}\n",
    "Matrix\n",
    "\\[\n",
    "\\textbf{X} = \\begin{bmatrix}\n",
    "x_{11} & x_{12} & \\dots & x_{1p} \\\\\n",
    "x_{21} & x_{22} & \\dots & x_{2p} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{n1} & x_{n2} & \\dots & x_{np} \\\\\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "\\section{Statistical learning}\n",
    "Input variables: predictors, independent variables, features, variables\n",
    "\n",
    "Output variables: response, dependent variable\n",
    "\n",
    "Statistical model:\n",
    "\\[\n",
    "Y = f(X) + \\epsilon\n",
    "\\]\n",
    "Systematic term and random term\n",
    "\n",
    "Statistical learning: set of approaches for estimating f\n",
    "\n",
    "\\subsection{Why estimate f?}\n",
    "Prediction and inference\n",
    "\n",
    "\\subsubsection{Prediction}\n",
    "\\[\n",
    "\\hat{Y} = \\hat{f}(X)\n",
    "\\]\n",
    "$\\hat{f}$ is the estimate for $f$, and $\\hat{Y}$ is the prediction for $Y$. \n",
    "\n",
    "Reducible error: systematic part\n",
    "\n",
    "Reducible error: error term\n",
    "\n",
    "\\[\n",
    "E(Y - \\hat{Y})^2 = E[f(X)+\\epsilon - \\hat{f}(X)]^2\n",
    "\\]\n",
    "\\[\n",
    "[f(X) - \\hat{f}(X)]^2 + \\textrm{Var}(\\epsilon)\n",
    "\\]\n",
    "\n",
    "\\subsubsection{Inference}\n",
    "Which predictors are associated with the response?\n",
    "\n",
    "What is the relationship between the response and each predictor?\n",
    "\n",
    "Can the relationship between Y and each predictor be adequately using a linear equation, or is the relationship more complicated?\n",
    "\n",
    "\\subsection{How do we estimate f?}\n",
    "Parametric or non-parametric\n",
    "\n",
    "\\subsubsection{Parametric methods}\n",
    "Parametric methods involve a two-step model-based approach\n",
    "\n",
    "1. Functional form or shape of f.\n",
    "Linear model\n",
    "\\[\n",
    "f(X) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_pX_p\n",
    "\\]\n",
    "\n",
    "2. Fit training data to model\n",
    "Ordinary least squares\n",
    "\n",
    "Fitting more flexible models may lead to overfitting the data\n",
    "\n",
    "\\subsubsection{Non-parametric methods}\n",
    "No assumption about the form of f is made\n",
    "\n",
    "Thin-plate spline\n",
    "\n",
    "\\subsection{Trade-off between prediction accuracy and model interpretability}\n",
    "\n",
    "\\begin{figure}\n",
    "    \\centering\n",
    "    \\includegraphics[width=1\\textwidth]{Pictures/MLflexibility.PNG}\n",
    "\\end{figure}\n",
    "\n",
    "Restrictive models are much more interpretable\n",
    "\n",
    "\\subsection{Measuring the quality of fit}\n",
    "Mean squared error (MSE)\n",
    "\\[\n",
    "MSE = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{f}(x_i))^2\n",
    "\\]\n",
    "\n",
    "We are interested in the accuracy of the predictions that we obtain when we apply our method to previously unseen test data\n",
    "\n",
    "Test data: calculate test MSE and evaluate the models\n",
    "\n",
    "No test observations available? \n",
    "\n",
    "Test MSE: U-shape property\n",
    "\n",
    "Cross-validation: method to evaluate the test MSE\n",
    "\n",
    "\\subsection{The bias-variance trade-off}\n",
    "The test MSE can always be decomposed into the sum of three fundamental qualities: varianc of $\\hat{f}(x_o)$, the squared bias of $\\hat{f}(x_0)$, and the variance of the error terms $\\epsilon$. \n",
    "\\[\n",
    "E\\bigg(y_0 - \\hat{f}(x_0) \\bigg)^2 = \\textrm{Var}(\\hat{f}(x_0)) + [Bias(\\hat{f}(x_0))]^2 + \\textrm{Var}(\\epsilon)\n",
    "\\]\n",
    "\n",
    "Averaging over all possible values of $x_0$ in the test set. \n",
    "\n",
    "Simultaneously achieving low variance and low bias. \n",
    "\n",
    "Variance: the amount by which $\\hat{f}$ would change if we estimated it using a different training data set\n",
    "\n",
    "Bias: error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model. More flexible methods result in less bias. \n",
    "\n",
    "Bias-variance trade-off: both low variance and low bias\n",
    "\n",
    "\\subsection{The classification setting}\n",
    "Estimation, the training error rate: the proportion of mistakes that are made \n",
    "if we apply our estimate $\\hat{f}$ to the training observations:\n",
    "\\[\n",
    "\\frac{1}{n}\\sum_{i=1}^n I(y_i \\neq \\hat{y}_i)\n",
    "\\]\n",
    "$\\hat{y}_i$ is the predicted class label for the $i$:th using $\\hat{f}$.\n",
    "Indicator variable, $I(y_i \\neq \\hat{y}_i) = 1$ if true and $0$ otherwise. \n",
    "\n",
    "Test error rare is \n",
    "\\[\n",
    "Ave(I(y_0 \\neq \\hat{y}_0))\n",
    "\\] \n",
    "\n",
    "\\subsubsection{The Bayes Classifier}\n",
    "Simple classifier, assigns each observation to the most likely class, given \n",
    "its predictor values. We should simply assign a test observation with predictor \n",
    "vector $x_0$ to the class $j$ for which\n",
    "\\[\n",
    "Pr(Y = j| X = x_0)\n",
    "\\]\n",
    "is the largest. \n",
    "\n",
    "The Bayes classifier produces the lowest possible test error rate, the Bayes \n",
    "error rate. The overall Bayes error rate is\n",
    "\\[\n",
    "1 - E\\Bigg( \\textrm{max}_j \\textrm{Pr}(Y = j|X) \\Bigg)\n",
    "\\]\n",
    "where the expectation averages the probability over all possible values of X.\n",
    "\n",
    "\\subsubsection{K-nearest neighbors}\n",
    "For real data, we do not know the conditional distribution of Y given X, and \n",
    "so computing the Bayes classifier is impossible.\n",
    "\n",
    "Given a positive integer $K$ and a test observation$x_0$, the KNN classifier identifies $K$ points in the training data that are closest to $x_0$, represented by $N_0$. Estimates the conditional probability for class $j$ as the fraction of points in $N_0$ whose response values equal $j$:\n",
    "\\[\n",
    "Pr(Y = j|X = x_0) = \\frac{1}{K}\\sum_{i \\in N_0} I(y_i = j)\n",
    "\\]\n",
    "KNN applies Bayes rule and classifies the test observation $x_0$ to the class with the largest probability.\n",
    "\n",
    "\\begin{figure}\n",
    "    \\centering\n",
    "    \\includegraphics[width=1\\textwidth]{Pictures/KNN.PNG}\n",
    "\\end{figure}\n",
    "\n",
    "KNN can often produce classifiers that are surprisingly close to the optimal Bayes classifier.\n",
    "\n",
    "The choice of $K$ has a drastic effect on the KNN classifier. As $K$ grows, the method becomes less flexible.\n",
    "\n",
    "\\section{Evaluation metrics: classification}\n",
    "True positive (TP): hits\n",
    "\n",
    "True enegative (TN): correct rejection\n",
    "\n",
    "False positive (FP): Type I error\n",
    "\n",
    "False negative (FN): Type II error\n",
    "\n",
    "True positive rate (TPR), sensitivity, recall:\n",
    "\\[\n",
    "TPR = \\frac{TP}{P} = \\frac{TP}{TP + FN} = 1 -FNR\n",
    "\\]\n",
    "True negative rate (TNR), specificity, selectivity:\n",
    "\\[\n",
    "TNR = \\frac{TN}{N} = 1 - FPR\n",
    "\\]\n",
    "Precision:\n",
    "\\[\n",
    "\\frac{TP}{TP+FP}\n",
    "\\]\n",
    "\n",
    "Some metrics are essentially defined for binary classification tasks ($f1_score, roc_auc_score$). In extending a binary metric to multiclass or multilabel problems, the data is treated as a collection of binary problems, one for each class. There are then a sumber of ways to average binary metric calculations across the set of classes:\n",
    "\\begin{enumerate}\n",
    "    \\item \"macro\": simply calculates the mean of the binary metrics, giving equal weight to each class.\n",
    "    \\item \"weighted\": accounts for class imbalance by computing the average of binary metrics in which each class' score is weighted by its presence in the true data. \n",
    "    \\item \"micro\": gives each sample-class pair an equal contribution to the overall metric. \n",
    "    \\item \"samples\": applies only to multilabel problems. \n",
    "\\end{enumerate}\n",
    "\n",
    "\\subsection{Confusion matrix}\n",
    "\\begin{minted}[breaklines]{python}\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = logreg.predict(X_test)\n",
    "confusion_matrix(y_true, y_pred)\n",
    "\\end{minted}\n",
    "\n",
    "\\subsection{Accuracy score}\n",
    "Accuracy score: the number of correct predictions/total predictions\n",
    "\n",
    "If $\\hat{y}_i$ is the predicted value of the $i$-th sample and $y_i$ is the corresponding true value, then the fraction of correct prediction sover $n$ samples is defined as\n",
    "\\[\n",
    "accuracy(y, \\hat{y}) = \\frac{1}{n}\\sum_{i=1}^n 1(\\hat{y}_i = y_i)\n",
    "\\]\n",
    "where $1(x)$ is the indicator function. \n",
    "\n",
    "\\begin{minted}[breaklines]{python}\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = [0, 2, 1, 3]\n",
    "y_true = [0, 1, 2, 3]\n",
    "accuracy_score(y_true, y_pred)\n",
    "\\end{minted}\n",
    "\n",
    "\\subsection{Balanced accuracy score}\n",
    "Balanced accuracy avoids inflated performance estimates on imbalanced datasets. In the binary case, balanced accuracy is equal to the arithmetic mean of sensitivity (true positive rate) and specificity (true negative rate), or the area under the ROC cruve with binary predictions rather than scores.\n",
    "\n",
    "If $y_i$ is the true value of the $i$-th sample, and $w_i$ is the corresponding sample weight, then we adjust the sample weight to:\n",
    "\\[\n",
    "\\hat{w}_i = \\frac{w_i}{\\sum_j 1(y_j = y_j)w_j}\n",
    "\\]\n",
    "where $1(x)$ is the indicator function. Given predicted $\\hat{y}_i$ for sample $i$, balanced accuracy is defined as:\n",
    "\\[\n",
    "\\frac{1}{\\sum_{\\hat{w}_i}}\\sum_i1(\\hat{y}_i = y_i)\\hat{w}_i\n",
    "\\]\n",
    "\n",
    "\\subsection{Hamming loss}\n",
    "If $\\hat{y}_j$ is the predicted value for the $j$-th label of a given sample, $y_j$ is the corresponding true value, and $n_{labels}$ is the number of classes or labels, then the Hamming loss between two samples is \n",
    "\\[\n",
    "\\frac{1}{n_{labels}} \\sum_{j=1}^{n_{labels}}1(\\hat{y}_j \\neq y_j)\n",
    "\\]\n",
    "where $1(x)$ is the indicator function.\n",
    "\n",
    "\\begin{minted}[breaklines]{python}\n",
    "from sklearn.metrics import hamming_loss\n",
    "y_pred = [1, 2, 3, 4]\n",
    "y_true = [2, 2, 3, 4]\n",
    "hamming_loss(y_true, y_pred)\n",
    "\\end{minted}\n",
    "\n",
    "\n",
    "\\subsection{ROC curve and AUC}\n",
    "The Receiver Operating Characteristic (ROC) metric is a method to evaluate classifier output quality. ROC curves typically feature true positive rate on the Y axis, and false positive rate on the X axis. This means that a larger area under the curve (AUC) is usually better. The steepness of ROC curves is also important, since it is ideal to maximize the true positive rate while minimizing the false positive rate. \n",
    "\n",
    "ROC curves are typically used in binary classification to study the output of a classifier. In order ti extend the ROC curve and ROC area to multilabel classification, it is necessary to binarize the output. \n",
    "\n",
    "Binary outcome:\n",
    "\\begin{minted}[breaklines]{python}\n",
    "from sklearn.metric import roc_curve\n",
    "y_pred-prob = logreg.predict_proba(X_test)[:,1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\\end{minted}\n",
    "\n",
    "Multiclass-outcome\n",
    "\\begin{minted}[breaklines]{python}\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "\n",
    "#Plot of ROC curve for a specific class    \n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr[2], tpr[2], color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()  \n",
    "\\end{minted}\n",
    "\n",
    "\\subsection{Binary cross-entropy/log-loss}\n",
    "For binary classification, the typical loss function is the binary cross-entropy/log loss. \\[\n",
    "H_p(q) = -\\frac{1}{N}\\sum_{i=1}^N y_i \\cdot \\log(p(y_i)) + (1-y_i)\\cdot\\log(1 - p(y_i))\n",
    "\\]\n",
    "where $y$ is the label and $p(y)$ is the predicted probability of the point being 1 or Yes for all $N$ points.\n",
    "\n",
    "Since we're trying to compute a loss, we need to penalize bad predictions. If the probability associated with the true class is 1.0, we need its loss to be zero. Conversely, if that probability is low, 0.01, we need its loss to be huge. It turn out that taking the (negative) log of the probability suits us well enough for this purpose. \n",
    "\n",
    "We can compute the entropy of a distribution, like our $q(y)$, using the formula below, where $C$ is the number of classes:\n",
    "\\[\n",
    "H(q) = - \\sum_{c=1}^Cq(y_c)\\cdot\\log(q(y_c))\n",
    "\\]\n",
    "We can try to approximate the true distribution with some other distribution, say $p(y)$. If we compute entropy like this, we are actually computing the cross-entropy between both distributions:\n",
    "\\[\n",
    "H_p(q) = -\\sum_{c=1}^Cq(y_c)\\cdot\\log(p(y_c))\n",
    "\\]\n",
    "Cross-entropy will have a bigger value than the entropy computed on the true distribution. The difference between cross-entropy and entropy is the Kullback-Leibler divergence:\n",
    "\\[\n",
    "D_{KL}(q\\|p) = H_p(q) - H(q) = \\sum_{c=1}^C q(y_c) \\cdot [\\log(q(y_c)) - \\log(p(y_c))]\n",
    "\\]\n",
    "The classifier \n",
    "\n",
    "\\begin{minted}[breaklines]{python}\n",
    "from sklearn.metrics import log_loss\n",
    "y_pred_prob = logreg.predict_proba(X_test)[:,1]\n",
    "log_loss(y_true, y_pred_prob)\n",
    "\\end{minted}\n",
    "\n",
    "\\subsection{Brier score}\n",
    "The smaller the Brier score, the better. Across all items in a set $N$ predictions, the Brier score measures the mean squared difference between the predicted probability assigned to the possible outcomes for item $i$, and the actual outcome (0 or 1). \n",
    "\n",
    "\\begin{minted}[breaklines]{python}\n",
    "from sklearn.metrics import brier_score_loss\n",
    "y_true = np.array([0,1,1,0])\n",
    "y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n",
    "brier_score_loss(y_true, y_prob)\n",
    "\\end{minted}\n",
    "\n",
    "\\subsection{Dscounted cumulative gain}\n",
    "Compute discounted cumulative gain\n",
    "\n",
    "Sum the true scores ranked in order induced by the predicted scores, after applying a logarithmic discount.\n",
    "\n",
    "\\begin{minted}[breaklines]{python}\n",
    "from sklearn.metrics import dcg_score\n",
    "y_true = np.array([0,1,1,0])\n",
    "y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n",
    "dcg_score(y_true, y_prob)\n",
    "\\end{minted}\n",
    "\n",
    "\\subsection{F1 score}\n",
    "The F1 score can be interpreted as a weighted average of the precision and recall, where F1 score reaches its best value at 1 anf worst score at 0. The relative contribution of precision and recall to the F1 score are equal. The formula for the F1 score is:\n",
    "\\[\n",
    "F1 = \\frac{2(P + R)}{P+R}\n",
    "\\]\n",
    "\n",
    "\\begin{minted}[breaklines]{python}\n",
    "from sklearn.metrics import f1_score\n",
    "y_true = [0, 1, 2, 0, 1, 2]\n",
    "y_pred = [0, 2, 1, 0, 0, 1]\n",
    "f1_score(y_true, y_pred, average='macro')\n",
    "\\end{minted}\n",
    "\n",
    "\\subsection{Precision-Recall}\n",
    "The precision-recall metric is a method to evaluate the classifier output quality. Prec ision-recall is a useful measure of success of prediction when the classes are very imbalanced. A high area under the curve represents both high recall and high precision, where high precision relates to low false positive rate, and high recall relates to low false negative rate. A system with high recall but low precision return many results, but most of its predicted labels are incorrect when compared to the training labels. A system with high precision but low recall returns very few results, but most of its predicted labels are correct when compared to the training labels. \n",
    "\n",
    "Precision $P$ is defined as the number of true positives $T_p$ over the number of true positives plus the number of false positives $F_p$. \n",
    "\\[\n",
    "P = \\frac{T_p}{T_p + F_p}\n",
    "\\]\n",
    "Recall $R$ is defined as the number of true positives $T_p$ over the number of true positives plus the number of false negatives $F_n$:\n",
    "\\[\n",
    "R = \\frac{T_p}{T_p + F_n}\n",
    "\\]\n",
    "\n",
    "Average precision (AP) summarizes the precision-recall plot as the weighted mean of precisions achieved at each threshold, with the increase in recall from the previous threshold used as the weight:\n",
    "\\[\n",
    "AP = \\sum_n (R_n - R_{n-1})P_n\n",
    "\\]\n",
    "where $P_n$ and $R_n$ are the precision and recall at the nth threshold. \n",
    "\n",
    "\\begin{minted}[breaklines]{python}\n",
    "from sklearn.metric import precision_recall_curve\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "\n",
    "plot_precision-recall_curve(classifier, X_test, y_test)\n",
    "\n",
    "y_score = logreg.pred_proba(X_test)[:,1]\n",
    "\n",
    "average_precision = average_precision_score(y_test, y_score)\n",
    "\\end{minted}\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
