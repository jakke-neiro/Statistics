{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\section{Markov chain Monte Carlo basics}\n",
    " When using MCMC methods, we estimate the posterior distribution and the intractable integrals using simulated samples from the posterior distribution.\n",
    "\n",
    "\\subsection{Markov chains}\n",
    "\\begin{Definition}\n",
    "A Markov chain is a discrete-time stochastic process $\\theta_1, \\theta_2, ..., \\theta_n$ that satisfies the Markov property:\n",
    "\\[\n",
    "p(\\theta_t | \\theta_1, ..., \\theta_{t-1}) = p( \\theta_t | \\theta_{t-1} \n",
    "\\]\n",
    "\n",
    "\\end{Definition}\n",
    "\n",
    "\n",
    "\\subsection{Markov chain convergence theory}\n",
    "MCMC we will need to focus on Markov chains that converge to a stationary distribution\n",
    "\n",
    "\\begin{Definition}\n",
    "Irreducible: any state can be reached from any other state with a positive probability in a finite number of steps\n",
    "\\end{Definition}\n",
    "\n",
    "\\begin{Definition}\n",
    "Periodic with period $k$: the number of steps required to return to the state is always visible by $k$\n",
    "\\end{Definition}\n",
    "\n",
    "\\begin{Definition}\n",
    "Ergodic: the expected return time to every state is finite.\n",
    "\\end{Definition}\n",
    "\n",
    "\\begin{Definition}\n",
    "Stationary: $p(\\theta_{t+1}, ..., \\theta{t+k})$ does not depend on $t$\n",
    "\\end{Definition}\n",
    "\n",
    "\\begin{Definition}\n",
    "Reversible: $p(\\theta_{t}, ..., \\theta{t+1})$ does not depend on $t$. A reversible Markov chain is always stationary.\n",
    "\\end{Definition}\n",
    "\n",
    "\\begin{Definition}\n",
    "Markov chain with transition kernel T satisfies the detailed balance condition if there exists a distribution $\\pi(\\theta)$ such that\n",
    "\\[\n",
    "\\pi(\\theta) T(\\theta, \\theta') = \\pi( \\theta') T (\\theta', \\theta)\n",
    "\\]\n",
    "\\end{Definition}\n",
    "\n",
    "\\subsection{Metropolis-Hastings algorithm}\n",
    "Metropolis-Hastings sampling works by deriving a Markov chain $(\\theta^{(1)}, \\theta^{(2)}, ...)$ whose stationary distribution is the target $\\pi(\\theta)$. A new state $\\theta '$ from a proposal density $q(\\theta '; \\theta^{(t)})$. \n",
    "\\[\n",
    "a = \\frac{\\pi(\\theta ')}{\\pi (\\theta^{t})}\\frac{q(\\theta^{(t)} ; \\theta ')}{q(\\theta ' ; \\theta^{(t)})}\n",
    "\\]\n",
    "if $a \\leq 1$, then the new state is always accepted\n",
    "\n",
    "Otherwise, the new state is accepted with probability $a$.\n",
    "\n",
    "If accepted $\\theta^{(t+1)} = \\theta '$\n",
    "\n",
    "If rejected, then we set $\\theta^{(t+1)} = \\theta^{(t)}$\n",
    "\n",
    "If the proposal $q$ is symmetric, then:\n",
    "\\[\n",
    "a = \\frac{\\pi(\\theta ')}{\\pi(\\theta^{(t)}}\n",
    "\\]\n",
    "\n",
    "\\subsection{Implementation of the Metropolis-Hastings algorithm}\n",
    "\n",
    "Draw samples from the Laplace distribution $\\pi^* (\\theta) = \\exp (- |\\theta| )$, normal proposal $q(\\theta ' ; \\theta) = N(\\theta '; \\theta, 1^2)$\n",
    "\n",
    "Logs of acceptance probability:\n",
    "\\[\n",
    "\\log (a) = \\log \\pi (\\theta ') - \\log \\pi (\\theta^{(t)}) + \\log q(\\theta^{(t)} ; \\theta ') - \\log q(\\theta '; \\theta^{(t)})\n",
    "\\]\n",
    "if $q$ is symmetric \n",
    "\\[\n",
    "\\log (a) = \\log \\pi (\\theta ') - \\log \\pi (\\theta^{(t)})\n",
    "\\]\n",
    "\n",
    "\n",
    "\\begin{minted}[breaklines]{Python}\n",
    "#We use logarithms of distributions\n",
    "\n",
    "#input x, output -np.abs(x)\n",
    "#the same as\n",
    "#def target(x):\n",
    "#\treturn -np.abs()x\n",
    "target = lambda x: -np.abs(x)\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "# Metropolis sampling (symmetric proposal) for given \n",
    "#log-target distribution\n",
    "def mhsample0(x0, n, logtarget, drawproposal):\n",
    "    x = x0\n",
    "    xs = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        x_prop = drawproposal(x)\n",
    "        if np.log(npr.rand()) < logtarget(x_prop) - logtarget(x):\n",
    "            x = x_prop\n",
    "        xs[i] = x\n",
    "    return xs\n",
    "\n",
    "# Testing Laplace target with normal proposal centred \n",
    "#around the previous value\n",
    "npr.seed(42)\n",
    "x = mhsample0(0.0, 10000, lambda x: -np.abs(x), lambda x: x+npr.normal())\n",
    " \n",
    "# Discard the first half of samples as warm-up\n",
    "x = x[len(x)//2:]\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "#Slicing, every 10th element is plotted\n",
    "ax[0].plot(x[::10])\n",
    "h = ax[1].hist(x[::10], 50, normed=True)\n",
    "plt.show()\n",
    "\n",
    "\\end{minted}\n",
    "\n",
    "\\subsection{MCMC sampling efficiency}\n",
    "The efficiency of an MCMC sampler depends on the proposal $q(\\theta '; \\theta)$. \n",
    "\n",
    "\\begin{minted}[breaklines]{Python}\n",
    "# Metropolis sampling (symmetric proposal) for given log-target distribution\n",
    "#The sampler prints the acceptance rate\n",
    "def mhsample1(x0, n, logtarget, drawproposal):\n",
    "    x = x0\n",
    "    xs = np.zeros(n)\n",
    "    accepts = 0\n",
    "    for i in range(n):\n",
    "        x_prop = drawproposal(x)\n",
    "        if np.log(npr.rand()) < logtarget(x_prop) - logtarget(x):\n",
    "            x = x_prop\n",
    "            accepts += 1\n",
    "        xs[i] = x\n",
    "    print(\"Sampler acceptance rate:\", accepts/n)\n",
    "    return xs\n",
    " \n",
    "# Testing Laplace target with a narrow normal proposal\n",
    "x = mhsample1(0.0, 10000, lambda x: -np.abs(x), lambda x: x+0.1*npr.normal())\n",
    " \n",
    "# Discard the first half of samples as warm-up\n",
    "\n",
    "x = x[len(x)//2:]\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].plot(x[::10])\n",
    "h = ax[1].hist(x[::10], 50, normed=True)\n",
    "plt.show()\n",
    "\n",
    "# Testing Laplace target with a well-tuned normal proposal\n",
    "x = mhsample1(0.0, 10000, lambda x: -np.abs(x), lambda x: x+2.5*npr.normal())\n",
    " \n",
    "# Discard the first half of samples as warm-up\n",
    "\n",
    "x = x[len(x)//2:]\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].plot(x[::10])\n",
    "h = ax[1].hist(x[::10], 50, normed=True)\n",
    "plt.show()\n",
    "\n",
    "# Testing Laplace target with a wide normal proposal\n",
    "x = mhsample1(0.0, 10000, lambda x: -np.abs(x), lambda x: x+50*npr.normal())\n",
    " \n",
    "# Discard the first half of samples as warm-up\n",
    "\n",
    "x = x[len(x)//2:]\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].plot(x[::10])\n",
    "h = ax[1].hist(x[::10], 50, normed=True)\n",
    "plt.show()\n",
    "\\end{minted}\n",
    "\n",
    "Too narrow proposals: very high acceptance rates and biased samples\n",
    "Low acceptance rates: spikes in the histogram of the samples\n",
    "\n",
    "Acceptance rate for MH: 50\\%\n",
    "\n",
    "Sampling multimodal distributions with MCMC is difficult!\n",
    "\n",
    "\\subsection{Initial best practices}\n",
    "Initial warm-up are discarded!\n",
    "\n",
    "\n",
    "\\subsection{Exercises}\n",
    "\\subsubsection{Metropolis-Hastings sampling of a 1D target}\n",
    "1. Write a Metropolis-Hastings sampler to draw samples from the standard zero mean unit variance normal distribution  using the distribution as the proposal. Draw 1000 samples and compute the acceptance rate of your sampler.\n",
    "\n",
    "2. Plot a normed histogram of the samples you have drawn together with the true density.\n",
    "\n",
    "3. Plot a trace plot of the samples: a line curve of the samples in order. Do consecutive samples appear independent or are they correlated? Plot a similar curve of the samples when they are permuted to a random order. Do the curves look simiar?\n",
    "\n",
    "4. Try increasing the number of samples to 10000 and only including every 10th sample in the plots. This is called thinning. (Hint: you can extract every 10th element of a numpy.array x using x[::10].)\n",
    "\n",
    "5. Try changing the bounds of the proposal distribution such that they remain symmetric about 0, run the sampler and redraw the plots. Can you make the samples appear more independent? Is there a connection between the acceptance rate and the appearance of the plots?\n",
    "\\begin{minted}[breaklines]{Python}\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#1d symmetric MH sampler\n",
    "def msample(x0, n, target, drawproposal):\n",
    "    x = x0\n",
    "    accepts = 0\n",
    "    lp = target(x)\n",
    "    xs = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        x_prop = drawproposal(x)\n",
    "        l_prop = target(x_prop)\n",
    "        if np.log(npr.rand()) < l_prop - lp:\n",
    "            x = x_prop\n",
    "            lp = l_prop\n",
    "            accepts += 1\n",
    "        xs[i] = x\n",
    "    print('Acceptance rate:', accepts/n)\n",
    "    return xs\n",
    "\n",
    "# Note that now Q(x',x^(t)) = Q(x^(t), x') so we do not need to include Q(x'; x) term for the acceptance probability\n",
    "x = msample(0.0, 10000, lambda x: -0.5*x**2, lambda x: x+10*(npr.rand()-0.5))\n",
    "x = x[len(x)//2:]\n",
    "h = plt.hist(x[::10], 50, normed=True)\n",
    "plt.show()\n",
    "plt.plot(x[::10])\n",
    "\\end{minted}\n",
    "\n",
    "\\subsubsection{Tuning a Metropolis-Hastings sampler}\n",
    "Implement the sampler and run it 10 times with a number of different values of sigma . Record the acceptance rates and the mean and variance of the samples you obtain. What value of  sigma  gives the most accurate \n",
    "\n",
    "\\begin{minted}[breaklines]{Python}\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# return acceptance rate to avoid too many prints\n",
    "def msample(x0, n, target, drawproposal):\n",
    "    x = x0\n",
    "    accepts = 0\n",
    "    lp = target(x)\n",
    "    xs = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        x_prop = drawproposal(x)\n",
    "        l_prop = target(x_prop)\n",
    "        if np.log(npr.rand()) < l_prop - lp:\n",
    "            x = x_prop\n",
    "            lp = l_prop\n",
    "            accepts += 1\n",
    "        xs[i] = x\n",
    "    #print('Acceptance rate:', accepts/n)\n",
    "    return xs, accepts/n\n",
    "\n",
    "def sample_normal(nsamples, repeats, drawproposal):\n",
    "    means = np.zeros(repeats)\n",
    "    stds = np.zeros(repeats)\n",
    "    accepts = np.zeros(repeats)\n",
    "    for i in range(repeats):\n",
    "        x, accepts[i] = msample(0.0, 10000, lambda x: -0.5*x**2, drawproposal)\n",
    "        # Drop warm-up samples\n",
    "        x = x[len(x)//2:]\n",
    "        means[i] = np.mean(x)\n",
    "        stds[i] = np.std(x)\n",
    "    return means, stds, np.mean(accepts)\n",
    "\n",
    "def check_errors(means, stds, truemean=0.0, truestd=1.0):\n",
    "    return (np.sqrt(np.mean((means-truemean)**2)), np.sqrt(np.mean((stds-truestd)**2)))\n",
    "\n",
    "npr.seed(42)\n",
    "widths = [0.1, 0.3, 1.0, 3.0, 10.0, 30.0]\n",
    "\n",
    "for w in widths:\n",
    "    means, stds, accepts = sample_normal(10000, 10, lambda x: x+w*npr.normal())\n",
    "    meanerr, stderr = check_errors(means, stds)\n",
    "    print('normal width %.1f' % w, 'acceptance rate', accepts,\n",
    "          'mean RMSE', meanerr, 'std RMSE', stderr)\n",
    "\n",
    "for w in widths:\n",
    "    means, stds, accepts = sample_normal(10000, 10, lambda x: x+w*(npr.random()-0.5))\n",
    "    meanerr, stderr = check_errors(means, stds)\n",
    "    print('uniform width %.1f' % w, 'acceptance rate', accepts,\n",
    "          'mean RMSE', meanerr, 'std RMSE', stderr)\n",
    "\n",
    "\\end{minted}\n",
    "\n",
    "\\subsubsection{Metropolis-Hastings sampling of a higher dimensional target}\n",
    "Implement a Metropolis-Hastings sampler for multivariate targets and test it for sampling from standard zero-mean unit covariance  dimensional multivariate normal   using as the proposal . Draw 10000 samples and thin by a factor of 10.\n",
    "Plot trace plots, histograms and pairwise scatter plots of all pairs of variables.\n",
    "\n",
    "\\begin{minted}[breaklines]{Python}\n",
    "#1\n",
    "#MH sampler multidimensional symmetric\n",
    "def d_msample(x0, n, target, drawproposal):\n",
    "    x = x0\n",
    "    d = len(x0)\n",
    "    accepts = 0\n",
    "    lp = target(x)\n",
    "    xs = np.zeros([n, d])\n",
    "    for i in range(n):\n",
    "        x_prop = drawproposal(x)\n",
    "        l_prop = target(x_prop)\n",
    "        if np.log(npr.rand()) < l_prop - lp:\n",
    "            x = x_prop\n",
    "            lp = l_prop\n",
    "            accepts += 1\n",
    "        xs[i] = x\n",
    "    print('Acceptance rate:', accepts/n)\n",
    "    return xs\n",
    "\n",
    "def lmvunitnormpdf(x, d=len(x)):\n",
    "    return -0.5*d*np.log(2*np.pi)-0.5*np.sum(x**2)\n",
    "\n",
    "target = lambda x: lmvunitnormpdf(x)\n",
    "# generate a matrix of N(0,1) distributed numbers with same shape as x\n",
    "propose_draws = lambda x, w: x + w * npr.randn(*x.shape)\n",
    "\n",
    "w=0.5\n",
    "d = 2\n",
    "x = d_msample(np.zeros(d), 10000, target, lambda x: propose_draws(x, w))\n",
    "x = x[len(x)//2:]\n",
    "\n",
    "#2\n",
    "plt.hist2d(x[::10, 0],x[::10, 1], 50, normed=True)\n",
    "plt.title('w = {}, d = {}'.format(w, d))\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x[::10])\n",
    "plt.title('w = {}, d = {}'.format(w, d))\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(x[::10, 0],x[::10, 1], marker = '.')\n",
    "plt.title('w = {}, d = {}'.format(w, d))\n",
    "plt.show()\n",
    "\n",
    "#3\n",
    "w = 10\n",
    "x = d_msample(np.zeros(d), 10000, target, lambda x: propose_draws(x, w))\n",
    "x = x[len(x)//2:]\n",
    "\n",
    "plt.hist2d(x[::10, 0],x[::10, 1], 50, normed=True)\n",
    "plt.title('w = {}, d = {}'.format(w, d))\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x[::10])\n",
    "plt.title('w = {}, d = {}'.format(w, d))\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(x[::10, 0],x[::10, 1], marker = '.')\n",
    "plt.title('w = {}, d = {}'.format(w, d))\n",
    "plt.show()\n",
    "\n",
    "#4\n",
    "w = 0.5\n",
    "d = 5\n",
    "x = d_msample(np.zeros(d), 10000, target, lambda x: propose_draws(x, w))\n",
    "x = x[len(x)//2:]\n",
    "\n",
    "plt.plot(x[::10])\n",
    "plt.title('w = {}, d = {}'.format(w, d))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "w = 10\n",
    "d = 5\n",
    "x = d_msample(np.zeros(d), 10000, target, lambda x: propose_draws(x, w))\n",
    "x = x[len(x)//2:]\n",
    "\n",
    "plt.plot(x[::10])\n",
    "plt.title('w = {}, d = {}'.format(w, d))\n",
    "plt.show()\n",
    "\\end{minted}\n",
    "\n",
    "\\subsubsection{Sampling a multimodal distribution, convergence checking}\n",
    "1. Plot a contour plot \n",
    "\n",
    "2. Write a Metropolis-Hastings sampler for the target using  \n",
    "\n",
    "3. Plot a scatter plot of the samples and compare it to the contour plot. Is the sampler exploring the density well?\n",
    "\n",
    "4. Start several chains from different initial values and compare the results. Do the chains converge?\n",
    "\n",
    "5. Try repeating the experiment.\n",
    "\n",
    "6. Try repeating the experiment using the same initial points\n",
    "\n",
    "7. Try to get the sampler to explore the density. Try running the sampler for longer, changing the proposal.\n",
    "\n",
    "\\begin{minted}[breaklines]{Python}\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc as scm\n",
    "\n",
    "#Multidimensional MH sampler\n",
    "def d_msample(x0, n, target, drawproposal):\n",
    "    x = x0\n",
    "    d = len(x0)\n",
    "    accepts = 0\n",
    "    lp = target(x)\n",
    "    xs = np.zeros([n, d])\n",
    "    for i in range(n):\n",
    "        x_prop = drawproposal(x)\n",
    "        l_prop = target(x_prop)\n",
    "        if np.log(npr.rand()) < l_prop - lp:\n",
    "            x = x_prop\n",
    "            lp = l_prop\n",
    "            accepts += 1\n",
    "        xs[i] = x\n",
    "    print('Acceptance rate:', accepts/n)\n",
    "    return xs\n",
    "\n",
    "def target(x, R):\n",
    "    means = np.array([[0, 0],[R, R],[-R,R],[R,-R],[-R,-R]])\n",
    "    K = means.shape[0]\n",
    "    logp = -0.5*np.sum((x[np.newaxis,:]-means)**2, 1)\n",
    "    return scm.logsumexp(logp)\n",
    "\n",
    "#Countour plotting\n",
    "R = 6.0\n",
    "x = np.linspace(-10, 10, 100)\n",
    "y = np.linspace(-10, 10, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = np.zeros(X.shape)\n",
    "for i, x_ in enumerate(x):\n",
    "    for j, y_ in enumerate(y):\n",
    "        Z[i,j] = target(np.array([x_, y_]), R)\n",
    "        \n",
    "plt.contour(X, Y, Z)\n",
    "\n",
    "R = 6.0\n",
    "\n",
    "x = d_msample(np.array([0.0, 0.0]), 10000, lambda x: target(x, R), lambda x: x+npr.randn(2))\n",
    "x = x[len(x)//2:]\n",
    "plt.plot(x[:,0], x[:,1], '.')\n",
    "\n",
    "R = 3.0\n",
    "\n",
    "x = d_msample(np.array([0.0, 0.0]), 10000, lambda x: target(x, R), lambda x: x+npr.randn(2))\n",
    "x = x[len(x)//2:]\n",
    "plt.plot(x[:,0], x[:,1], '.')\n",
    "\n",
    "R = 100.0\n",
    "\n",
    "x = d_msample(np.array([0.0, 0.0]), 10000, lambda x: target(x, R), lambda x: x+npr.randn(2))\n",
    "x = x[len(x)//2:]\n",
    "plt.plot(x[:,0], x[:,1], '.')\n",
    "\n",
    "R = 6.0\n",
    "\n",
    "x = d_msample(np.array([0.0, 0.0]), 10000, lambda x: target(x, R), lambda x: x+npr.randn(2))\n",
    "x = x[len(x)//2:]\n",
    "plt.plot(x[:,0], x[:,1], '.')\n",
    "\n",
    "R = 6.0\n",
    "\n",
    "npr.seed(42)\n",
    "# Increase step length and iteration count\n",
    "x = d_msample(np.array([0.0, 0.0]), 100000, lambda x: target(x, R), lambda x: x+2*npr.randn(2))\n",
    "x = x[len(x)//2:]\n",
    "plt.plot(x[:,0], x[:,1], '.')\n",
    "\\end{minted}\n",
    "\n",
    "\\section{Bayesian inference with MCMC}\n",
    "The most important use of MCMC sampling in statistics is in Bayesian inference and drawing samples from the posterior distribution\n",
    "\n",
    "\\subsection{Bayesian inference}\n",
    "Given the observations $D$, a model $p(D | \\theta)$ with parameters $\\theta$, and prior probability $p(\\theta)$ of the parameters, the posterior probability distribution is \n",
    "\\[\n",
    "p(\\theta | D) = \\frac{p(D | \\theta) p(\\theta)}{p(D)} = \\frac{p(D | \\theta) p(\\theta)}{\\int_{\\theta}p(D|\\theta)p(\\theta)d\\theta}\n",
    "\\]\n",
    "However, the integral or the marginal likelihood is hard to compute, and cannot be used directly. But in terms of the posterior distribution, it is just a scalar normaliser.\n",
    "\n",
    "If the used prior is conjugate to the likelihood, then the posterior will have the same functional form. \n",
    "\n",
    "Let us consider Bayesian inference of the mean $\\mu$ of normally distributed data $D=(x_1, ..., x_n)$ with known variance $\\sigma_x^2$.\n",
    "\n",
    "Assuming the observations are independent \n",
    "\\[\n",
    "p(D | \\theta) = \\prod_{i=1}^n p(x_i|\\mu) = \n",
    "\\prod_{i=1}^n N(x_i; \\mu, \\sigma_x^2)\n",
    "\\]\n",
    "\n",
    "We use a normal prior\n",
    "\\[\n",
    "p(\\mu) = N(\\mu; \\mu_0, \\sigma_0^2)\n",
    "\\]\n",
    "\n",
    "Then we get \n",
    "\\[\n",
    "p(\\mu | D) \\propto \\prod_{i=1}^n N(x_i; \\mu, \\sigma_x^2) N(\\mu; \\mu_0, \\sigma_0^2)\n",
    "\\]\n",
    "\n",
    "\\subsection{Sampling the posterior with MCMC}\n",
    "Metropolis-Hastings algorithm can work with an unnormalised density\n",
    "\n",
    "We define the target as\n",
    "\\[\n",
    "\\log \\pi ^* (\\theta) = \\log p(D | \\mu) + \\log p(\\mu)\n",
    "\\]\n",
    "\n",
    "And use the normal proposal $q(\\theta'; \\theta) = N(\\theta'; \\theta, \\sigma_{prop}^2$\n",
    "\n",
    "\\begin{minted}{Python}\n",
    "# Define the normal pdf. Note parameters: mean, standard deviation\n",
    "# Note the sum to allow evaluation for a data set at once\n",
    "def lnormpdf(x, mu, sigma):\n",
    "    return np.sum(-0.5*np.log(2*np.pi) - np.log(sigma) - \n",
    "    0.5 * (x-mu)**2/sigma**2)\n",
    " \n",
    "# Define the target log-pdf as a sum of likelihood and prior terms\n",
    "def target(mu, data, sigma_x, mu0, sigma0):\n",
    "    return lnormpdf(data, mu, sigma_x) + lnormpdf(mu, mu0, sigma0)\n",
    " \n",
    "# Simulate n=100 points of normally distributed data about mu=0.5\n",
    "n = 100\n",
    "data = 0.5 + npr.normal(size=n)\n",
    " \n",
    "# Set the prior parameters\n",
    "sigma_x = 1.0\n",
    "mu0 = 0.0\n",
    "sigma0 = 3.0\n",
    " \n",
    "# Run the sampler\n",
    "x = mhsample1(0.0, 10000,\n",
    "              lambda mu: target(mu, data, sigma_x, mu0, sigma0),\n",
    "              lambda x: x+0.2*npr.normal())\n",
    " \n",
    "# Discard the first half of samples as warm-up\n",
    "x = x[len(x)//2:]\n",
    "# Plot a histogram of the posterior samples\n",
    "plt.hist(x, 30, normed=True)\n",
    "# Compare with analytical result from the above example\n",
    "tt = np.linspace(0.1, 0.9, 50)\n",
    "m_post = sigma0**2 * np.sum(data) / (n*sigma0**2 + sigma_x**2)\n",
    "s2_post = 1/(n/sigma_x**2 + 1/sigma0**2)\n",
    "y = np.array([np.exp(lnormpdf(t, m_post, np.sqrt(s2_post))) for t in tt])\n",
    "plt.plot(tt, y)\n",
    "plt.show()\n",
    "\\end{minted}\n",
    "\n",
    "\\subsection{Sampling bounded variables}\n",
    "Many probabilistic models contain variables that have some constraints: variances need to be positive and point probabilities need to be in the interval [0,1].\n",
    "\n",
    "Variables $\\theta_I$ with bounds $\\theta_I \\in B$\n",
    "\n",
    "\\subsubsection{Zero likelihood approach}\n",
    "Convergence to the correct distribution: $P^*(\\theta) = 0$, when $\\theta_I$ not in $B$. \n",
    "\n",
    "\\subsubsection{Proposal that respects the bounds}\n",
    "$\\theta_i > 0$. One way to enforce the constraint is to propose $\\theta ' = |\\theta_0'|$, where $\\theta_0'$ follows some proposals $q_0(\\theta_0'; \\theta)$. \n",
    "\n",
    "\\subsubsection{Transformations to unbounded variables}\n",
    "Samplers for bounded variables can also be implemented using a change of variables through a transformation to unbounded variables.\n",
    "\n",
    "\\begin{enumerate}\n",
    "    \\item If $\\theta_i > 0$, choose $g(\\phi) = \\exp(\\theta)$\n",
    "    \\item If $\\theta_i \\in (0,1)$, choose $g(\\phi) = \\frac{1}{(1 + \\exp(-\\phi))}$: the logistic transformation\n",
    "    \\item If $\\theta_i \\in (0,1), i \\in I$ with $\\sum_{i \\in I} \\theta_i = 1$, $g_i(\\phi) = \\frac{\\exp(\\phi_i)}{\\sum_{i' \\in I} \\exp (\\phi_{i'}}$\n",
    "\\end{enumerate}\n",
    "\n",
    "\\begin{minted}{Python}\n",
    "import scipy.integrate as si\n",
    "import numpy as np\n",
    "\n",
    "#g(phi) = exp(phi)\n",
    "\n",
    "def p_theta(theta, l):\n",
    "    return l * np.exp(-l * theta)\n",
    "    \n",
    "\n",
    "def p_phi(phi, l):\n",
    "    return l * np.exp(-l * np.exp(phi)) * np.exp(phi)\n",
    "    \n",
    "\n",
    "# Note the different bounds for the integrals!\n",
    "print('p_theta normaliser:', si.quad(lambda x: p_theta(x, 2.0), 0, np.inf))\n",
    "\n",
    "print('p_phi normaliser:  ', si.quad(lambda x: p_phi(x, 2.0), -100, 100))\n",
    "# Note: setting bounds to infinity would give an error, but\n",
    "# this range is sufficient in practice\n",
    "\n",
    "# Note the different bounds for the integrals!\n",
    "print('p_theta probability:', si.quad(lambda x: p_theta(x, 2.0), \n",
    "1, np.exp(1)))\n",
    "\n",
    "print('p_phi probability:  ', si.quad(lambda x: p_phi(x, 2.0), 0, 1))\n",
    "\\end{minted}\n",
    "\n",
    "\\subsection{Exercises}\n",
    "\\subsubsection{Metropolis-Hastings sampling for the posterior distribution of a probabilistic model}\n",
    "Estimating the mean $\\mu$ of a normal distribution for some observed data $X=(x_i)_{i=1}^n$ that are assumed to be independent given $\\mu$.\n",
    "\n",
    "$p(x_i | \\mu) = N(x_i ; \\mu, \\sigma^2)$\n",
    "\n",
    "Our goal is $p(\\mu | X)$:\n",
    "\\[\n",
    "p(\\mu | X) = \\frac{p(X | \\mu) p(\\mu)}{p(X)} = \\frac{\\prod_{i=1}^n p(x_i | \\mu) p(\\mu)}{p(X)}\n",
    "\\]\n",
    "\n",
    "1. Assume $p(\\mu) = N(\\mu; 0, \\sigma_0^2)$. Implement Metropolis-Hastings to sample from $p(\\mu | X)$ using a normal proposal and $\\sigma_0^2 = 100$. \n",
    "\n",
    "2. Compare with exact solution\n",
    "\n",
    "3. Repeat with $\\sigma_0^2 = 1$\n",
    "\n",
    "4. Try $p(\\mu) = Laplace(\\mu ; 0, b_0) = \\frac{1}{2b_0}\\exp{(-|\\mu|/b_0)}$ with $b_0 = 10$\n",
    "\n",
    "\\begin{minted}{Python}\n",
    "\\%matplotlib inline\n",
    "#Import packages\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy.integrate\n",
    "\n",
    "#Import data as a numpy.array\n",
    "data = pd.read_csv('http://www.helsinki.fi/~ahonkela/teaching/\n",
    "compstats1/toydata.txt', sep='\\t', header=None)\n",
    "data = data.values\n",
    "data = np.array(data[:,0])\n",
    "\n",
    "#Known parameters\n",
    "sigma0 = 10\n",
    "sigma = np.sqrt(2)\n",
    "\n",
    "#1D Metropolis-Hastings sampler with acceptance rate. target and drawproposals are functions with input x, output f(x)\n",
    "def msample(x0, n, target, drawproposal):\n",
    "    x = x0\n",
    "    accepts = 0\n",
    "    lp = target(x)\n",
    "    xs = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        x_prop = drawproposal(x)\n",
    "        l_prop = target(x_prop)\n",
    "        if np.log(npr.rand()) < l_prop - lp:\n",
    "            x = x_prop\n",
    "            lp = l_prop\n",
    "            accepts += 1\n",
    "        xs[i] = x\n",
    "    print('Acceptance rate:', accepts/n)\n",
    "    return xs\n",
    "\n",
    "# Note: sigma = standard deviation, not variance\n",
    "#Logarithm of a multivariate standard normal distribution\n",
    "def lnormpdf(x, mu, sigma):\n",
    "    return -0.5*np.log(2*np.pi) - np.log(sigma) - 0.5*(x-mu)**2/sigma**2\n",
    "\n",
    "# check that the density integrates to 1\n",
    "print(scipy.integrate.quad(lambda x: np.exp(lnormpdf(x, 0, 10)), -100, 100))\n",
    "\n",
    "# Target function input: mu (the parameter) output: probability p(mu | data) \n",
    "# p(mu | data) = p(data | mu) * p(mu)\n",
    "target = lambda mu: np.sum(lnormpdf(data, mu, sigma)) + lnormpdf(mu, 0, sigma0)\n",
    "#Run the sampler. Target p(\\mu) and proposal q(x)\n",
    "x = msample(1.0, 100000, target, lambda x: x+0.2*npr.normal())\n",
    "#Discard first half of samples\n",
    "x = x[len(x)//2:]\n",
    "\n",
    "print(np.mean(x), np.std(x))\n",
    "\n",
    "#Plotting data\n",
    "n = len(data)\n",
    "t = np.linspace(3.5, 4.5, 100)\n",
    "plt.hist(x, 30, normed=True)\n",
    "#Plotting exact solution\n",
    "plt.plot(t, np.exp(lnormpdf(t, sigma0**2/(sigma**2/n + sigma0**2) * np.mean(x), np.sqrt(1/(1/sigma0**2 + n/sigma**2)))))\n",
    "plt.show()\n",
    "\n",
    "print(np.mean(x), np.std(x))\n",
    "\n",
    "sigma0 = 1\n",
    "target2 = lambda mu: np.sum(lnormpdf(data, mu, sigma)) + lnormpdf(mu, 0, sigma0)\n",
    "x = msample(1.0, 100000, target2, lambda x: x+0.2*npr.normal())\n",
    "x = x[len(x)//2:]\n",
    "print(np.mean(x), np.std(x))\n",
    "\n",
    "n = len(data)\n",
    "t = np.linspace(3.5, 4.5, 100)\n",
    "plt.hist(x, 30, normed=True)\n",
    "plt.plot(t, np.exp(lnormpdf(t, sigma0**2/(sigma**2/n + sigma0**2) * np.mean(data), np.sqrt(1/(1/sigma0**2 + n/sigma**2)))))\n",
    "plt.show()\n",
    "\\end{minted}\n",
    "\n",
    "\\subsubsection{Metropolis-Hastings sampling of constrained parameters using transformations}\n",
    "Bayesian inference of the variance of normally distributed data. We use transformations to enforce the positivity of the variance parameter\n",
    "\n",
    "\\[\n",
    "p(x_i | \\sigma) = N(x_i ; 0, \\sigma^2)\n",
    "\\]\n",
    "\\[\n",
    "p(\\sigma) = Exponential(\\sigma; 1) = e^{-\\sigma}\n",
    "\\]\n",
    "\n",
    "We will apply transformation $\\sigma = g(y) = \\exp(y)$ as $\\sigma \\in R^{+}$, $y \\in R$.\n",
    "\\[\n",
    "p_Y(y) = p_{\\sigma}(\\sigma) |\\frac{dg}{dy}|\n",
    "\\]\n",
    "\n",
    "1. Express and plot the prior density $p(\\sigma)$ as a function $y = \\log \\sigma$ \n",
    "2. Evaluate $\\int_{-\\infty}^{\\infty}f_Y(y)dy$\n",
    "3. Generate a data set $x_i ~ N(0, 2^2)$\n",
    "4. Implement MH sampler $y = \\log \\sigma$ when $\\log P(y) = \\sum_{i=1}^{10} \\log p(x_i | y) + \\log p(y)$ using $Q(x'; x) = N(x'; x, 1)$ as the proposal\n",
    "5. Plot a histogram\n",
    "\n",
    "\\begin{minted}{Python}\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.integrate\n",
    "\n",
    "#Using the change of variables formula pY(y) = ps(s)|dg/dy|\n",
    "#log(e^{-exp(y)}*e^y) = -e^y + y\n",
    "def logf(y):\n",
    "    return -np.exp(y) + y\n",
    "\n",
    "t = np.linspace(-5, 5, 50)\n",
    "#Plot the prior density\n",
    "plt.plot(t, np.exp(logf(t)))\n",
    "#Checking density by integrating\n",
    "print(scipy.integrate.quad(lambda x: np.exp(logf(x)), -30, 30))\n",
    "\n",
    "#1D log of standard normal, independent x_i!\n",
    "def lnormpdf(x, sigma):\n",
    "    return np.sum(-0.5*np.log(2*np.pi) - np.log(sigma) - 0.5*x**2/sigma**2)\n",
    "\n",
    "#Metropolis-Hastings sampler\n",
    "def msample(x0, n, target, drawproposal):\n",
    "    x = x0\n",
    "    accepts = 0\n",
    "    lp = target(x)\n",
    "    xs = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        x_prop = drawproposal(x)\n",
    "        l_prop = target(x_prop)\n",
    "        if np.log(npr.rand()) < l_prop - lp:\n",
    "            x = x_prop\n",
    "            lp = l_prop\n",
    "            accepts += 1\n",
    "        xs[i] = x\n",
    "    print('Acceptance rate:', accepts/n)\n",
    "    return xs\n",
    "\n",
    "#Generate a data set of 10 points\n",
    "d = npr.normal(0, 2, 10)\n",
    "\n",
    "# Note that now Q(x',x^(t)) = Q(x^(t), x') so we do not need \n",
    "#to include Q(x'; x) term for the acceptance probability\n",
    "x = msample(0.0, 10000, lambda y: logf(y) + lnormpdf(d, np.exp(y)), lambda y: y+npr.randn())\n",
    "#Discarding the first half\n",
    "x = x[len(x)//2:]\n",
    "\n",
    "plt.show()\n",
    "plt.hist(np.exp(x[::10]), 50, normed=True)\n",
    "print('Data std', np.std(d))\n",
    "print('posterior mean of sigma', np.mean(np.exp(x)))\n",
    "\\end{minted}\n",
    "\n",
    "\\subsubsection{MCMC with an asymmetric proposal}\n",
    "The target distribution:\n",
    "\\[\n",
    "p(n) = \\begin{cases}\n",
    "1/21 & when 1 \\leq n \\leq 21 \\\\\n",
    "0 & otherwise\n",
    "\\end{cases}\n",
    "\\]\n",
    "The proposal: \n",
    "\\[\n",
    "q(n';n) = \\begin{cases}\n",
    "1/2 & when |n' - n| = 1 \\\\\n",
    "0 & otherwise\n",
    "\\end{cases}\n",
    "\\]\n",
    "\n",
    "1. Implement the sampler and run it for 100 000 iterations\n",
    "\n",
    "2. Use an alternative proposal\n",
    "\\[\n",
    "q'(n';n) = \\begin{cases}\n",
    "1 & when (n, n') \\in \\{(1,2), (21,20)\\} \\\\\n",
    "1/2 & when 2 \\leq n \\leq 20 and |n' - n| = 1 \\\\\n",
    "0 & otherwise\n",
    "\\end{cases}\n",
    "\\]\n",
    "but do not include the term in the acceptaqnce ratio\n",
    "\n",
    "3. Use the full acceptance rule\n",
    "\n",
    "\\begin{minted}{Python}\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Normal symmetric 1D Metropolis-Hastings sampler\n",
    "def msample(x0, n, target, drawproposal):\n",
    "    x = x0\n",
    "    accepts = 0\n",
    "    lp = target(x)\n",
    "    xs = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        x_prop = drawproposal(x)\n",
    "        l_prop = target(x_prop)\n",
    "        if np.log(npr.rand()) < l_prop - lp:\n",
    "            x = x_prop\n",
    "            lp = l_prop\n",
    "            accepts += 1\n",
    "        xs[i] = x\n",
    "    print('Acceptance rate:', accepts/n)\n",
    "    return xs\n",
    "\n",
    "def logtarget(x):\n",
    "    if x >= 1 and x <= 21:\n",
    "        return -np.log(21)\n",
    "    else:\n",
    "        return -np.inf\n",
    "\n",
    "def proposal1(x):\n",
    "    return x+npr.choice(np.array([-1, 1]))\n",
    "    \n",
    "npr.seed(42)\n",
    "x = msample(3, 30000, logtarget, proposal1)\n",
    "plt.plot(x)\n",
    "plt.show()\n",
    "h = plt.hist(x, 21)\n",
    "\n",
    "#2. \n",
    "def proposal2(x):\n",
    "    if x == 1:\n",
    "        return 2\n",
    "    if x == 21:\n",
    "        return 20\n",
    "    return x+npr.choice(np.array([-1, 1]))\n",
    "\n",
    "npr.seed(42)\n",
    "x = msample(3, 30000, logtarget, proposal2)\n",
    "plt.plot(x)\n",
    "plt.show()\n",
    "h = plt.hist(x, 21)\n",
    "\n",
    "#3. \n",
    "#Asymmetric Metropolis-Hastings sampler\n",
    "def mhsample(x0, n, target, proposal, drawproposal):\n",
    "    x = x0\n",
    "    accepts = 0\n",
    "    lp = target(x)\n",
    "    xs = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        x_prop = drawproposal(x)\n",
    "        l_prop = target(x_prop)\n",
    "        if np.log(npr.rand()) < l_prop - lp + proposal(x_prop, x) - proposal(x, x_prop):\n",
    "            x = x_prop\n",
    "            lp = l_prop\n",
    "            accepts += 1\n",
    "        xs[i] = x\n",
    "    print('Acceptance rate:', accepts/n)\n",
    "    return xs\n",
    "\n",
    "def logproposal2(x, y):\n",
    "    if x == 1 and y == 2:\n",
    "        return 0\n",
    "    if x == 21 and y == 20:\n",
    "        return 0\n",
    "    if np.abs(x - y) == 1:\n",
    "        return np.log(0.5)\n",
    "    else:\n",
    "        return -np.inf\n",
    "\n",
    "npr.seed(43)\n",
    "x = mhsample(3, 30000, logtarget, logproposal2, proposal2)\n",
    "plt.plot(x)\n",
    "plt.show()\n",
    "h = plt.hist(x, 21)\n",
    "\\end{minted}\n",
    "\n",
    "\\subsubsection{Gibbs sampling}\n",
    "Special form of multivariate Metropolis-Hastings sampling where a vector of variables $\\theta = (\\theta_1, \\dots, \\theta_n)$ is updated cyclically. For each update. $\\theta_i$ is drawn from the exact conditional distribution $p(\\theta_i | \\theta_{\\i}$ given the other variables $\\theta_{\\i}$.\n",
    "\n",
    "\\begin{minted}{Python}\n",
    "def gibbs2d(x0, n, conditional_draw):\n",
    "    d = 2\n",
    "    x = x0\n",
    "    xs = np.zeros([n, d])\n",
    "    for t in range(n):\n",
    "        for i in range(d):\n",
    "            x[i] = conditional_draw(x[1-i])\n",
    "        xs[t]=x\n",
    "    return xs\n",
    "\n",
    "rho = 0.8\n",
    "conditional_draw = lambda x: npr.normal(loc=rho*x, scale=np.sqrt(np.sqrt(1-rho**2)))\n",
    "x = gibbs2d(np.zeros(2), 10000, conditional_draw)\n",
    "plt.hist2d(x[::10, 0],x[::10, 1], 50, normed=True)\n",
    "plt.show()\n",
    "\\end{minted}\n",
    "\n",
    "\\section{MCMC diagnostics and sampling multimodal distributions}\n",
    "\n",
    "\\subsection{MCMC diagnostics}\n",
    "Impossible to prove that a MCMC have converged, but a number of diagnostics have been developed to detect if it has not converged.  Also efficiency is important\n",
    "\n",
    "\\subsubsection{Convergence diagnostics}\n",
    "Starting $m$ chains from widely dispersed starting points end up producing similar results\n",
    "\n",
    "Scale reduction $\\hat{R}$ that measures the factor by which the scale of the current distribution might be reduced if the simulations were continued until $n \\to \\infty$ . Based on within-chain variance $W$ and between-chain variance $B$ \n",
    "\n",
    "Assuming $\\theta_{ij}$ is the $i$th sample from chain $j$ (out of $m$)\n",
    "\\[\n",
    "B = \\frac{n}{m-1}\\sum_{j=1}^m (\\theta_{:j} - \\theta_{::})^2 \n",
    "\\]\n",
    "\\[\n",
    "\\theta_{:j} = \\frac{1}{n} \\sum_{i = 1}^n \\theta_{iJ},  \\theta_{::} = \\frac{1}{m} \\sum_{j=1}^m \\theta_{:j}\n",
    "\\]\n",
    "\\[\n",
    "W = \\frac{1}{m}\\sum_{j=1}^m s_j^2, s_j^2 = \\frac{1}{n-1}\\sum_{i=1}^n (\\theta_{ij} - \\theta_{:j})^2\n",
    "\\]\n",
    "\\[\n",
    "Var(\\theta) = \\frac{n-1}{n}W + \\frac{1}{n}B, \\hat{R} = \\sqrt{\\frac{Var(\\theta)}{W}}\n",
    "\\]\n",
    "Threshold 1.1 is commonly used to consider a chain as having converged.\n",
    "\n",
    "Split-$\\hat{R}$ : split the chains to two halves and consider these as independent chains \n",
    "\n",
    "\\subsubsection{Diagnostics of sampling efficiency}\n",
    "Measured using autocorrelations of the samples, which measure the correlation of neighbouring samples after a given lag. \n",
    "\n",
    "ESS: effective sample size, measure of how many independent samples a given sample is worth.\n",
    "\n",
    "Autocorrelation $p_k$ of a wide-sense stationary process $\\theta_1, \\theta_2, \\dots, $with lag $k$ is defined as\n",
    "\\[\n",
    "p_k = \\frac{E[(\\theta_i - \\mu)(\\theta_{i+k} - \\mu)]}{\\sigma^2}, k=0,1,2, \\dots, \n",
    "\\]$M$ samples\n",
    "\\[\n",
    "ESS = \\frac{M}{1 + 2 \\sum_{k=1}^{\\infty}p_k}\n",
    "\\]\n",
    "Long lags can be unreliable! Then truncate the sum in the denominator\n",
    "\n",
    "\\subsection{Good practices for MCMC sampling}\n",
    "\\begin{enumerate}\n",
    "\\item  Run three or more chains from very different points\n",
    "\\item Discard warm-up or burn-in period\n",
    "\\item Tune the proposal\n",
    "\\item Check for convergence by using the $\\hat{R}$ statistic\n",
    "\\item Mix the samples from all chains for final analysis\n",
    "\\item Compare your inference with results from cimpler models or approximations\n",
    "\\end{enumerate}\n",
    "\n",
    "\\subsection{Improving MCMC convergence}\n",
    "\\subsubsection{Blocking for high-dimensional distributions}\n",
    "Acceptance rates for proposals naturally go down as the dimensionality of the problem increases. If a scalar proposal has a probability $p$ of being accepted, $d$ independent such proposals will have the probability $p^d$\n",
    "\n",
    "Blocking: divide the parameters to subsets and update each subset alone in turn.\n",
    "\n",
    "Gibbs sampler: implement an extreme form of blocking where each variable is updated i turn. Blocking can lead to inefficient sampling if variables in different blocks are strongly correlated. \n",
    "\n",
    "\\subsubsection{Parallel tempering for MCMC for multimodal distributions}\n",
    "Sampling multimodal distributions with MCMC is hard. Combining small-scale local exploration with large global jumps\n",
    "\n",
    "Multimodal distributions: run several MCMC chains in parallel!\n",
    "\n",
    "Instead of sampling a single target $\\pi(\\theta) $, we sample $\\pi(\\theta)^{\\beta}$ for a range of values $0 \\leq \\beta \\leq 1$. Smooth transition from the actual target to a uniform distribution. \n",
    "\n",
    "Bayesian models: $\\pi_{\\beta}(\\theta) = p(\\theta)p(D | \\theta)^{\\beta}$\n",
    "\n",
    "Given a set of temperature values $B = (\\beta_1 = 1, \\dots, \\beta_b = 0)$ , we can define an augmented system over \n",
    "\\[\n",
    "\\Theta = (\\theta^{(1)}, \\dots, \\theta^{(b)})\n",
    "\\]\n",
    "with \n",
    "\\[\n",
    "\\pi(\\Theta) = \\prod_{i=1}^b \\pi_{\\beta_i}(\\theta^{(i)})\n",
    "\\]\n",
    "Following the blocking approach, we introduce a sequence of proposals that are considered and accepted/rejected in sequence:\n",
    "\n",
    "1. Standard MH proposals for each $\\pi_{\\beta_i}(\\theta^{(i)})$\n",
    "2. A proposal to swap the states of neighbouring chains\n",
    "\n",
    "Acceptance probability\n",
    "\\[\n",
    "a = \\frac{\\pi_{\\beta_i}(\\theta^{(i+1)})\\pi_{\\beta_i+1}(\\theta^{(i)})}{\\pi_{\\beta_i}(\\theta^{(i)})\\pi_{\\beta_i+1}(\\theta^{(i+1)})}\n",
    "\\]\n",
    "\\subsubsection{Parallel tempering example}\n",
    "Multimodal target $\\pi(\\theta) = \\exp(-\\gamma (\\theta^2 - 1)^2 )$. We use a normal proposal $q_{\\beta}(\\theta'; \\theta) = N(\\theta'; \\theta, \\sigma_{prop, \\beta}^2)$ with $\\sigma_{prop, \\beta}^2 = \\frac{1}{\\beta}0.1^2$\n",
    "\n",
    "\\begin{minted}{Python}\n",
    "import scipy.integrate\n",
    " \n",
    "def ltarget(theta, gamma):\n",
    "    return -gamma*(theta**2-1)**2\n",
    " \n",
    "# Find the normaliser of the target for visualisation\n",
    "Z = scipy.integrate.quad(lambda theta: np.exp(ltarget(theta, 64.0)), -2, 2)\n",
    " \n",
    "npr.seed(42)\n",
    "theta = mhsample1(1.0, 10000, lambda theta: ltarget(theta, 64.0),\n",
    "              lambda theta: theta + 0.1*npr.normal())\n",
    "              \n",
    "theta = theta[len(theta)//2:]\n",
    "h = plt.hist(theta, 50, normed=True)\n",
    "t = np.linspace(-1.2, 1.2, 200)\n",
    "plt.plot(t, np.exp(ltarget(t, 64.0)) / Z[0])\n",
    "plt.show()\n",
    "\n",
    "print(np.mean(theta))\n",
    "\n",
    "betas = np.logspace(-3, 0, 5)\n",
    "print(betas)\n",
    "\n",
    "def pt_target(theta, beta, gamma):\n",
    "    return beta * ltarget(theta, gamma)\n",
    " \n",
    "def pt_msample(theta0, n, betas, target, drawproposal):\n",
    "    CHAINS = len(betas)\n",
    "    accepts = np.zeros(CHAINS)\n",
    "    swapaccepts = np.zeros(CHAINS-1)\n",
    "    swaps = np.zeros(CHAINS-1)\n",
    "    # All variables are duplicated for all the chains\n",
    "    theta = theta0 * np.ones(CHAINS)\n",
    "    lp = np.zeros(CHAINS)\n",
    "    thetas = np.zeros((n, CHAINS))\n",
    "    for j in range(CHAINS):\n",
    "        lp[j] = target(theta[j], betas[j])\n",
    "    for i in range(n):\n",
    "        # Independent moves for every chain, MH acceptance\n",
    "        for j in range(CHAINS):\n",
    "            theta_prop = drawproposal(theta[j], betas[j])\n",
    "            l_prop = target(theta_prop, betas[j])\n",
    "            if np.log(npr.rand()) < l_prop - lp[j]:\n",
    "                theta[j] = theta_prop\n",
    "                lp[j] = l_prop\n",
    "                accepts[j] += 1\n",
    "        # Swap move for two chains, MH acceptance:\n",
    "        j = npr.randint(CHAINS-1)\n",
    "        h = target(theta[j+1],betas[j])+target(theta[j],betas[j+1]) - lp[j] - lp[j+1]\n",
    "        swaps[j] += 1\n",
    "        if np.log(npr.rand()) < h:\n",
    "            # Swap theta[j] and theta[j+1]\n",
    "            temp = theta[j]\n",
    "            theta[j] = theta[j+1]\n",
    "            theta[j+1] = temp\n",
    "            lp[j] = target(theta[j], betas[j])\n",
    "            lp[j+1] = target(theta[j+1], betas[j+1])\n",
    "            swapaccepts[j] += 1\n",
    "        thetas[i,:] = theta\n",
    "    print('Acceptance rates:', accepts/n)\n",
    "    print('Swap acceptance rates:', swapaccepts/swaps)\n",
    "    return thetas\n",
    " \n",
    "npr.seed(42)\n",
    "betas = np.logspace(-3, 0, 5)\n",
    "theta = pt_msample(1.0, 10000, betas,\n",
    "                   lambda theta, beta: pt_target(theta, beta, 64.0),\n",
    "                   lambda theta, beta: theta + 0.1/np.sqrt(beta)*npr.normal())\n",
    "                   \n",
    "theta = theta[len(theta)//2:]\n",
    "h = plt.hist(theta[:,-1], 50, normed=True)\n",
    "t = np.linspace(-1.2, 1.2, 200)\n",
    "plt.plot(t, np.exp(ltarget(t, 64.0)) / Z[0])\n",
    "plt.show()\n",
    "\n",
    "print(np.mean(theta[:,-1]))\n",
    "\n",
    "N = len(betas)\n",
    "fix, ax = plt.subplots(N, 1, figsize=[6.4, 9.6])\n",
    "for i in range(N):\n",
    "    ax[i].set_title('beta = %.2f' % betas[i])\n",
    "    ax[i].plot(theta[:,i])\n",
    "    plt.subplots_adjust(hspace=0.4)\n",
    "plt.show()\n",
    "\\end{minted}\n",
    "\n",
    "\\subsection{More MCMC convergence theory}\n",
    "Sampling expectations converge almost surely to corresponding expectations under the stationary distribution\n",
    "\\[\n",
    "\\lim_{N \\to \\infty} = \\frac{1}{N} \\sum_{i=1}^N h(\\theta_i) \\to E_{\\pi}[h(\\theta)] = \\int h(\\theta) \\pi(\\theta) d\\theta\n",
    "\\]assuming $\\int |h(\\theta)|\\pi(\\theta)d\\theta < \\infty$\n",
    "\\begin{Theorem}\n",
    "The Metropolis-Hastings acceptance probability for proposal $q(\\theta';\\theta)$ and target $\\pi(\\theta)$,\n",
    "\\[\n",
    "f(\\theta' | \\theta) = min \\Bigg( 1, \\frac{\\pi(\\theta')q(\\theta; \\theta')}{\\pi(\\theta)q(\\theta';\\theta)}\\Bigg)\n",
    "\\]\n",
    "defines a Markov chain with the transition kernel $T(\\theta, \\theta') = f(\\theta' | \\theta)q(\\theta'; \\theta)$. This chain satisfies the detailed balance condition.  \n",
    "\\end{Theorem}\n",
    "\n",
    "\\subsection{Exercises}\n",
    "\\subsubsection{MCMC with a fixed proposal}\n",
    "MCMC sampling can also be used so that the proposal $q(x';x)$ is independent of $x$. Metropolis-Hastings sampler for the $N(0,1)$ distribution using $Laplace(0,1)$ as the fixed proposal.\n",
    "\n",
    "\\begin{minted}{Python}\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#The Laplace function\n",
    "def laplacetarget(x):\n",
    "    return -np.log(2) - np.abs(x)\n",
    "\n",
    "#The standard normal\n",
    "def normaltarget(x):\n",
    "    return -0.5*(x**2)\n",
    "\n",
    "#Asymnmetric MH sampler\n",
    "def mhsample(x0, n, target, proposal, drawproposal):\n",
    "    x = x0\n",
    "    accepts = 0\n",
    "    lp = target(x)\n",
    "    xs = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        x_prop = drawproposal(x)\n",
    "        l_prop = target(x_prop)\n",
    "        if np.log(npr.rand()) < l_prop - lp + proposal(x_prop, x) - proposal(x, x_prop):\n",
    "            x = x_prop\n",
    "            lp = l_prop\n",
    "            accepts += 1\n",
    "        xs[i] = x\n",
    "    print('Acceptance rate:', accepts/n)\n",
    "    return xs\n",
    "\n",
    "npr.seed(42)\n",
    "\n",
    "#The proposal q(x';x) is independent of x! \n",
    "#lambda x,y: laplacetarget(y)\n",
    "x = mhsample(1.0, 30000, normaltarget, lambda x,y: laplacetarget(y), lambda x: npr.laplace())\n",
    "x = x[len(x)//2:]\n",
    "plt.plot(x)\n",
    "plt.show()\n",
    "h = plt.hist(x, 100)\n",
    "print('sampled', np.mean(x**2), np.mean(x**4))\n",
    "y = npr.laplace(size=10000)\n",
    "print('Laplace', np.mean(y**2), np.mean(y**4))\n",
    "y = npr.normal(size=10000)\n",
    "print('normal', np.mean(y**2), np.mean(y**4))\n",
    "\n",
    "\\end{minted}\n",
    "\n",
    "\\subsubsection{Parallel tempering MCMC for multimodal distributions}\n",
    "Bimodal distribution $\\pi(\\theta) = \\exp(-\\gamma(\\theta^2 - 1)^2)$\n",
    "\n",
    "\\begin{minted}{Python}\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Normal symmetric Metropolis-Hastings sampler\n",
    "def msample(x0, n, target, drawproposal):\n",
    "    x = x0\n",
    "    accepts = 0\n",
    "    lp = target(x)\n",
    "    xs = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        x_prop = drawproposal(x)\n",
    "        l_prop = target(x_prop)\n",
    "        if np.log(npr.rand()) < l_prop - lp:\n",
    "            x = x_prop\n",
    "            lp = l_prop\n",
    "            accepts += 1\n",
    "        xs[i] = x\n",
    "    print('Acceptance rate:', accepts/n)\n",
    "    return xs[len(xs)//2:]\n",
    "    \n",
    "def target(x, gamma):\n",
    "    return -gamma*(x**2-1)**2\n",
    "\n",
    "npr.seed(42)\n",
    "x = msample(1.0, 10000, lambda x: target(x, 16.0), lambda x: x + 1.0*npr.normal())\n",
    "print(np.mean(x))\n",
    "h = plt.hist(x, 50)\n",
    "plt.show()\n",
    "x = msample(1.0, 10000, lambda x: target(x, 32.0), lambda x: x + 0.5*npr.normal())\n",
    "print(np.mean(x))\n",
    "h = plt.hist(x, 50)\n",
    "plt.show()\n",
    "x = msample(1.0, 10000, lambda x: target(x, 64.0), lambda x: x + 0.2*npr.normal())\n",
    "print(np.mean(x))\n",
    "h = plt.hist(x, 50)\n",
    "plt.show()\n",
    "\n",
    "#Parallel tempering target\n",
    "def par_target(x, beta, gamma):\n",
    "    return beta*target(x, gamma)\n",
    "\n",
    "#Parallel tempering MH sampler\n",
    "def par_msample(x0, n, betas, target, drawproposal):\n",
    "    CHAINS = len(betas)\n",
    "    accepts = np.zeros(CHAINS)\n",
    "    swapaccepts = np.zeros(CHAINS-1)\n",
    "    swaps = np.zeros(CHAINS-1)\n",
    "    x = x0 * np.ones(CHAINS)\n",
    "    lp = np.zeros(CHAINS)\n",
    "    xs = np.zeros((n, CHAINS))\n",
    "    for j in range(CHAINS):\n",
    "        lp[j] = target(x[j], betas[j])\n",
    "    for i in range(n):\n",
    "        # Independent moves for every chain\n",
    "        for j in range(CHAINS):\n",
    "            x_prop = drawproposal(x[j], betas[j])\n",
    "            l_prop = target(x_prop, betas[j])\n",
    "            if np.log(npr.rand()) < l_prop - lp[j]:\n",
    "                x[j] = x_prop\n",
    "                lp[j] = l_prop\n",
    "                accepts[j] += 1\n",
    "        # Swap move for two chains:\n",
    "        j = npr.randint(CHAINS-1)\n",
    "        h = target(x[j+1],betas[j])+target(x[j],betas[j+1]) - lp[j] - lp[j+1]\n",
    "        swaps[j] += 1\n",
    "        if np.log(npr.rand()) < h:\n",
    "            temp = x[j]\n",
    "            x[j] = x[j+1]\n",
    "            x[j+1] = temp\n",
    "            lp[j] = target(x[j], betas[j])\n",
    "            lp[j+1] = target(x[j+1], betas[j+1])\n",
    "            swapaccepts[j] += 1\n",
    "        xs[i,:] = x\n",
    "    print('Acceptance rates:', accepts/n)\n",
    "    print('Swap acceptance rates:', swapaccepts/swaps)\n",
    "    return xs[len(xs)//2:]\n",
    " \n",
    " npr.seed(42)\n",
    "betas = np.logspace(-3, 0, 5)\n",
    "x = par_msample(1.0, 10000, betas,\n",
    "                 lambda x, beta: par_target(x, beta, 64.0),\n",
    "                 lambda x, beta: x + 0.1/np.sqrt(beta)*npr.normal())\n",
    "h = plt.hist(x[:,-1], 50)\n",
    "plt.show()\n",
    "print(np.mean(x[:,-1]))\n",
    "\\end{minted}\n",
    "\n",
    "\\subsubsection{Blocking in MCMC}\n",
    "We will sample a $d = 100$ dimensional multivariate normal with two covariances:\n",
    "a) $\\Sigma = I_d$\n",
    "b) $\\Sigma = AA^T + \\epsilon I_d$where all entries of $A \\in R^{d \\times k}$ follow $N(0,1)$\n",
    "\n",
    "1. Sample without blocking\n",
    "2. Sample with blocking\n",
    "\n",
    "\\begin{minted}{Python}\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "\n",
    "#A blocked normal proposal\n",
    "class blocked_normal_proposal:\n",
    "    \"\"\"A class for a blocked normal proposal.\n",
    "      p = blocked_normal_proposal(d, blocks)\n",
    "    creates a proposal object yielding d-dimensional normal random numbers\n",
    "    where each of the \"blocks\" blocks in turn follows N(0, I) when calling\n",
    "      p.draw()\n",
    "    \"\"\"\n",
    "    def __init__(self, d, blocks):\n",
    "        self.d = d\n",
    "        self.blocks = blocks\n",
    "        self.curblock = 0\n",
    "    \n",
    "    def next_block_indices(self):\n",
    "        I = np.zeros(self.d, np.bool)\n",
    "        I[((self.curblock*self.d)//self.blocks):(((self.curblock+1)*self.d)//self.blocks)] = True\n",
    "        self.curblock = (self.curblock + 1) % self.blocks\n",
    "        return I\n",
    "    \n",
    "    def draw(self):\n",
    "        res = np.zeros(self.d)\n",
    "        I = self.next_block_indices()\n",
    "        res[I] = npr.normal(size=np.sum(I))\n",
    "        return res\n",
    "\n",
    "# Use this to get help:\n",
    "# ? blocked_normal_proposal\n",
    "p = blocked_normal_proposal(6, 3)\n",
    "print(p.draw())\n",
    "print(p.draw())\n",
    "print(p.draw())\n",
    "print(p.draw())\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "\n",
    "def d_msample(x0, n, target, drawproposal):\n",
    "    x = x0\n",
    "    d = len(x0)\n",
    "    accepts = 0\n",
    "    lp = target(x)\n",
    "    xs = np.zeros([n, d])\n",
    "    for i in range(n):\n",
    "        x_prop = drawproposal(x)\n",
    "        l_prop = target(x_prop)\n",
    "        if  np.log(npr.rand()) < l_prop - lp:\n",
    "            x = x_prop\n",
    "            lp = l_prop\n",
    "            accepts += 1\n",
    "        xs[i] = x\n",
    "    print('Acceptance rate:', accepts/n)\n",
    "    return xs\n",
    "    \n",
    " def mvnormtarget(x):\n",
    "    return -0.5*np.sum(x**2)\n",
    "\n",
    "def sample_nd(d, samples=100000, width=1.0):\n",
    "    return d_msample(np.ones(d), samples, mvnormtarget, lambda x: x+width*npr.normal(size=d))\n",
    "\n",
    "npr.seed(42)\n",
    "x = sample_nd(1000, 10000, 0.1)\n",
    "np.mean(np.mean(x, 0)**2)\n",
    "\n",
    "def sample_nd_blocked(d, blocks, samples=100000, width=1.0):\n",
    "    p = blocked_normal_proposal(d, blocks)\n",
    "    return d_msample(np.ones(d), samples, mvnormtarget, lambda x: x+width*p.draw())\n",
    "    \n",
    "npr.seed(42)\n",
    "x = sample_nd_blocked(1000, 500, 10000, 2.0)\n",
    "print(np.mean(np.mean(x, 0)**2))\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import numpy.linalg as nlg\n",
    "import scipy.linalg as slg\n",
    "\n",
    "A = npr.normal(size=(100, 2))\n",
    "Sigma = A @ A.T + 0.1*np.eye(100)\n",
    "M = nlg.cholesky(Sigma)\n",
    "\n",
    "def mvnormtarget2(x):\n",
    "    return -0.5*np.sum(slg.solve_triangular(M, x, lower=True)**2)\n",
    "\n",
    "def sample_nd2(d, samples=100000, width=1.0):\n",
    "    x = d_msample(np.ones(d), samples, mvnormtarget2, lambda x: x+width*npr.normal(size=d))\n",
    "    return x[len(x)//2:]\n",
    "\n",
    "def sample_nd2_blocked(d, blocks, samples=100000, width=1.0):\n",
    "    p = blocked_normal_proposal(d, blocks)\n",
    "    x = d_msample(np.ones(d), samples, mvnormtarget2, lambda x: x+width*p.draw())\n",
    "    return x[len(x)//2:]\n",
    "\n",
    "npr.seed(42)\n",
    "x = sample_nd2(100, 10000, 0.05)\n",
    "print(np.mean(np.mean(x, 0)**2))\n",
    "plt.plot(x[:,20])\n",
    "\n",
    "npr.seed(42)\n",
    "x = sample_nd2_blocked(100, 50, 10000, 0.8)\n",
    "print(np.mean(np.mean(x, 0)**2))\n",
    "plt.plot(x[:,20])\n",
    "\\end{minted}\n",
    "\n",
    "\n",
    "\\section{MCMC for model comparison and other Monte Carlo techniques}\n",
    "\n",
    "\\subsection{Importance sampling}\n",
    "Importance sampling is another general purpose tool for evaluating expectations over a difficult density $f(x)$ by drawing samples from an easy-to-sample surrogate distribution $g(x)$ and weighting them by the ratio of the true density  \n",
    "\n",
    "\\[\n",
    "I_h = E_{f(x)}[h(x)] = \\int h(x) f(x) dx = \\int h(x) \\frac{f(x)}{g(x)} g(x) dx = E_{g(x)}\\Bigg[ h(x) \\frac{f(x)}{g(x)}\\Bigg]\n",
    "\\]\n",
    "The corresponding Monte Carlo approximation is \n",
    "\\[\n",
    "I_h \\approx \\frac{1}{n} \\sum_{i=1}^n h(x_i) \\frac{f(x_i)}{g(x_i)} = \\frac{1}{n} \\sum_{i=1}^n h(x_i) w_i\n",
    "\\]\n",
    "where $x_i \\sim g(x)$\n",
    "\n",
    "It is necessary to have $g(x) = 0 \\implies h(x)f(x)= 0$\n",
    "\n",
    "It is required that \n",
    "\\[\n",
    "E_g \\Bigg[h^2(x) \\frac{f^2(x)}{g^2(x)} \\Bigg] = \\int h^2(x) \\frac{f^2(x)}{g(x)}dx < \\infty\n",
    "\\]\n",
    "which is guaranteed for example if \n",
    "\\[\n",
    "Var_f[h(X)] < \\infty, and \\frac{f(x)}{g(x)} \\leq M, \\forall x  \\textrm{ for some } M>0\n",
    "\\]\n",
    "\\subsection{Bayesian model comparison}\n",
    "In many inference problems we do not have a single model known beforehand but we want to consider multiple possible models\n",
    "\n",
    "Marginal likelihood measures the fit of the model $M_i$ to data $D$\n",
    "\\[\n",
    "p(D | M_i) = \\int_{\\Theta} p(D | \\theta, M_i) p(\\theta | M_i) d\\theta\n",
    "\\]\n",
    "\\subsubsection{Model comparison and Bayes factors}\n",
    "Bayes rule\n",
    "\\[\n",
    "p(M_i | D) = \\frac{p(D |M_i) p(M_i)}{\\sum_i p(D |M_i) p(M_i)}\n",
    "\\]\n",
    "Bayes factor: $B_{12} = \\frac{p(M_1 | D)}{p(M_2 |D)}\\frac{p(D | M_1)}{p(D | M_2)}$\n",
    "\n",
    "\\subsubsection{Caveats}\n",
    "Depends on the prior\n",
    "\n",
    "\\subsection{Marginal likelihood evaluation using thermodynamic integration}\n",
    "Evaluating marginal likelihood with MCMC is non-trivial, results in a very high variance of the results. Use thermodynamic integration!\n",
    "\n",
    "Drawing samples from\n",
    "\\[\n",
    "\\pi_{\\beta}^* (\\theta) = p(D | \\theta)^{\\beta} p(\\theta), \\beta \\in [0,1]\n",
    "\\]\n",
    "Denoting \\[\n",
    "Z_{\\beta} = \\int_{\\Theta} \\pi_{\\beta}^* (\\theta) d \\theta\n",
    "\\]We have the marginal likelihood $p(D) = Z_1$ while $Z_0 = \\int_{\\Theta} p(\\theta) d\\theta = 1$. The normalized distribution\n",
    "\\[\n",
    "\\pi_{\\beta} (\\theta) = \\frac{\\pi_{\\beta}^*(\\theta)}{Z_{\\beta}}\n",
    "\\]Thermodynaimic integration is based on numeric integration\n",
    "\\[\n",
    "\\ln Z_1 - \\ln Z_0 = \\int_0^1 \\frac{\\partial \\ln Z_{\\beta}}{\\partial \\beta} d \\beta\n",
    "\\]\n",
    "which can be avaluated as \n",
    "\\[\n",
    "\\ln p(D) = \\ln Z_1 - \\ln Z_0 = \\int_0^1 E_{\\beta}[\\ln p(D | \\theta)] d \\beta\n",
    "\\]\n",
    "where $E_{\\beta}[]$ is an expectation over the annealed posterior $\\pi_{\\beta}(\\theta)$ which can be evaluated using Monte Carlo samples from the corresponding chain. \n",
    "\n",
    "\\subsection{Thermodynamic integration example}\n",
    "\\begin{minted}{Python}\n",
    "import scipy.integrate\n",
    "def lnormpdf(x, mu, sigma):\n",
    "    return np.sum(-0.5*np.log(2*np.pi) - np.log(sigma) - 0.5 * (x-mu)**2/sigma**2)\n",
    " \n",
    "npr.seed(42)\n",
    "# Simulate n=100 points of normally distributed data about mu=0.5\n",
    "n = 100\n",
    "data = 0.5 + npr.normal(size=n)\n",
    " \n",
    "# Set the prior parameters\n",
    "sigma_x = 1.0\n",
    "mu0 = 0.0\n",
    "sigma0 = 3.0\n",
    "likelihood = lambda mu: lnormpdf(data, mu, sigma_x)\n",
    "prior = lambda mu: lnormpdf(mu, mu0, sigma0)\n",
    "CHAINS = 20\n",
    "ITERS = 10000\n",
    "betas = np.concatenate((np.array([0.0]), np.logspace(-5, 0, CHAINS)))\n",
    "xx = np.zeros((ITERS, CHAINS+1))\n",
    "for i in range(CHAINS+1):\n",
    "    xx[:,i] = mhsample1(np.zeros(1), ITERS, lambda x: betas[i]*likelihood(x)+prior(x),\n",
    "                        lambda x: x + 0.2/np.sqrt(betas[i]+1e-3)*npr.normal(size=1))\n",
    "                        \n",
    "lls_ti = np.zeros(xx.shape)\n",
    "for i in range(lls_ti.shape[0]):\n",
    "    for j in range(lls_ti.shape[1]):\n",
    "        lls_ti[i,j] = likelihood(xx[i,j])\n",
    "ll = np.mean(lls_ti, 0)\n",
    "plt.plot(betas, ll)\n",
    "plt.xlabel(r'$\\beta$')\n",
    "plt.ylabel(r'$E_\\beta[ \\log p(\\mathcal{D} | \\theta)]$')\n",
    "plt.show()\n",
    "\n",
    "print('Marginal likelihood:',scipy.integrate.simps(ll, betas))\n",
    "\\end{minted}\n",
    "\n",
    "\\subsection{Exercises}\n",
    "\\subsubsection{Importance sampling}\n",
    "Evaluate the expectations $E[x^2]$ and $E[x^4]$ over standard normal $N(0,1)$ using importance sampling with $Laplace(0,1)$ as the surrogate density. Then swap the roles\n",
    "\n",
    "\\begin{minted}{Python}\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "\n",
    "def laplacetarget(x):\n",
    "    return -np.log(2) - np.abs(x)\n",
    "\n",
    "def normaltarget(x):\n",
    "    return -0.5*np.log(2*np.pi)-0.5*(x**2)\n",
    "\n",
    "npr.seed(42)\n",
    "x = npr.laplace(size=10000)\n",
    "print('E_Laplace[x^2] =', np.mean(x**2))\n",
    "print('E_Laplace[x^4] =', np.mean(x**4))\n",
    "print('E_N,importance[x^2] =', np.mean(x**2 * np.exp(normaltarget(x) - laplacetarget(x))))\n",
    "print('E_N,importance[x^4] =', np.mean(x**4 * np.exp(normaltarget(x) - laplacetarget(x))))\n",
    "\n",
    "y = npr.normal(size=10000)\n",
    "print('E_N[x^2] =', np.mean(y**2))\n",
    "print('E_N[x^4] =', np.mean(y**4))\n",
    "print('E_Lap,importance[x^2] =', np.mean(y**2 * np.exp(laplacetarget(y) - normaltarget(y))))\n",
    "print('E_Lap,importance[x^4] =', np.mean(y**4 * np.exp(laplacetarget(y) - normaltarget(y))))\n",
    "\n",
    "\\end{minted}\n",
    "\n",
    "\\subsubsection{Thermodynamic integration for marginal likelihood estimation}\n",
    "\n",
    "\\section{Automatic differentiation and optimisation}\n",
    "Many problems in computational statistics involve the search or optimisation of a complicated function over a high dimensional space. Use derivatives and gradients of the function!\n",
    "\n",
    "Automatic differentiation can be used to compute derivatives easily, exactly and efficiently.\n",
    "\n",
    "\\subsection{Derivatives of multivariate functions }\n",
    "Let $f: \\mathcal{R}^n \\to \\mathcal{R}^m$. When $m=1$ and $n>1$, the $n$-dimensional vector of partial derivatives measuring the change of the function in each coordinate direction is the gradient\n",
    "\n",
    "\\[\n",
    "\\nabla f(x) = \\Bigg( \\frac{\\partial f}{\\partial x_1}, \\dots, \\frac{\\partial f}{\\partial x_n} \\Bigg)\n",
    "\\]\n",
    "Geometrically, the gradient is a vector pointing to the direction where the function grows the fastest.\n",
    "\n",
    "When both $n,m > 1$, the $m \\times n$ matrix of partial derivatives is the Jacobian matrix J\n",
    "\\[\n",
    "J_{ij} = \\frac{\\partial f_i}{\\partial x_j}\n",
    "\\]\n",
    "The Jacobian matrix defines a mapping which is locally the best linear approximation of f.\n",
    "\n",
    "When $m=1$ and $n>1$, we can also define an $n \\times n$ matrix $H$ of second derivatives called the Hessian matrix\n",
    "\\[\n",
    "H_{ij} = \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}\n",
    "\\]\n",
    "Geometrically the Hessian matrix defines the curvature of the function at a point.\n",
    "\n",
    "Gradients and other derivatives of a function are important because they enable efficient optimisation using for example Newton's method for minimisation. \n",
    "\n",
    "Newton's method for minimisation in higher dimensions\n",
    "\\[\n",
    "x_{n+1} = x_n - [Hf(x_n)]^{-1}\\nabla f(x_n)\n",
    "\\]\n",
    "\n",
    "\\subsection{Approaches to differentiation}\n",
    "There are three main approaches for differentiation of a function $f:\\mathcal{R}^n \\to \\mathcal{R}^m$\n",
    "\n",
    "\\begin{enumerate}\n",
    "    \\item Numerical differentiation: $f'(x) \\approx \\frac{f(x+h) - f(x)}{h}$\n",
    "    \\item Analytic differentiation\n",
    "    \\item Automatic differentiation\n",
    "\\end{enumerate}\n",
    "\n",
    "Numerical differentiation provides limited accuracy, analytic differentiation requires lot of manual work, but automatic differentiation is a convenient computational solution that is easy, accurate and numerically stable. \n",
    "\n",
    "\\subsection{Different modes of automatic differentiation}\n",
    "There are two different modes: forward mode and backward mode. \n",
    "\n",
    "\\subsection{Automatic differentiation in Python}\n",
    "\\subsubsection{Autograd}\n",
    "\\begin{minted}{Python}\n",
    "import autograd.numpy as np\n",
    "import autograd.numpy.linalg as npl\n",
    "from autograd import grad\n",
    " \n",
    "def f(x, a):\n",
    "    return np.sin(a*x**3 - 2*x**2 - 4*x)\n",
    " \n",
    "g = grad(f)\n",
    " \n",
    "t = np.linspace(-2.2, 3.5, 50)\n",
    " \n",
    "# grad() only works for functions with a scalar value so it cannot be\n",
    "# used with this this f() when the input is a vector, therefore we\n",
    "# need a for loop to compute the values\n",
    "df = np.zeros(t.shape)\n",
    "for i in range(len(t)):\n",
    "    df[i] = g(t[i], 1.0)\n",
    "plt.plot(t, f(t, 1.0), color='b', label=r'$f$')\n",
    "plt.plot(t, df, color='g', label=r\"$f'$\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\\end{minted}\n",
    "\n",
    "It is important to use autograd.numpy and autograd.scipy!\n",
    "\n",
    "Use floats instead of integers, 1.0 and not 1!\n",
    "\n",
    "\\subsubsection{PyTorch}\n",
    "\\begin{minted}{Python}\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "dtype = torch.double\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    " \n",
    "def f(x, a):\n",
    "    return torch.cos(a*x**3 - 2*x**2 - 4*x)\n",
    " \n",
    "t = torch.linspace(-2.2, 3.5, 50, device=device, dtype=dtype)\n",
    " \n",
    "# variables used for gradient evaluation are created with requires_grad=True\n",
    "x = torch.zeros(1, requires_grad=True, device=device, dtype=dtype)\n",
    " \n",
    "# gradients again need to be evaluated \n",
    "df = np.zeros(t.shape)\n",
    "for i in range(len(t)):\n",
    "    # set the value of the input variable without changing its gradient\n",
    "    # via .data attribute\n",
    "    x.data = t[i]\n",
    "    v = f(x, 1.0)\n",
    "    # .backward() on function output computes gradients\n",
    "    v.backward()\n",
    "    # access gradients via .grad attribute\n",
    "    df[i] = x.grad\n",
    "    # gradient needs to be reset before computing the next value\n",
    "    x.grad.zero_()\n",
    " \n",
    "# Numpy arrays needed for plotting can be accessed via .numpy()\n",
    "plt.plot(t.numpy(), f(t, 1.0).numpy(), color='b', label=r'$f$')\n",
    "plt.plot(t.numpy(), df, color='g', label=r\"$f'$\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\\end{minted}\n",
    "\n",
    "\\subsection{Gradient-based optimisation methods}\n",
    "Approximations to Newton's method\n",
    "\\begin{enumerate}\n",
    "    \\item Quasi-Newton methods: BFGS, L-BFGS\n",
    "    \\item Conjugate gradient methods\n",
    "    \\item Gradient descent with plain\n",
    "\\end{enumerate}\n",
    "Often BFGS and L-BFGS are good choices\n",
    "\n",
    "\\subsection{Stochastic gradient optimisation}\n",
    "Many optimisation problems in statistics such as maximum likelihood estimation with independent identically distributed (iid) observations are of the form\n",
    "\\[\n",
    "\\textrm{min}F(\\theta) = \\sum_{i=1}^N f(\\textbf{x}_i, \\mathbf{\\theta})\n",
    "\\]\n",
    "When $N >> 1$, we can approximate the gradient by using a subset of the data $\\{x_{s1}, \\dots, x_{sM} \\}$\n",
    "\\[\n",
    "\\nabla F (\\theta) \\approx \\frac{N}{M} \\sum_{i=1}^M \\nabla f(x_{si}, \\theta) = g(\\theta)\n",
    "\\]\n",
    "Iteratively updating \n",
    "\\[\n",
    "\\theta_{n+1} = \\theta_n - \\eta_n g(\\theta_n)\n",
    "\\]\n",
    "will converge\n",
    "\n",
    "\\subsection{Optimisation with constraints}\n",
    "Many optimisation problems have constraints, such as when optimising the standard deviation $\\sigma$, $\\sigma > 0$. Transformations\n",
    "\n",
    "\\subsection{Optimisation examples}\n",
    "Let's consider maximum likelihood estimation of Student's t-distribution as an example.  \n",
    "\n",
    "Our goal is to estimate the maximum likelihood location parameter and scale parameter of the Student's t-distribution from observations. Given the data set $D = (x_1, \\dots, x_2)$, this can be written as the minimisation problem with the objective \n",
    "\\[\n",
    "f(\\mathbf{\\theta}) = \\sum_{i=1}^n - \\log p(x_i | \\mathbf{\\theta})\n",
    "\\]\n",
    "\n",
    "\\begin{minted}{Python}\n",
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "import autograd.scipy.stats as sst\n",
    "import autograd\n",
    "import scipy.optimize\n",
    " \n",
    "# Generate a synthetic data set\n",
    "x = npr.randn(100) + 0.5\n",
    "print('data mean:', x.mean(), 'data std:', x.std())\n",
    "\n",
    "def student_logpdf_autograd(x, m, s, nu):\n",
    "    return sst.t.logpdf(x, nu, m, s)\n",
    " \n",
    "def ltarget(theta):\n",
    "    return -np.sum(student_logpdf_autograd(x,\n",
    "    theta[0], np.exp(theta[1]), 5.0))\n",
    " \n",
    "g = autograd.grad(ltarget)\n",
    " \n",
    "theta_opt = scipy.optimize.minimize(ltarget, \n",
    "np.zeros(2), method='BFGS', jac = g)\n",
    "print('location estimate:', theta_opt.x[0],\n",
    "      'scale estimate:', np.exp(theta_opt.x[1]))\n",
    "\\end{minted}\n",
    "\n",
    "\n",
    "\\subsection{Exercises}\n",
    "\\subsubsection{Autograd tutorial}\n",
    "\\begin{minted}{Python}\n",
    "# Example usage of Autograd\n",
    "\n",
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "d_sigmoid = grad(sigmoid)\n",
    "# Above d_sigmoid is now callable function that\n",
    "#returns the derivative of sigmoid\n",
    "print(d_sigmoid(0.0))\n",
    "# See how it differs from the finite differences\n",
    "h=1e-9\n",
    "print( (sigmoid(h)-sigmoid(0.0))/(h) )\n",
    "\\end{minted}\n",
    "\n",
    "\\section{Variational inference}\n",
    "MCMC methods approximate the posterior $p( \\theta | D) $ through samples. Variational inference provides an alternative approach: fitting an approximation $q(\\theta) \\approx p(\\theta | D)$ with a simple functional form, such as a suitable normal distribution, and casting the inference task as an optimisation problem.\n",
    "\n",
    "Good approximations of posterior means, but often underestimate posterior variances. Variational inference is faster than MCMC.\n",
    "\n",
    "\\subsection{Variational inference basics}\n",
    "Variational inference is based on fitting an approximation $q(\\theta)$ to the posterior by minimising the Kullback-Leibler divergence\n",
    "\n",
    "\\[\n",
    "D_{KL}(q(\\theta) | |p(\\theta)|D)) = E_{q(\\theta)} \\Bigg[\\log \\frac{q(\\theta)}{p(\\theta | D)} \\Bigg]\n",
    "\\]\n",
    "The Kullback-Leibler divergence is not a proper metric. Nevertheless it satisfies:\n",
    "\\[\n",
    "D_{KL}(q || p) \\leq 0 \\forall q, p\n",
    "\\]\n",
    "\\[\n",
    "D_{KL}(q || p) = 0, q = p\n",
    "\\]The exact $D_{KL}$ is difficult to compute, evidence lower-bound (ELBO) is used instead:\n",
    "\\[\n",
    "L = E_{q(\\theta)} \\Bigg[ \\log \\frac{p(\\theta, D)}{q(\\theta)} \\Bigg] = \\log p(D) - D_{KL} (q(\\theta) || p(\\theta | D))\n",
    "\\]\n",
    "Because $D_{KL} (q(\\theta) || p(\\theta | D)) \\geq 0$, we have $L \\leq \\log p(D)$\n",
    "\n",
    "\\subsection{Classical variational inference}\n",
    "Classical algorithms are based on cyclic optimisation of $L$ one variable at a time while keeping others fixed.\n",
    "\n",
    "\\subsection{Doubly stochastic variational inference (DSVI) and Automatic differentiation variational inference (ADVI)}\n",
    "The algorithms use the reparametrisation trick\n",
    "\n",
    "Gradients of the expectations \n",
    "\\[\n",
    "E_{q(\\theta)}[*]\n",
    "\\]\n",
    "with respect to a changing distribution such as \n",
    "\\[\n",
    "q(\\theta) = N(\\theta; \\mu, \\Sigma)\n",
    "\\]\n",
    "can be turned to expectations with respect to a fixed distribution via reparametrisation\n",
    "\\[\n",
    "\\theta = L \\eta + \\mu\n",
    "\\]\n",
    "where $\\eta \\sim N(0, I)$ and $L$ is the Cholesky factor of $\\Sigma $ satisfying $LL^T = \\Sigma$\n",
    "\n",
    "The ELBO can now be written as \n",
    "\\[\n",
    "L = \\int \\phi (\\eta) \\log \\frac{p(L \\eta + \\mu, D)|L|}{\\phi(\\eta)} d \\eta\n",
    "\\]where $\\phi(\\eta)$ is the density of $\\eta$ and $|L|$ comes from the coordinate transformation\n",
    "\n",
    "We can thus write\n",
    "\\[\n",
    "L = E_{\\phi (\\eta)} [\\log p(L \\eta + \\mu, D)] + \\log |C| + E_{\\phi (\\eta)}[- \\log \\phi (\\eta)]\n",
    "\\]\n",
    "Gradients\\[\n",
    "\\nabla_{\\mu} L = E_{\\phi (\\eta)} [\\nabla_{\\theta} \\log p(\\theta, D)]\n",
    "\\]\n",
    "\\[\n",
    "\\nabla_{L} L = E_{\\phi (\\eta)} [\\nabla_{theta} \\log p(\\theta, D)]\\times \\eta^T + \\Delta_L\n",
    "\\]\n",
    "where $\\Delta_L = \\textrm{diag}(1/l_{11}, \\dots, 1/l_{dd})$\n",
    "Expectations with Monte Carlo, gradients with automatic differentiation\n",
    "\n",
    "\\subsection{Stochastic optimisation for variational inference}\n",
    "We can maximise $L$ with stochastic gradient ascent\n",
    "\\[\n",
    "\\mu_{t+1} = \\mu_t + p_t \\tilde{\\nabla}_{\\mu}L(\\mu_t, L_t)\n",
    "\\]\n",
    "\\[\n",
    "L_{t+1} = L_t + p_t \\tilde{\\nabla}_{L}L(\\mu_t, L_t)\n",
    "\\]\n",
    "\n",
    "\\subsection{Example}\n",
    "The target joint distribution:\n",
    "\\[\n",
    "\\log p(D, \\mu) = \\sum_{i =1}^n \\log N(x_i ; \\mu, \\sigma_x^2) + \\log N(\\mu; \\mu_0, \\sigma_0^2)\n",
    "\\]We use the normal approximation $q(\\mu) = N(\\mu'; \\hat{\\mu}, \\tilde{\\mu}^2)$ which can be reparametrised using $\\mu = \\hat{\\mu} + \\tilde{\\mu} \\eta_{\\mu}$, $\\eta_{\\mu} \\sim N(0,1)$\n",
    "\n",
    "\\subsubsection{Autograd}\n",
    "\\begin{minted}{Python}\n",
    "import autograd.numpy as np\n",
    "import autograd\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "# Define the normal pdf. Note parameters: mean, standard deviation\n",
    "def lnormpdf(x, mu, sigma):\n",
    "    return -0.5*np.log(2*np.pi) - np.log(sigma) - 0.5 * (x-mu)**2/sigma**2\n",
    " \n",
    "# Doubly stochastic variational inference\n",
    "# Algorithm 1 of Titsias and Lázaro-Gredilla (2014)\n",
    "def dsvi(m0, c0, gradient, sample_eta, rho0, t0 = 100, niters = 10000):\n",
    "    \"\"\"Doubly stochastic variational inference from\n",
    "    Algorithm 1 of Titsias and Lázaro-Gredilla (2014)\n",
    "    Arguments:\n",
    "    m0: initial value of mean (scalar)\n",
    "    c0: initial value of standard deviation (scalar)\n",
    "    logjoint: function returning the value of the log-joint distribution p(X, theta)\n",
    "    sample_eta: function sampling fixed parameters eta\n",
    "    rho0: initial learning rate the rho_t = rho0 / (t0 + t)\n",
    "    t0: t0 for the above (default: 100)\n",
    "    niters: number of iterations (default: 10000)\"\"\"\n",
    "    m = m0\n",
    "    c = c0\n",
    "    mhist = np.zeros(niters)\n",
    "    chist = np.zeros(niters)\n",
    "    for t in range(niters):\n",
    "        eta = sample_eta()\n",
    "        theta = c * eta + m\n",
    "        g = gradient(theta)\n",
    "        m = m + rho0 / (t0 + t) * g\n",
    "        c = c + rho0 / (t0 + t) * (g * eta + 1/c)\n",
    "        mhist[t] = m\n",
    "        chist[t] = c\n",
    "    return m, c, mhist, chist\n",
    " \n",
    "# Simulate n=100 points of normally distributed data about mu=0.5\n",
    "n = 100\n",
    "data = 0.5 + npr.normal(size=n)\n",
    " \n",
    "# Set the prior parameters\n",
    "sigma_x = 1.0\n",
    "mu0 = 0.0\n",
    "sigma0 = 3.0\n",
    " \n",
    "# Define the target log-pdf as a sum of likelihood and prior terms\n",
    "def logjoint(mu, data=data, sigma_x=sigma_x, mu0=mu0, sigma0=sigma0):\n",
    "    return lnormpdf(data, mu, sigma_x).sum() + lnormpdf(mu, mu0, sigma0)\n",
    " \n",
    "m, c, mhist, chist = dsvi(1.0, 1.0, autograd.grad(logjoint), npr.normal, 0.1)\n",
    "print(m, c)\n",
    "\n",
    "plt.plot(mhist)\n",
    "plt.plot(chist)\n",
    "plt.show()\n",
    "\\end{minted}\n",
    "\n",
    "\\begin{minted}[breaklines]{Python}\n",
    "tt = np.linspace(0.1, 0.9, 50)\n",
    "m_post = sigma0**2 * np.sum(data) / (n*sigma0**2 + sigma_x**2)\n",
    "s2_post = 1/(n/sigma_x**2 + 1/sigma0**2)\n",
    "y = np.exp(lnormpdf(tt, m_post, np.sqrt(s2_post)))\n",
    "plt.plot(tt, y, label='exact')\n",
    "# Note: c is +/- sqrt(variance), need abs() to get standard deviation!\n",
    "y_vi = np.exp(lnormpdf(tt, m, np.abs(c)))\n",
    "plt.plot(tt, y_vi, label='variational approximation')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\\end{minted}\n",
    "\n",
    "\\subsubsection{PyTorch}\n",
    "\n",
    "\\begin{minted}{Python}\n",
    "import torch\n",
    "import math\n",
    " \n",
    "dtype = torch.double\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    " \n",
    "# Define the normal pdf. Note parameters: mean, standard deviation\n",
    "def lnormpdf(x, mu, sigma):\n",
    "    return -0.5*math.log(2*math.pi) - torch.log(sigma) - 0.5 * (x-mu)**2/sigma**2\n",
    " \n",
    "def dsvi(m0, c0, logjoint, sample_eta, rho0, t0 = 100, niters = 10000):\n",
    "    \"\"\"Doubly stochastic variational inference from\n",
    "    Algorithm 1 of Titsias and Lázaro-Gredilla (2014)\n",
    "    Arguments:\n",
    "    m0: initial value of mean (tensor of length 1)\n",
    "    c0: initial value of standard deviation (tensor of length 1)\n",
    "    logjoint: function returning the value of the log-joint distribution p(X, theta)\n",
    "    sample_eta: function sampling fixed parameters eta\n",
    "    rho0: initial learning rate the rho_t = rho0 / (t0 + t)\n",
    "    t0: t0 for the above (default: 100)\n",
    "    niters: number of iterations (default: 10000)\"\"\"\n",
    "    m = m0\n",
    "    c = c0\n",
    "    mhist = torch.zeros(niters, device=device, dtype=dtype)\n",
    "    chist = torch.zeros(niters, device=device, dtype=dtype)\n",
    "    for t in range(niters):\n",
    "        eta = sample_eta()\n",
    "        theta = torch.tensor(c * eta + m, requires_grad=True,\n",
    "                             device=device, dtype=dtype)\n",
    "        v = logjoint(theta)\n",
    "        v.backward()\n",
    "        g = theta.grad\n",
    "        m = m + rho0 / (t0 + t) * g\n",
    "        c = c + rho0 / (t0 + t) * (g * eta + 1/c)\n",
    "        theta.grad.zero_()\n",
    "        mhist[t] = m\n",
    "        chist[t] = c\n",
    "    return m, c, mhist, chist\n",
    "# Set the seed for PyTorch RNGs\n",
    "torch.manual_seed(42)\n",
    "# Simulate n=100 points of normally distributed data about mu=0.5\n",
    "n = 100\n",
    "data = 0.5 + torch.randn(n, device=device, dtype=dtype)\n",
    " \n",
    "# Set the prior parameters\n",
    "sigma_x = torch.tensor(1.0, device=device, dtype=dtype)\n",
    "mu0 = torch.tensor(0.0, device=device, dtype=dtype)\n",
    "sigma0 = torch.tensor(3.0, device=device, dtype=dtype)\n",
    " \n",
    "# Define the target log-pdf as a sum of likelihood and prior terms\n",
    "def logjoint(mu, data=data, sigma_x=sigma_x, mu0=mu0, sigma0=sigma0):\n",
    "    return lnormpdf(data, mu, sigma_x).sum() + lnormpdf(mu, mu0, sigma0)\n",
    " \n",
    "m, c, mhist, chist = dsvi(torch.tensor(1.0, device=device, dtype=dtype),\n",
    "                          torch.tensor(1.0, device=device, dtype=dtype),\n",
    "                          logjoint,\n",
    "                          lambda: torch.randn(1, device=device, dtype=dtype),\n",
    "                          0.1)\n",
    "print(m.item(), c.item())\n",
    "\n",
    "plt.plot(mhist.numpy())\n",
    "plt.plot(chist.numpy())\n",
    "plt.show()\n",
    "\\end{minted}\n",
    "\n",
    "\\subsection{Exercises}\n",
    "\\subsubsection{Examining the fit of a variational approximation}\n",
    "Target distribution is $p(\\mu) = N(4, 2^2)$ and our approximation $q(\\mu)$ is also normal\n",
    "\n",
    "Parametrise  $\\mu$  as  $μ=c⋅z+m$  and write a function to evaluate the negative Kullback-Leibler (KL) divergence\n",
    "\\[\n",
    "L=−D_{KL}(q(μ)‖p(μ))=E_{q(μ)}[\\log p(μ)−\\log q(μ)]\n",
    "\\]\n",
    "\\[\n",
    "=E_{\\Phi (z)}[\\log p(\\mu)−\\log \\Phi(z)+\\log |c|]\n",
    "\\]\n",
    "Evaluate the expectation by sampling  $z \\sim \\Phi(z)=N(z;0,1) $. Evaluate the KL divergence for a range of values for  $m$  while keeping  $c=2$  fixed and for  c  while keeping  $m=4$  fixed, and plot the resulting values as a function of  m  and as a function of  c  (one line graph for each).\n",
    "\n",
    "\\begin{minted}{Python}\n",
    "%matplotlib inline\n",
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Log of normal\n",
    "def lnormpdf(x, mu, sigma):\n",
    "    return -0.5*np.log(2*np.pi*sigma**2) - 0.5*(x-mu)**2/sigma**2\n",
    "\n",
    "#Kullback-Leibler divergence\n",
    "def kl(m, c, N=1000):\n",
    "    z = npr.normal(size=N)\n",
    "    mu = c*z + m\n",
    "    return np.mean(lnormpdf(mu, 4, 2) - lnormpdf(z, 0, 1) + np.log(np.abs(c)))\n",
    "\n",
    "m = np.linspace(0, 8, 100)\n",
    "y = np.array([kl(mm, 2) for mm in m])\n",
    "plt.plot(m, y)\n",
    "plt.show()\n",
    "c = np.linspace(-1, 5, 100)\n",
    "y = np.array([kl(4, cc) for cc in c])\n",
    "plt.plot(c, y)\n",
    "plt.show()\n",
    "\\end{minted}\n",
    "\n",
    "\\subsubsection{Fitting a variational approximation}\n",
    "Use stochastic gradient optimisation to maximise  $L$  using Algorithm 1 of Titsias and Lázaro-Gredilla:\n",
    "\n",
    "The function gradient(theta) here is assumed to return the gradient $ ∇\\log p(θ)$  (or  $∇ \\log p(θ,X)$  for posterior inference). The  $\\log q(z)$  term can be ignored as it does not depend on  m  and  c .\n",
    "\n",
    "In order for the stochastic gradient algorithm to converge, the step size sequence  $p_t$  should satisfy  $\\sum_{t=1}^{\\infty}p_t=1$ ,  $\\sum_{t=1}^{\\infty}p_t^2 < \\infty$ . A common choice for such a sequence is \n",
    "\\[\n",
    "p_t = \\frac{p_0}{t_0 + t}\n",
    "\\]which depends on parameters  $p_0$  and  $t_0$ that need to be tuned for each problem. You should try different values and plot the iterates $ m$  and  $c$  to see how they behave. Suitable  $p_0$  can vary significantly for different problems, while something like  $t_0=100$  might be a good starting point for many problems.\n",
    "\n",
    "\\begin{minted}{Python}\n",
    "import autograd\n",
    "\n",
    "#doubly stochastic variational inference\n",
    "def dsvi(sample_z, m0, c0, gradient, rho_0, t0 = 100, niters = 10000):\n",
    "    m = m0\n",
    "    c = c0\n",
    "    mhist = np.zeros(niters)\n",
    "    chist = np.zeros(niters)\n",
    "    for t in range(niters):\n",
    "        z = sample_z()\n",
    "        theta = c * z + m\n",
    "        g = gradient(theta)\n",
    "        m = m + rho_0 / (t0 + t) * g\n",
    "        c = c + rho_0 / (t0 + t) * (g * z + 1/c)\n",
    "        mhist[t] = m\n",
    "        chist[t] = c\n",
    "    return m, c, mhist, chist\n",
    "\n",
    "def target(x):\n",
    "    return lnormpdf(x, 4, 2)\n",
    "\n",
    "m, c, mhist, chist = dsvi(npr.normal, 1.0, 1.0, autograd.grad(target), 10)\n",
    "print(m, c)\n",
    "plt.plot(mhist)\n",
    "plt.plot(chist)\n",
    "\\end{minted}\n",
    "\n",
    "\\subsubsection{Variational inference for normal mean estimation from data}\n",
    "Use doubly stochastic variational inference to approximate the posterior for  $μ$  for a normal model  $x_i∼N(μ,σ2)$  over the toy data with prior $p(μ)=N(μ_0,σ^20)$ where  $σ^2=2$,$μ_0=3$,$σ_0^2=10$ \n",
    "\n",
    "Parametrise  $μ$  as  $μ=c⋅z+m$  and write a function to evaluate the ELBO\n",
    "\\[\n",
    "L=E_{q(μ)} [ \\log p(X,μ)−\\log q(μ)]=\n",
    "\\]\n",
    "\\[\n",
    "E_{ϕ(z)}[\\sum_{i=1}^n \\log p(x_i|μ)+\\log p(μ)−\\log \\phi(z)+\\log |c|]\n",
    "\\]\n",
    "by sampling  $z \\sim \\phi(z)=N(z;0,1) $.\n",
    "\n",
    "Compare the result with  the exact posterior\n",
    "$p(\\mu | X, \\sigma^2) = N(\\mu; m_{exact}, v_{exact})$\n",
    "\n",
    "\\begin{minted}[breaklines]{Python}\n",
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "import pandas as pd\n",
    "import scipy.special as scs\n",
    "\n",
    "data = pd.read_csv('http://www.helsinki.fi/~ahonkela/teaching/compstats1/toydata2.txt', sep='\\t', header=None)\n",
    "data = data.values\n",
    "data = np.array(data[:,0])\n",
    "\n",
    "print(np.mean(data), np.std(data))\n",
    "\n",
    "sigma0 = np.sqrt(10)\n",
    "mu0 = 3\n",
    "sigma = np.sqrt(2)\n",
    "n = len(data)\n",
    "\n",
    "def target2(mu):\n",
    "    return np.sum(lnormpdf(data, mu, sigma)) + lnormpdf(mu, mu0, sigma0)\n",
    "\n",
    "m, c, mhist, chist = dsvi(npr.normal, 1.0, 1.0, autograd.grad(target2), 0.5)\n",
    "print(m, c)\n",
    "v_exact = 1/(1/sigma0**2 + n/sigma**2)\n",
    "m_exact = v_exact * (np.sum(data)/sigma**2 + mu0/sigma0**2)\n",
    "print(m_exact, np.sqrt(v_exact))\n",
    "plt.plot(mhist)\n",
    "plt.plot(chist)\n",
    "\\end{minted}\n",
    "\n",
    "\\subsubsection{Variational approximation for a multimodal distribution}\n",
    "Going back to the setting of Problem 1, repeat the fitting of a normal variational approximation  $q(x)$ to a target distribution that is a mixture of two normals:\n",
    "$p(x)=1/2N(x;−R,1)+1/2N(x;R,1)$\n",
    " \n",
    "1. Estimate the optimal variational approximation  $q(x)$  that follows a normal distribution for the above problem. Plot the convergence curve ( m  and  c  values as a function of the number of iterations) to make sure you are converging efficiently.\n",
    "\n",
    "2. Run the estimation for several values of  $R$  in the range  $[0,5]$ . Plot the estimated mean and variance as a function of  $R$ . What do you observe?\n",
    "\n",
    "3. Analyse the phase transition in the results. Plot the target distribution and the approximation for a few cases around the phase transition. Can you explain the transition?\n",
    "\n",
    "\\begin{minted}[breaklines]{Python}\n",
    "import autograd.scipy.misc as scm\n",
    "\n",
    "def target3(x, R):\n",
    "    return scm.logsumexp(np.log(0.5) + np.array([lnormpdf(x, -R, 1), lnormpdf(x, R, 1)]))\n",
    "\n",
    "rvals = np.linspace(0.1, 5, 25)\n",
    "mvals = np.zeros(len(rvals))\n",
    "cvals = np.zeros(len(rvals))\n",
    "for i in range(len(rvals)):\n",
    "    m, c, _, _ = dsvi(npr.normal, 1.0, 1.0, autograd.grad(lambda x: target3(x, rvals[i])), 10)\n",
    "    mvals[i] = m\n",
    "    cvals[i] = c\n",
    "    print(rvals[i], m, c)\n",
    "plt.plot(rvals, mvals)\n",
    "plt.plot(rvals, cvals)\n",
    "\n",
    "i = 24\n",
    "m, c, mv, cv = dsvi(npr.normal, 1.0, 1.0, autograd.grad(lambda x: target3(x, rvals[i])), 10)\n",
    "plt.plot(mv)\n",
    "plt.plot(cv)\n",
    "\n",
    "t = np.linspace(-8, 8, 100)\n",
    "I = [14, 15]\n",
    "for i in I:\n",
    "    target_y = np.exp(np.array([target3(x, rvals[i]) for x in t]))\n",
    "    approx_y = np.exp(lnormpdf(t, mvals[i], np.abs(cvals[i])))\n",
    "    plt.plot(t, target_y, 'r')\n",
    "    plt.plot(t, approx_y, 'b')\n",
    "    plt.show()\n",
    "\\end{minted}\n",
    "\n",
    "\\section{Hamiltonian Monte Carlo HMC}\n",
    "Using gradient information of the target function can help improve MCMC convergence. HMC is especially useful in high dimensions where a simple random walk does not converge easily. \n",
    "\n",
    "\\subsection{Metropolis-adjusted Langevin algorithm MALA}\n",
    "Metropolis-adjusted Langevin algorithm stimulates a reversible Langevin diffusion whose stationary distribution is the target $\\pi(\\theta)$:\n",
    "\\[\n",
    "d \\theta_t = \\sigma d B_t + \\frac{\\sigma^2}{2} \\nabla \\log \\pi (\\theta_t) dt\n",
    "\\]This expression is a stochastic differential equation, where $B_t$ is the standard $d$-dimensional Brownian motion\n",
    "\n",
    "Time discretisation:\n",
    "\\[\n",
    "\\theta'_{t+1} = \\theta_t + \\sigma_n z_{t+1} + \\frac{\\sigma_n^2}{2} \\nabla \\log \\pi (\\theta_t)\n",
    "\\]\n",
    "where $z_{t+1} \\sim N(0, I_d)$\n",
    "\n",
    "The proposal is accepted as usual with probability\n",
    "\\[\n",
    "\\alpha (\\theta_t, \\theta'_{t+1} ) = \\frac{\\pi(\\theta'_{t+1})}{\\pi(\\theta_t)} \\frac{q(\\theta_t; \\theta'_{t+1})}{q(\\theta'_{t+1}; \\theta_t)}\n",
    "\\]\n",
    "where \n",
    "\\[\n",
    "q(\\theta'_{t+1}; \\theta_t) = N \\Bigg(\\theta'_{t+1}; \\theta_t + \\frac{\\sigma_n^2}{2} \\nabla \\log \\pi (\\theta_t), \\sigma_n^2 I_d \\Bigg)\n",
    "\\]The MALA algorithm itself is not very widely used in practice\n",
    "\n",
    "\\subsection{Hamiltonian Monte Carlo HMC}\n",
    "Currently the most efficient general purpose samplers. Defining a dynamical system where $-\\log \\pi (\\theta)$ represents the potential energy of the system which is then simulated using Hamiltonian dynamics.\n",
    "\n",
    "For HMC we augment each regular position variable $\\theta_i$ with a corresponding momentum variable $r_i \\sim N(0,1)$. Denoting the potential energy defined by the target as $U(\\theta) = - \\log \\pi (\\theta)$, the momentum variables have a corresponding kinetic energy $K(r) = \\sum_{i =1}^d \\frac{1}{2}r_i^2$with total energy given by $H(\\theta, r) = U(\\theta) + K(r)$\n",
    "\n",
    "Hamiltonian dynamics of the system\n",
    "\\[\n",
    "\\frac{d\\theta_i}{dt} = \\frac{\\partial H(\\theta, r)}{\\partial r_i} = r_i\n",
    "\\]\n",
    "\\[\n",
    "\\frac{dr_i}{dt} = - \\frac{\\partial H(\\theta, r)}{\\partial \\theta_i} = \\frac{\\partial \\log \\pi (\\theta)}{\\partial \\theta_i}\n",
    "\\]\\subsection{Numerical solution of Hamilton's dynamics}\n",
    "Leapfrog integrator:\n",
    "\\[\n",
    "r(t + \\epsilon / 2) = r(t) - (\\epsilon/2)\\nabla_{\\theta}U(\\theta(t))\n",
    "\\]\n",
    "\\[\n",
    "\\theta(t + \\epsilon ) = \\theta(t) - \\epsilon r(t + \\epsilon/2)\n",
    "\\]\n",
    "\\[\n",
    "r(t + \\epsilon) = r(t + \\epsilon/2) - (\\epsilon/2)\\nabla_{\\theta}U(\\theta(t + \\epsilon))\n",
    "\\]\\subsection{HMC algorithm}\n",
    "Each step of the algorithm starts with simulating a normally distributed random momentum $r$. Given this momentum, a trajectory of $L$ leapfrog steps is simulated to obtain a new proposal pair $( \\theta', r')$. The proposal pair is then either accepted or rejected according to MH acceptance rule in the augmented space of $(\\theta, r)$. \n",
    "\\[\n",
    "\\log a = H(\\theta, r) - H(\\theta', r')\n",
    "\\]\n",
    " \\subsection{Tuning HMC}\n",
    "HMC has two tunable parameters: $\\epsilon$ and $L$\n",
    "\n",
    "The effect of increasing $\\epsilon$ can be quite dramatic. Acceptance rate of about 0.8 is in general a good target. \n",
    "\n",
    "\\subsection{HMC theory}\n",
    "Ultimately, the HMC sampler is just a Metropolis-Hastings sampler with a very complex proposal. HMC does not solve the problem of multiple modes! \n",
    "\n",
    "\\subsection{Examples}\n",
    "\\[\n",
    "\\log \\pi(\\theta) = -20(||\\theta||_2 - 10)^2\n",
    "\\]\n",
    "\\begin{minted}[breaklines]{Python}\n",
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "import autograd\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "def hmc(theta0, M, target, epsilon, L):\n",
    "    thetas = np.zeros([M, len(theta0)])\n",
    "    gradF = autograd.grad(target)\n",
    "    theta = np.copy(theta0)\n",
    "    g = gradF(theta)  # set gradient using initial theta\n",
    "    logP = target(theta)  # set objective function too\n",
    "    accepts = 0\n",
    "    for m in range(M): # draw M samples\n",
    "        p = npr.normal(size=theta.shape)  # initial momentum is Normal(0,1)\n",
    "        H = p.T @ p / 2 - logP   # evaluate H(theta,p)\n",
    "        thetanew = np.copy(theta)\n",
    "        gnew = np.copy(g)\n",
    "        for l in range(L): # make L 'leapfrog' steps\n",
    "            p = p + epsilon * gnew / 2   # make half-step in p\n",
    "            thetanew = thetanew + epsilon * p    # make step in theta\n",
    "            gnew = gradF(thetanew)           # find new gradient\n",
    "            p = p + epsilon * gnew / 2   # make half-step in p\n",
    "        logPnew = target(thetanew)   # find new value of H\n",
    "        Hnew = p.T @ p / 2 - logPnew\n",
    "        dH = H - Hnew    # Decide whether to accept\n",
    "        if np.log(npr.rand()) < dH:\n",
    "            g = gnew\n",
    "            theta = thetanew\n",
    "            logP = logPnew\n",
    "            accepts += 1\n",
    "        thetas[m,:] = theta\n",
    "    print('Acceptance rate:', accepts/M)\n",
    "    return thetas\n",
    " \n",
    "def sphere(theta):\n",
    "    return -20*(np.sqrt(np.sum(theta**2))-10)**2\n",
    " \n",
    "samples = hmc(np.array([3.0, 0.0]), 200, lambda theta: sphere(theta), 0.2, 50)\n",
    "\n",
    "samples = samples[samples.shape[0]//2:]\n",
    "plt.plot(samples[:,0], samples[:,1])\n",
    "plt.show()\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "def sphere(theta):\n",
    "    return -20*(torch.sqrt(torch.sum(theta**2))-10)**2\n",
    " \n",
    "def hmc(theta0, M, target, epsilon, L):\n",
    "    dtype = torch.double\n",
    "    device = theta0.device\n",
    "    thetas = torch.zeros([M, len(theta0)], device=device, dtype=dtype)\n",
    "    theta = torch.tensor(theta0, requires_grad=True, device=device, dtype=dtype)\n",
    "    logP = target(theta)  # set objective function too\n",
    "    logP.backward()\n",
    "    g = theta.grad\n",
    "    accepts = 0\n",
    "    for m in range(M): # draw M samples\n",
    "        p = torch.normal(torch.zeros(theta.shape, device=device,\n",
    "                                     dtype=dtype))  # initial momentum is Normal(0,1)\n",
    "        H = p.dot(p) / 2 - logP   # evaluate H(theta,p)\n",
    "        thetanew = torch.tensor(theta.data, requires_grad=True,\n",
    "                            device=device, dtype=dtype)\n",
    "        gnew = torch.tensor(g, device=device, dtype=dtype)\n",
    "        for l in range(L): # make L 'leapfrog' steps\n",
    "            p.data += epsilon * gnew.data / 2   # make half-step in p\n",
    "            thetanew.data += epsilon * p.data          # make step in theta\n",
    "            if thetanew.grad is not None:\n",
    "                thetanew.grad.zero_()\n",
    "            logPnew = target(thetanew)\n",
    "            logPnew.backward()\n",
    "            gnew = thetanew.grad             # find new gradient\n",
    "            p.data += epsilon * gnew.data / 2   # make half-step in p\n",
    "        logPnew = target(thetanew)   # find new value of H\n",
    "        Hnew = p.dot(p) / 2 - logPnew\n",
    "        dH = H - Hnew    # Decide whether to accept\n",
    "        if torch.log(torch.rand(1, dtype=dtype, device=device)) < dH:\n",
    "            g = gnew\n",
    "            theta = thetanew\n",
    "            logP = logPnew\n",
    "            accepts += 1\n",
    "        thetas[m,:] = theta.detach()\n",
    "    print('Acceptance rate:', accepts/M)\n",
    "    return thetas\n",
    " \n",
    "# Set the seed for PyTorch RNGs\n",
    "torch.manual_seed(42)\n",
    "samples = hmc(torch.tensor([3.0, 0.0], device='cpu', dtype=torch.double),\n",
    "              200, lambda theta: sphere(theta), 0.2, 50)\n",
    "              \n",
    "samples = samples[samples.shape[0]//2:]\n",
    "plt.plot(samples.numpy()[:,0], samples.numpy()[:,1])\n",
    "plt.show()  \n",
    "\n",
    "\\end{minted}\n",
    "\n",
    "\\subsection{Exercises}\n",
    "We will consider the following examples as test cases:\n",
    "\n",
    "1. Circle: $\\log p_1(x) = -20*(||x||_2 - 10)^2$\n",
    "\n",
    "2. Correlated 2-d normal: $p_2(x) = N(x; 0, \\Sigma_2)$, where $\\Sigma_2 = \\begin {bmatrix} 1 & p \\\\ p & 1 \\end{bmatrix}$with $p=0.998$\n",
    "\n",
    "3. 20-d normal: $p_3(x) = N(x; 0, \\Sigma_3)$, where $\\Sigma_3 = diag(0.05^2, 0.1, \\dots, 0.95^2, 1.00^2)$\n",
    "\n",
    "\\subsubsection{MCMC videos}\n",
    "\\begin{minted}[breaklines]{Python}\n",
    "# Hint: many latter examples will use autograd so we will use autograd.numpy libraries throughout\n",
    "%matplotlib inline\n",
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Multidimensional MH sampler\n",
    "def d_mhsample(x0, n, target, proposal, drawproposal, return_accrate=False):\n",
    "    x = x0\n",
    "    d = len(x0)\n",
    "    accepts = 0\n",
    "    lp = target(x)\n",
    "    xs = np.zeros([n, d])\n",
    "    for i in range(n):\n",
    "        x_prop = drawproposal(x)\n",
    "        l_prop = target(x_prop)\n",
    "        a = l_prop - lp + proposal(x_prop, x) - proposal(x, x_prop)\n",
    "        if  0<a or npr.rand() < np.exp(a):\n",
    "            x = x_prop\n",
    "            lp = l_prop\n",
    "            accepts += 1\n",
    "        xs[i] = x\n",
    "    if return_accrate:\n",
    "        return accepts/n\n",
    "    else:\n",
    "        print('Acceptance rate:', accepts/n)\n",
    "        return xs[len(xs)//2:]\n",
    "        \n",
    " import autograd.numpy.linalg as npl\n",
    "\n",
    "def circle(x):\n",
    "    return -20*(np.sqrt(x[0]**2+x[1]**2)-10)**2\n",
    "\n",
    "def correlated_normal(x, rho=0.998):\n",
    "    Sigma = np.array([[1.0, rho],[rho, 1.0]])\n",
    "    return -0.5*(x.T @ npl.solve(Sigma, x))\n",
    "\n",
    "def big_normal(x):\n",
    "    return -0.5*np.sum(x**2 / (np.arange(1, 21)/20)**2)\n",
    "\n",
    "samples1 = d_mhsample(np.zeros(2), 30000, circle, lambda x,y: 0, lambda x: x+npr.normal(size=2))\n",
    "plt.plot(samples1[:,0], samples1[:,1], '.')\n",
    "print(np.mean(samples1, 0), np.std(samples1, 0))\n",
    "\n",
    "samples2 = d_mhsample(np.zeros(2), 30000, correlated_normal, lambda x,y: 0, lambda x: x+0.2*npr.normal(size=2))\n",
    "plt.plot(samples2[:,0], samples2[:,1], '.')\n",
    "print(np.mean(samples2, 0), np.std(samples2, 0))\n",
    "\n",
    "samples3 = d_mhsample(np.zeros(20), 50000, big_normal, lambda x,y: 0, lambda x: x+0.1*npr.normal(size=20))\n",
    "#plt.plot(samples3[:,0], samples3[:,1], '.')\n",
    "plt.plot(np.arange(1, 21)/20, np.mean(samples3, 0))\n",
    "plt.plot(np.arange(1, 21)/20, np.std(samples3, 0))\n",
    "plt.xlabel('True standard deviation')\n",
    "plt.ylabel('Estimated mean')\n",
    "#print(np.mean(samples3, 0), np.std(samples3, 0))\n",
    "\n",
    "# Handy trick for plotting the samples of the last example in parallel\n",
    "plt.plot(samples3[::10] + 2*np.arange(20))\n",
    "\n",
    "from IPython.display import HTML\n",
    "from matplotlib import animation, rc\n",
    "\n",
    "samples = samples1\n",
    "\n",
    "# First set up the figure, the axis, and the plot element we want to animate\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_xlim((-12, 12))\n",
    "ax.set_ylim((-12, 12))\n",
    "\n",
    "x = np.linspace(-12.0, 12.0, 100)\n",
    "y = np.linspace(-12.0, 12.0, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "Z = np.zeros((len(x), len(y)))\n",
    "# Need to use a for loop here because f() does not support vectorised evaluation\n",
    "for i, x_i in enumerate(x):\n",
    "    for j, y_j in enumerate(y):\n",
    "        Z[i,j] = circle(np.array([x_i,y_j]))\n",
    "\n",
    "plt.contour(X, Y, np.log(np.abs(Z.T)))\n",
    "pts, = ax.plot([], [], 'r', lw=2)\n",
    "\n",
    "# initialization function: plot the background of each frame\n",
    "def init():\n",
    "    pts.set_data([], [])\n",
    "    return (pts,)\n",
    "\n",
    "# animation function. This is called sequentially\n",
    "def animate(i):\n",
    "    pts.set_data(samples[np.max([0, 10*i-500]):10*i:5,0], samples[np.max([0, 10*i-500]):10*i:5,1])\n",
    "    return (pts,)\n",
    "\n",
    "# call the animator. blit=True means only re-draw the parts that have changed.\n",
    "anim = animation.FuncAnimation(fig, animate, init_func=init,\n",
    "                               frames=1000, interval=20, blit=True)\n",
    "\n",
    "HTML(anim.to_html5_video())\n",
    "\\end{minted}\n",
    "\n",
    "\\subsubsection{HMC sampling}\n",
    "\\begin{minted}[breaklines]{Python}\n",
    "%matplotlib inline\n",
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "import autograd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#HMC sampler\n",
    "def hmc(theta0, M, target, epsilon0, L):\n",
    "    thetas = np.zeros([M, len(theta0)])\n",
    "    gradF = autograd.grad(target)\n",
    "    theta = np.copy(theta0)\n",
    "    g = gradF(theta)  # set gradient using initial theta\n",
    "    logP = target(theta)  # set objective function too\n",
    "    accepts = 0\n",
    "    for m in range(2*M): # draw M samples after M warm-up iterations\n",
    "        p = npr.normal(size=theta.shape)  # initial momentum is Normal(0,1)\n",
    "        H = p.T @ p / 2 - logP   # evaluate H(x,p)\n",
    "        thetanew = np.copy(theta)\n",
    "        gnew = np.copy(g)\n",
    "        for l in range(L): # make L 'leapfrog' steps\n",
    "            epsilon = npr.uniform(0.8, 1.2) * epsilon0  # optional: randomise epsilon for improved theoretical convergence properties\n",
    "            p = p + epsilon * gnew / 2   # make half-step in p\n",
    "            thetanew = thetanew + epsilon * p    # make step in theta\n",
    "            gnew = gradF(thetanew)           # find new gradient\n",
    "            p = p + epsilon * gnew / 2   # make half-step in p\n",
    "        logPnew = target(thetanew)   # find new value of H\n",
    "        Hnew = p.T @ p / 2 - logPnew\n",
    "        dH = Hnew - H    # Decide whether to accept\n",
    "        if np.log(npr.rand()) < -dH:\n",
    "            g = gnew\n",
    "            theta = thetanew\n",
    "            logP = logPnew\n",
    "            accepts += 1\n",
    "        if m >= M:\n",
    "            thetas[m-M,:] = theta\n",
    "    print('Acceptance rate:', accepts/(2*M))\n",
    "    return thetas\n",
    "    \n",
    "#Circle sample\n",
    "samples1 = hmc(np.array([2.0, 0.0]), 500, lambda theta: circle(theta), 0.2, 10)\n",
    "plt.plot(samples1[:,0], samples1[:,1], '.')\n",
    "print(np.mean(samples1, 0), np.std(samples1, 0))\n",
    "\n",
    "#Correlated normal\n",
    "samples2 = hmc(np.array([3.0, 0.0]), 100, correlated_normal, 0.07, 30)\n",
    "plt.plot(samples2[:,0], samples2[:,1], '.')\n",
    "print(np.mean(samples2, 0), np.std(samples2, 0))\n",
    "\n",
    "samples3 = hmc(np.zeros(20), 2000, big_normal, 0.05, 20)\n",
    "#plt.plot(samples3[:,0], samples3[:,1], '.')\n",
    "#print(np.mean(samples3, 0), np.std(samples3, 0))\n",
    "plt.plot(np.arange(1, 21)/20, np.mean(samples3, 0))\n",
    "plt.plot(np.arange(1, 21)/20, np.std(samples3, 0))\n",
    "plt.xlabel('True variance')\n",
    "plt.ylabel('Estimated mean')\n",
    "\n",
    "plt.plot(samples3 + 2*np.arange(20))\n",
    "\n",
    "from IPython.display import HTML\n",
    "from matplotlib import animation, rc\n",
    "\n",
    "samples = samples1\n",
    "\n",
    "# First set up the figure, the axis, and the plot element we want to animate\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_xlim((-12, 12))\n",
    "ax.set_ylim((-12, 12))\n",
    "\n",
    "x = np.linspace(-12.0, 12.0, 100)\n",
    "y = np.linspace(-12.0, 12.0, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "Z = np.zeros((len(x), len(y)))\n",
    "# Need to use a for loop here because f() does not support vectorised evaluation\n",
    "for i, x_i in enumerate(x):\n",
    "    for j, y_j in enumerate(y):\n",
    "        Z[i,j] = circle(np.array([x_i,y_j]))\n",
    "\n",
    "plt.contour(X, Y, np.log(np.abs(Z.T)))\n",
    "pts, = ax.plot([], [], 'r', lw=2)\n",
    "\n",
    "# initialization function: plot the background of each frame\n",
    "def init():\n",
    "    pts.set_data([], [])\n",
    "    return (pts,)\n",
    "\n",
    "# animation function. This is called sequentially\n",
    "def animate(i):\n",
    "    pts.set_data(samples[np.max([0, i-50]):i,0], samples[np.max([0, i-50]):i,1])\n",
    "    return (pts,)\n",
    "\n",
    "# call the animator. blit=True means only re-draw the parts that have changed.\n",
    "anim = animation.FuncAnimation(fig, animate, init_func=init,\n",
    "                               frames=1000, interval=20, blit=True)\n",
    "\n",
    "HTML(anim.to_html5_video())\n",
    "\\end{minted}\n",
    "\n",
    "\\subsubsection{MALA sampling}\n",
    "\\begin{minted}[breaklines]{Python}\n",
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "\n",
    "def lnormpdf(x, y, sigma):\n",
    "    n = len(x)\n",
    "    return n/2*np.log(2*np.pi*sigma**2) - 0.5/sigma**2*np.sum((x-y)**2)\n",
    "\n",
    "def malasample(theta0, n, target, sigma, return_accrate=False):\n",
    "    dtarget = grad(target)\n",
    "    theta = theta0\n",
    "    d = len(theta0)\n",
    "    accepts = 0\n",
    "    lp = target(theta)\n",
    "    g_theta = dtarget(theta)\n",
    "    thetas = np.zeros([n, d])\n",
    "    for i in range(n):\n",
    "        theta_prop = theta + sigma*npr.normal(size=d) + sigma**2/2 * g_theta\n",
    "        l_prop = target(theta_prop)\n",
    "        g_theta_prop = dtarget(theta_prop)\n",
    "        a = (l_prop - lp \n",
    "             + lnormpdf(theta,      theta_prop + sigma**2/2 * g_theta_prop, sigma)\n",
    "             - lnormpdf(theta_prop, theta      + sigma**2/2 * g_theta,      sigma))\n",
    "        if  np.log(npr.rand()) < a:\n",
    "            theta = theta_prop\n",
    "            lp = l_prop\n",
    "            g_theta = g_theta_prop\n",
    "            accepts += 1\n",
    "        thetas[i] = theta\n",
    "    if return_accrate:\n",
    "        return accepts/n\n",
    "    else:\n",
    "        print('Acceptance rate:', accepts/n)\n",
    "        return thetas[len(thetas)//2:]\n",
    "\n",
    "\n",
    "samples = malasample(np.array([0.0, 6.0]), 30000, circle, 0.3)\n",
    "plt.plot(samples[:,0], samples[:,1], '.')\n",
    "\n",
    "# First set up the figure, the axis, and the plot element we want to animate\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_xlim((-12, 12))\n",
    "ax.set_ylim((-12, 12))\n",
    "\n",
    "x = np.linspace(-12.0, 12.0, 100)\n",
    "y = np.linspace(-12.0, 12.0, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "Z = np.zeros((len(x), len(y)))\n",
    "# Need to use a for loop here because f() does not support vectorised evaluation\n",
    "for i, x_i in enumerate(x):\n",
    "    for j, y_j in enumerate(y):\n",
    "        Z[i,j] = circle(np.array([x_i,y_j]))\n",
    "\n",
    "plt.contour(X, Y, np.log(np.abs(Z.T)))\n",
    "pts, = ax.plot([], [], 'r', lw=2)\n",
    "\n",
    "# initialization function: plot the background of each frame\n",
    "def init():\n",
    "    pts.set_data([], [])\n",
    "    return (pts,)\n",
    "\n",
    "# animation function. This is called sequentially\n",
    "def animate(i):\n",
    "    pts.set_data(samples[np.max([0, 3*i-300]):3*i, 0], samples[np.max([0, 3*i-300]):3*i, 1])\n",
    "    return (pts,)\n",
    "\n",
    "# call the animator. blit=True means only re-draw the parts that have changed.\n",
    "anim = animation.FuncAnimation(fig, animate, init_func=init,\n",
    "                               frames=1000, interval=20, blit=True)\n",
    "\n",
    "HTML(anim.to_html5_video())\n",
    "\n",
    "samples = malasample(np.array([0.0, 10.0]), 10000, circle, 0.3)\n",
    "plt.plot(samples[:,0], samples[:,1], '.')\n",
    "\n",
    "samples = malasample(np.array([0.0, 0.0]), 10000, correlated_normal, 0.13)\n",
    "plt.plot(samples[:,0], samples[:,1], '.')\n",
    "print(np.mean(samples, 0), np.std(samples, 0))\n",
    "\n",
    "samples3 = malasample(np.zeros(20), 50000, big_normal, 0.1)\n",
    "print(samples3.shape)\n",
    "plt.plot(np.arange(1, 21)/20, np.mean(samples3, 0))\n",
    "plt.plot(np.arange(1, 21)/20, np.std(samples3, 0))\n",
    "plt.xlabel('True variance')\n",
    "#plt.ylabel('Estimated mean')\n",
    "\n",
    "plt.plot(samples3[::10] + 2*np.arange(20))\n",
    "\\end{minted}\n",
    "\n",
    "\\section{Dynamic Hamiltonian Monte Carlo and No-U-Turn Sampling (NUTS)}\n",
    "The biggest problem in standard HMC is the choice of the number of steps $L$ for the Hamiltonian system simulation. HMC is an all-or-nothing game. \n",
    "\n",
    "\\subsection{No-U-Turn Sampling (NUTS)}\n",
    "No-U-Turn Sampler (NUTS) was the first proposed dynamic HMC algorithm. NUTS extends the simulated trajectory until it observes a U-turn of the path turning back on itself. \n",
    "\n",
    "\\subsection{NUTS $\\epsilon$ tuning}\n",
    "$\\epsilon$ is tuned by first doubling or halving the value to find an  $ϵ$ that gives Langevin proposal acceptance rate of approximately 0.5.\n",
    "\n",
    "\\subsection{NUTS sampling}\n",
    "The sampling iteration in NUTS differs from HMC in two respects:\n",
    "\\begin{enumerate}\n",
    "\\item The simulated trajectory or path in the Hamiltonian system is extended dynamically both forward and backward until a termination condition designed to detect a U-turn is met\n",
    "\\item The next point is sampled from valid alternatives generated during above extension step instead of a single accept/reject choice.\n",
    "\\end{enumerate}\n",
    "\n",
    "\n",
    "\n",
    "\\subsection{Exercises}\n",
    "\\subsubsection{NUTS}\n",
    "Redo the examplkes from the previous section (HMC)\n",
    "\\begin{minted}[breaklines]{Python}\n",
    "# Hint: many latter examples will use autograd so we will use autograd.numpy libraries throughout\n",
    "%matplotlib inline\n",
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "import matplotlib.pyplot as plt\n",
    "import nuts\n",
    "\n",
    "def d_mhsample(x0, n, target, proposal, drawproposal, return_accrate=False):\n",
    "    x = x0\n",
    "    d = len(x0)\n",
    "    accepts = 0\n",
    "    lp = target(x)\n",
    "    xs = np.zeros([n, d])\n",
    "    for i in range(n):\n",
    "        x_prop = drawproposal(x)\n",
    "        l_prop = target(x_prop)\n",
    "        a = l_prop - lp + proposal(x_prop, x) - proposal(x, x_prop)\n",
    "        if  0<a or npr.rand() < np.exp(a):\n",
    "            x = x_prop\n",
    "            lp = l_prop\n",
    "            accepts += 1\n",
    "        xs[i] = x\n",
    "    if return_accrate:\n",
    "        return accepts/n\n",
    "    else:\n",
    "        print('Acceptance rate:', accepts/n)\n",
    "        return xs[len(xs)//2:]\n",
    "        \n",
    "import autograd.numpy.linalg as npl\n",
    "\n",
    "def circle(x):\n",
    "    return -20*(np.sqrt(x[0]**2+x[1]**2)-10)**2\n",
    "\n",
    "def correlated_normal(x, rho=0.998):\n",
    "    Sigma = np.array([[1.0, rho],[rho, 1.0]])\n",
    "    return -0.5*(x.T @ npl.solve(Sigma, x))\n",
    "\n",
    "def big_normal(x):\n",
    "    return -0.5*np.sum(x**2 / (np.arange(1, 21)/20)**2)\n",
    "    \n",
    "import nuts\n",
    "\n",
    "samples1 = nuts.nuts6(circle, 100, 100, np.array([6.0, 0.0]))\n",
    "plt.plot(samples1[:,0], samples1[:,1], '.')\n",
    "print(np.mean(samples1, 0), np.std(samples1, 0))\n",
    "\n",
    "import nuts\n",
    "\n",
    "samples = nuts.nuts6(correlated_normal, 1000, 100, np.zeros(2))\n",
    "plt.plot(samples[:,0], samples[:,1], '.')\n",
    "print(np.mean(samples, 0), np.std(samples, 0))\n",
    "\n",
    "import nuts\n",
    "\n",
    "samples3 = nuts.nuts6(big_normal, 10000, 1000, np.zeros(20))\n",
    "#plt.plot(samples3[:,0], samples3[:,1], '.')\n",
    "plt.plot(np.arange(1, 21)/20, np.mean(samples3, 0))\n",
    "plt.plot(np.arange(1, 21)/20, np.std(samples3, 0))\n",
    "plt.xlabel('True standard deviation')\n",
    "plt.ylabel('Estimated mean + standard deviation')\n",
    "#print(np.mean(samples3, 0), np.std(samples3, 0))\n",
    "\n",
    "p = plt.plot(samples3[::5] + 2*np.arange(20))\n",
    "\\end{minted}\n",
    "\n",
    "\\subsubsection{HMC sampling for posterior inference in linear regression}\n",
    "Problem 4 from Automatic differentiation\n",
    "\n",
    "\\begin{minted}[breaklines]{Python}\n",
    "import pandas as pd\n",
    "import autograd.numpy as np\n",
    "import autograd\n",
    "from scipy.optimize import minimize\n",
    "import nuts\n",
    "\n",
    "def standardize(x):\n",
    "    return (x - np.mean(x)) / (2*np.std(x))\n",
    "\n",
    "# load the data from CSV file using pandas\n",
    "fram = pd.read_csv('http://www.helsinki.fi/~ahonkela/teaching/compstats1/fram.txt', sep='\\t')\n",
    "# convert the variables of interest to numpy arrays for autograd compatibility\n",
    "# input: Framingham relative weight - the ratio of the subjects weight to the median weight for their sex-height group\n",
    "x = np.array(fram['FRW'])\n",
    "# target: Systolic blood pressure, examination 1\n",
    "y = np.array(fram['SBP'])\n",
    "\n",
    "xs = standardize(x)\n",
    "ys = standardize(y)\n",
    "\n",
    "def linear_regression_logl(coefs, x, y):\n",
    "    return np.sum(-0.5*np.log(2*np.pi)-0.5*(y - coefs[0] - coefs[1] * x)**2)\n",
    "\n",
    "def linear_regression(x, y):\n",
    "    myfun = lambda c: -linear_regression_logl(c, np.array(x), np.array(y))\n",
    "    dmyfun = autograd.grad(myfun)\n",
    "    v = minimize(myfun, np.ones(2), jac=dmyfun)\n",
    "    return v\n",
    "\n",
    "def laplace_regression_logl(coefs, x, y):\n",
    "    return np.sum(-np.log(2)-np.abs(y - coefs[0] - coefs[1] * x))\n",
    "\n",
    "def laplace_regression(x, y):\n",
    "    myfun = lambda c: -laplace_regression_logl(c, np.array(x), np.array(y))\n",
    "    dmyfun = autograd.grad(myfun)\n",
    "    v = minimize(myfun, np.ones(2), jac=dmyfun, method='L-BFGS-B')\n",
    "    return v\n",
    "\n",
    "def normal_logp(x, mu, sigma):\n",
    "    return np.sum(-0.5*np.log(2*np.pi*sigma**2) - 0.5*((x-mu)**2/sigma**2))\n",
    "\n",
    "def norm_linreg_target(theta, x, y):\n",
    "    return linear_regression_logl(theta, xs, ys) + normal_logp(theta[0], 0, 2) + normal_logp(theta[1], 0, 2)\n",
    "\n",
    "def lap_linreg_target(theta, x, y):\n",
    "    return laplace_regression_logl(theta, xs, ys) + normal_logp(theta[0], 0, 2) + normal_logp(theta[1], 0, 2)\n",
    "\n",
    "samples_norm = nuts.nuts6(lambda theta: norm_linreg_target(theta, xs, ys), 1000, 200, np.array([1.0, 0.0]))\n",
    "print(np.mean(samples_norm, 0), np.std(samples_norm, 0))\n",
    "print(linear_regression(xs, ys)['x'])\n",
    "\n",
    "import nuts\n",
    "samples_lap = nuts.nuts6(lambda theta: lap_linreg_target(theta, xs, ys), 1000, 200, np.array([1.0, 0.0]))\n",
    "print(np.mean(samples_lap, 0), np.std(samples_lap, 0))\n",
    "print(laplace_regression(xs, ys)['x'])\n",
    "\\end{minted}\n",
    "\n",
    "\\subsubsection{HMC sampling for posterior inference 2}\n",
    "Apply HMC/NUTS to perform Bayesian inference over the mean $\\mu$ and the standard deviation $\\sigma$ in a normal model\n",
    " \\[\n",
    " p(x_i | \\mu, \\sigma^2) = N(x_i; \\mu, \\sigma^2)\n",
    " \\]\n",
    " \\[\n",
    " p(\\mu) = N(\\mu; \\mu_0, \\sigma_0^2)\n",
    " \\]\n",
    "\\[\n",
    "p(\\sigma^2) = InvGamma(\\alpha, \\beta)\n",
    "\\]\n",
    "\\begin{minted}[breaklines]{Python}\n",
    "%matplotlib inline\n",
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "import pandas as pd\n",
    "import autograd.scipy.special as scs\n",
    "import matplotlib.pyplot as plt\n",
    "import nuts\n",
    "\n",
    "data = pd.read_csv('http://www.helsinki.fi/~ahonkela/teaching/compstats1/toydata2.txt', sep='\\t', header=None)\n",
    "data = data.values\n",
    "data = np.array(data[:,0])\n",
    "\n",
    "def lnorm(x, mu, sigma2):\n",
    "    return np.sum(-0.5*np.log(2*np.pi*sigma2) - 0.5*(x-mu)**2/sigma2)\n",
    "\n",
    "def linvgamma(x, alpha, beta):\n",
    "    return np.sum(alpha*np.log(beta) - scs.gammaln(alpha) - (alpha+1)*np.log(x) - beta/x)\n",
    "\n",
    "def linvgamma_t(y, alpha, beta):\n",
    "    return linvgamma(np.exp(y), alpha, beta) + np.sum(y)\n",
    "\n",
    "def target(theta, mu0, sigma0, alpha, beta):\n",
    "    mu = theta[0]\n",
    "    sigma2 = np.exp(np.min([theta[1], 30]))\n",
    "    return lnorm(data, mu, sigma2) + lnorm(mu, mu0, sigma0) + linvgamma_t(sigma2, alpha, beta)\n",
    "\n",
    "print(np.mean(data), np.var(data))\n",
    "samples = nuts.nuts6(lambda theta: target(theta, 0.0, 100.0, 2.0, 2.0), 1000, 200, np.zeros(2))\n",
    "samples[:,1] = np.exp(samples[:,1])\n",
    "print(np.mean(samples, 0), np.std(samples, 0))\n",
    "\\end{minted}\n",
    "\n",
    "\\section{Likelihood-free inference and approximate Bayesian computation ABC}\n",
    "MCMC and variational inference are general inference methods for models where we can compute the likelihood $p(D|\\theta)$. \n",
    "\n",
    "Some models are computationally intractable!\n",
    "\n",
    "Likelihood-free inference methods are based on simulating new synthetic data sets. The inference is based on comparing the simulated synthetic data sets with the observed data set\n",
    "\n",
    "In all model-based statistical inference, the $\\textbm{likelihood function}$ is of central importance, since it expresses the probability of the observed data under a particular statistical model, and thus quantifies the support data lend to particular values of parameters and to choices among different models. For simple models, an analytical formula for the likelihood function can typically be derived. However, for more complex models, an analytical formula might be elusive or the likelihood function might be computationally very costly to evaluate.\n",
    "\n",
    "\\subsection{Approximate Bayesian computation}\n",
    "The idea of ABC is simple\n",
    "\\begin{enumerate}\n",
    "    \\item Sample $\\theta^*$ from the prior $p(\\theta)$\n",
    "    \\item Simulate a new observed data set $D^*$ conditional to $\\theta^*$: $D^* = \\eta(\\theta^*) ~ p(D|\\theta^*)$\n",
    "    \\item Accept or reject the sample according to criteria specified below (Exact rejection ABC or Summary statistics ABC)\n",
    "\\end{enumerate}\n",
    "\n",
    "\\subsection{Exact rejection ABC}\n",
    "A simulated sample $\\theta^*$ is accepted if and only if $D^* = D$. \n",
    "\n",
    "\\subsection{Summary statistics ABC}\n",
    "ABC can be made more efficient by requiring that only some summary statistics $S(D)$ should match between the generated and the observed data sets. E.g. means and variances of the data. \n",
    "\n",
    "We can accept proposals with \n",
    "\\[\n",
    "|| S(D) - S(D^*) || < \\epsilon\n",
    "\\]\n",
    "for some $\\epsilon > 0$. \n",
    "\n",
    "\\subsection{What is approximate ABC sampling from?}\n",
    "The above approximate procedure of accepting samples with $||D - D^*|| < \\epsilon $ can be generalised to a procedure where the proposed sample $\\theta^*$ is accepted with probability \n",
    "\\[\n",
    "a = \\frac{\\pi_{\\epsilon}(D - D^*)}{c}\n",
    "\\]\n",
    "where $\\pi_{\\epsilon}()$ is a suitable probability density and the constant $c$ is chosen to guarantee that $\\pi_{\\epsilon}(D - D^*)/c$ defines a probability. \n",
    "Applying ABC with this acceptance probability can be shown to be equivalent to sampling from the posterior distribution of the model.\n",
    "\n",
    "\\subsection{Further developments}\n",
    "Methods coupling ABC with some form of MCMC or otherwise learning where to sample the proposals can help.\n",
    "\n",
    "ABC bears clear conceptual similarity with generative adversarial networks (GANs) that are widely used in machine learning with deep neural networks. GAN learning is based on a game where the generator attempts to generate synthetic samples and the discriminator tries to distinguish these. \n",
    "\n",
    "\\subsection{Exercises}\n",
    "\\subsubsection{Exact rejection ABC}\n",
    "Implement an exact rejection ABC sampler to sample from the posterior distribution of the probability $p$ of obtaining heads in a coin flip when observing $r$ heads in $n$ throws:\n",
    "\\[\n",
    "p \\sim \\textrm{Uniform(0,1)}, r \\sim \\textrm{Binom(n,p)}\n",
    "\\]\n",
    "In other words, $\\textrm{Uniform(0,1)}$ is the prior distribution for the parameter $p$.\n",
    "\n",
    "1. Simulate 1000 samples from the posterior when $n = 20$, $r = 10$.\n",
    "\n",
    "\\begin{minted}{Python}\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#N is the number of samples\n",
    "#n throws\n",
    "#r heads\n",
    "\n",
    "def abc_coinflip(N, n, r):\n",
    "    samples = np.zeros(N)\n",
    "    nsamples = 0\n",
    "    while nsamples < N:\n",
    "        #Simulate a probability p    \n",
    "        p = npr.uniform()\n",
    "        #p* is accepted if D*=D\n",
    "        if npr.binomial(n, p) == r:\n",
    "            samples[nsamples] = p\n",
    "            nsamples += 1\n",
    "    return samples\n",
    "\n",
    "x = abc_coinflip(1000, 20, 10)\n",
    "h = plt.hist(x, 30)\n",
    "\\end{minted}\n",
    "\n",
    "2. Simulate 1000 samples from the posterior when $n = 40$, $r = 25$ and estimate the probability $p < 0,5$.\n",
    "\n",
    "\\begin{minted}{Python}\n",
    "x = abc_coinflip(1000, 40, 25)\n",
    "h = plt.hist(x, 30)\n",
    "print(np.mean(x < 0.5))\n",
    "\\end{minted}\n",
    "\n",
    "3. Simulate 1000 samples from the posterior when $n = 80$, $r = 50$ and estimate the probability $p < 0,5$.\n",
    "\n",
    "\\begin{minted}{Python}\n",
    "x = abc_coinflip(1000, 80, 50)\n",
    "h = plt.hist(x, 30)\n",
    "print(np.mean(x < 0.5))\n",
    "\\end{minted}\n",
    "\n",
    "\\subsubsection{ABC for a discrete switching time series}\n",
    "We will consider a Markov model with two states 0 and 1. The model is\n",
    "\\[\n",
    "p(x_{t+1} | x_t) = \\begin{cases}\n",
    "1 - \\pi, & if x_{t+1} = x_t\\\\\n",
    "\\pi, & if x_{t+1} \\neq x_t\n",
    "\\end{cases}\n",
    "\\]\n",
    "i.e. the state will flip with probability $\\pi$. We will assume the chain starts in state $x_0 = 0$.\n",
    "\n",
    "We set a uniform prior $p(\\pi) = Uniform( \\pi; 0,1)$\n",
    "\n",
    "1. Implement an exact ABC sampler that generates full sequences and accepts if the full sequences match. Test your sampler by generating at least 100 samples from the posterior distribution given a single observed sequence \"0011001100\".\n",
    "\n",
    "\\begin{minted}{Python}\n",
    "#N is the number of samples\n",
    "def abc_switching_exact(N, seq):\n",
    "    samples = np.zeros(N)\n",
    "    nsamples = 0\n",
    "    n = len(seq)\n",
    "    while nsamples < N:\n",
    "        p = npr.uniform()\n",
    "        i = 1\n",
    "        accept = True\n",
    "        while i < n and accept:\n",
    "            if seq[i] == seq[i-1]:\n",
    "                if npr.uniform() < p:\n",
    "                    accept = False\n",
    "            else:\n",
    "                if npr.uniform() > p:\n",
    "                    accept = False\n",
    "            i += 1\n",
    "        if accept:\n",
    "            samples[nsamples] = p\n",
    "            nsamples += 1\n",
    "    return samples\n",
    "\n",
    "#x = abc_switching_exact(10000, '01010101010101010101')\n",
    "#h = plt.hist(x, 30)\n",
    "%time x = abc_switching_exact(10000, '0011001100')\n",
    "h = plt.hist(x, 30)\n",
    "print(np.mean(x), np.std(x))\n",
    "\\end{minted}\n",
    "\n",
    "2. Implement an ABC sampler that uses the number of state flips $S(X) = #\\{i | x_i \\neq x_{i+1}\\}$ as the sufficient statistic. Are the two samplers sampling from the same distribution? How long sequences can each of them handle?\n",
    "\n",
    "\\begin{minted}{Python}\n",
    "def abc_switching_summary(N, n, switches):\n",
    "    samples = np.zeros(N)\n",
    "    nsamples = 0\n",
    "    while nsamples < N:\n",
    "        p = npr.uniform()\n",
    "        if npr.binomial(n, p) == switches:\n",
    "            samples[nsamples] = p\n",
    "            nsamples += 1\n",
    "    return samples\n",
    "\n",
    "%time y = abc_switching_summary(10000, 9, 4)\n",
    "h = plt.hist(y, 30)\n",
    "print(np.mean(y), np.std(y))\n",
    "\\end{minted}\n",
    "\n",
    "3. Extend your sampler from subtask 2 by allowing acceptance when $|S(X) - S(X^*)| \\leq \\epsilon$. Test this sampler and the one from subtask 2 for a sequence of length 200 that always flips state after 2 symbols, i.e. \"00110011...0011\". Check how the accuracy of the posterior mean and standard deviation, and the computing time depend on $\\epsilon$.\n",
    "\n",
    "\\begin{minted}{Python}\n",
    "y = abc_switching_summary(10000, 199, 99)\n",
    "h = plt.hist(y, 30)\n",
    "print(np.mean(y), np.std(y))\n",
    "\n",
    "def abc_switching_summary_approx(N, n, switches, epsilon):\n",
    "    samples = np.zeros(N)\n",
    "    nsamples = 0\n",
    "    while nsamples < N:\n",
    "        p = npr.uniform()\n",
    "        if np.abs(npr.binomial(n, p) - switches) <= epsilon:\n",
    "            samples[nsamples] = p\n",
    "            nsamples += 1\n",
    "    return samples\n",
    "\n",
    "z = abc_switching_summary_approx(10000, 199, 99, 3)\n",
    "h = plt.hist(z, 30)\n",
    "print(np.mean(z), np.std(z))\n",
    "\\end{minted}\n",
    "\n",
    "\\subsubsection{Rejection ABC with continuous data}\n",
    "Infer the posterior for $\\mu$ and $\\sigma^2$ for a normal model $x_i ~ N(\\mu, \\sigma^2)$ over the toy data with priors \n",
    "\\[\n",
    "p(\\mu) = N(4, \\sqrt{10}^2), p(\\sigma^2) = Gamma(2,2)\n",
    "\\]\n",
    "We will use the mean and standard deviation of $x_i$ as the summary statistics $S(x)$ and accept samples if $||S(x^*) - S(x)|| < \\epsilon$\n",
    "\n",
    "1. Generate 1000 samples from the prior, evaluate the errors in the summary statistics and plot their distribution\n",
    "\n",
    "\\begin{minted}{Python}\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import pandas as pd\n",
    "\n",
    "def compute_summaries(x):\n",
    "    return np.array([np.mean(x), np.std(x)])\n",
    "\n",
    "data = pd.read_csv('http://www.helsinki.fi/~ahonkela/teaching/compstats1/toydata.txt', sep='\\t', header=None)\n",
    "data = data.values\n",
    "data = np.array(data[:,0])\n",
    "datasummaries = compute_summaries(data)\n",
    "\n",
    "mu0 = 4.0\n",
    "sigma0 = np.sqrt(10)\n",
    "alpha0 = 2.0\n",
    "beta0 = 2.0\n",
    "\n",
    "def normal_abc_errors(N, epsilon = np.inf, Ndata=len(data)):\n",
    "    samples = np.zeros((N, 2))\n",
    "    errors = np.zeros(N)\n",
    "    nsamples = 0\n",
    "    while nsamples < N:\n",
    "        m = npr.normal(mu0, sigma0)\n",
    "        v = npr.gamma(alpha0, 1/beta0)\n",
    "        mydata = npr.normal(m, np.sqrt(v), Ndata)\n",
    "        myerr = np.sqrt(np.sum((datasummaries - compute_summaries(mydata))**2))\n",
    "        if myerr <= epsilon:\n",
    "            samples[nsamples,0] = m\n",
    "            samples[nsamples,1] = v\n",
    "            errors[nsamples] = myerr\n",
    "            nsamples += 1\n",
    "    return samples, errors\n",
    "\n",
    "x, errs = normal_abc_errors(1000)\n",
    "h = plt.hist(errs, 30)\n",
    "\\end{minted}\n",
    "\n",
    "2. Compute the approximate posterior distributions using $\\epsilon = 3$ and $\\epsilon = 1$. Compare their means and standard deviations.\n",
    "\n",
    "\\begin{minted}{Python}\n",
    "x3, errs = normal_abc_errors(1000, 3.0)\n",
    "x = x3\n",
    "h = plt.hist(x[:,0], 30)\n",
    "plt.show()\n",
    "h = plt.hist(x[:,1], 30)\n",
    "\n",
    "x1, errs = normal_abc_errors(1000, 1.0)\n",
    "x = x1\n",
    "h = plt.hist(x[:,0], 30)\n",
    "plt.show()\n",
    "h = plt.hist(x[:,1], 30)\n",
    "\\end{minted}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
