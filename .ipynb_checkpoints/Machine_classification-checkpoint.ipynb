{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\section{Classification}\n",
    "The response variable is qualitative!\n",
    "\n",
    "Logistic regression, linear discriminant analysis, K-nearest neighbors\n",
    "\n",
    "\\subsection{Why not linear regression?}\n",
    "A dummy variable encoding implies an ordering on the outcomes, putting some categories above others. \n",
    "\n",
    "\\subsection{Logistic regression}\n",
    "Logistic regression models the probability that Y belongs to a particular category.\n",
    "\n",
    "\\subsubsection{The logistic model}\n",
    "How should we model the relationship between $p(X) = Pr(Y = 1 | X)$?\n",
    "\n",
    "In logistic regression, we use the logistic function:\n",
    "\\[\n",
    "p(X) = \\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}}\n",
    "\\]\n",
    "The model is fitted with maximum likelihood \n",
    "\n",
    "We find that \n",
    "\\[\n",
    "\\frac{p(X)}{1-p(X)}=e^{\\beta_0 + \\beta_1 X} \n",
    "\\]\n",
    "which is the odds. \n",
    "\n",
    "By taking the logarithm of both sides we arrive at\n",
    "\\[\n",
    "\\log \\Bigg(\\frac{p(X)}{1-p(X)} \\Bigg) = \\beta_0 + \\beta_1 X\n",
    "\\]\n",
    "The logistic regression model has a logit that is linear in X.\n",
    "\n",
    "\\subsubsection{Estimating the regression coefficients}\n",
    "Likelihood function:\n",
    "\\[\n",
    "l(\\beta_0, \\beta_1) = \\prod_{i:y_i = 1}p(x_i) \\prod_{i': y_i' = 0} (1 - p(x_i'))\n",
    "\\]\n",
    "The estimates $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ are chosen to maximize this likelihood function.\n",
    "\n",
    "\\subsubsection{Making predictions}\n",
    "Simply use the formula\n",
    "\\[\n",
    "\\hat{p}(X) = \\frac{e^{\\hat{\\beta}_0 + \\hat{\\beta}_1X}}{1 + e^{\\hat{\\beta}_0+\\hat{\\beta}_1X}}\n",
    "\\]\n",
    "\n",
    "\\subsubsection{Multiple logistic regression}\n",
    "Predicting a binary response using multiple predictors\n",
    "\\[\n",
    "\\log \\Bigg(\\frac{p(X)}{1 - p(X)} \\Bigg) = \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p \n",
    "\\]\n",
    "which can be rewritten as \\[\n",
    "p(X) = \\frac{e^{\\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p}}{1 + e^{\\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p}}\n",
    "\\]\n",
    "\n",
    "\\subsubsection{Logistic regression for >2 response classes}\n",
    "Linear discriminant analysis is popular for the multiple-class setting!\n",
    "\n",
    "\\subsection{Linear discriminant analysis}\n",
    "When the classes are well-separated, the parameter estimates for the logistic regression model are surprisingly unstable. \n",
    "\n",
    "If $n$ is small and the distribution of the predictors $X$ is approximately normal in each of the classes, the linear discriminant model is again more stable than the logistic regression model\n",
    "\n",
    "\\subsubsection{Using Bayes' theorem for classification}\n",
    "Let $\\pi_k$ represent the overall or prior probability that a randomly chosen observation comes from the $k$th class. Let $f_k(x) = Pr(X = x | Y = k)$ denote the density function of $X$ for an observation that comes from the $k$th class. Then Bayes' theorem states that\n",
    "\\[\n",
    "Pr(Y = k|X=x) = \\frac{\\pi_k f_k(x)}{\\sum_{l=1}^K \\pi_l f_l(x)}\n",
    "\\]\n",
    "\n",
    "We know that the Bayes classifier has the lowest possible error rate out of all classifiers. If we can find a way to estimate $f_k(X)$, then we can develop a classifier that approximates the Bayes classifier.\n",
    "\n",
    "\\subsubsection{Linear discriminant analysis for p=1}\n",
    "Suppose we assume that $f_k(x)$ is normal or Gaussian. In the one-dimensional setting, the normal density takes the form\n",
    "\\[\n",
    "f_k(x) = \\frac{1}{\\sqrt{2 \\pi} \\sigma_k} \\exp \\Bigg(- \\frac{1}{2 \\sigma_k^2} (x - \\mu_k)^2 \\Bigg)\n",
    "\\]\n",
    "\n",
    "where $\\mu_k$ and $\\sigma_k^2$ are the mean and variance parameters for the $k$th class. We find that\n",
    "\\[\n",
    "p_k(x) = \\frac{\\pi_k \\frac{1}{\\sqrt{2 \\pi}\\sigma}\\exp \\Bigg(- \\frac{1}{2\\sigma^2}(x - \\mu_k)^2 \\Bigg)}{\\sum_{l=1}^K\\pi_l \\frac{1}{\\sqrt{2 \\pi}\\sigma}\\exp \\Bigg(- \\frac{1}{2\\sigma^2}(x - \\mu_l)^2 \\Bigg)}\n",
    "\\]\n",
    "\n",
    "Taking the log and rearranging the terms, we can show that this is equivalent to assigning the observation to the class for which \n",
    "\\[\n",
    "\\delta_k (x) = x\\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2} + \\log(\\pi_k)\n",
    "\\]\n",
    "is the largest. \n",
    "\n",
    "\\begin{example}\n",
    "$K=2$ and $\\pi_1 = \\pi_2$, then the Bayes classifier assigns an observation to class 1 if $2x(\\mu_1 - \\mu_2) > \\mu_1^2 - \\mu_2^2$ and to class 2 otherwise. In this case, the Bayes decision boundary corresponds to the point where\n",
    "\\[\n",
    "x = \\frac{\\mu_1^2 - \\mu_2^2}{2(\\mu_1 - \\mu_2} = \\frac{\\mu_1 + \\mu_2}{2}\n",
    "\\]\n",
    "\\end{example}\n",
    "\n",
    "The linear discriminant analysis (LDA) method approximates the Bayes classifier by plugging estimates for $\\pi_k$, $\\mu_k$ and $\\sigma^2$. In particular, the following estimates are used:\n",
    "\\[\n",
    "\\hat{\\mu}_k = \\frac{1}{n_k}\\sum_{i:y_i = k}x_i\n",
    "\\]\n",
    "\\[\n",
    "\\hat{\\sigma}^2 = \\frac{1}{n-K}\\sum_{k=1}^K \\sum_{y_i = k}(x_i - \\hat{\\mu}_k)^2\n",
    "\\]\n",
    "where $n$ is the total number of training observations, and $n_k$ is the number or training observations inn the kth class. LDA estimates $\\pi_k$ as \n",
    "\\[\n",
    "\\hat{\\pi}_k = n_k/n\n",
    "\\]\n",
    "LDA assigns an observation $X = x$ to the class for which\n",
    "\\[\n",
    "\\hat{\\delta}_k(x) = x \\frac{\\hat{\\mu}_k}{\\hat{\\sigma}^2} -\n",
    "\\frac{\\hat{\\mu}_k^2}{2 \\hat{\\sigma}^2} + \\log(\\hat{\\pi}_k)\n",
    "\\]\n",
    "is largest. The word linear in the classifier's name stems from the fact that the discriminant functions are linear functions of x.\n",
    "\n",
    "The LDA classifier results from assuming that the observations within each class come from a normal distribution with class-specific mean vector and a common variance $\\sigma^2$.\n",
    "\n",
    "\\subsubsection{Linear discriminant analysis for p > 1}\n",
    "We now extend the LDA classifier to the case of multiple predictors. We will assume that $X = (X_1, X_2, \\dots, X_p)$ is drawn from a multivariate Gaussian (or multivariate normal) distribution, with a class-specific mean vector and a common covariance matrix.  \n",
    "\n",
    "The multivariate Gaussian density is defined as\n",
    "\\[\n",
    "f(x) = \\frac{1}{(2\\pi)^{p/2}|\\mathbf{\\Sigma}|^{1/2}} \\exp \\bigg( -\\frac{1}{2}(x-\\mu)^T \\mathbf{\\Sigma}^{-1}(x - \\mu) \\bigg)\n",
    "\\]\n",
    "Using this we can see that the Bayes classifier assigns an observation $X=x$ to the class for which \n",
    "\\[\n",
    "\\delta_k(x) = x^T \\mathbf{\\Sigma}^{-1}\\mu_k - \\frac{1}{2}\\mu_k^T \\mathbf{\\Sigma}^{-1}\\mu_k + \\log(\\pi_k)\n",
    "\\]\n",
    "is the largest. \n",
    "\n",
    "The Bayes decision boundaries are the set of values $x$ for which $\\delta_k(x) = \\delta_l(x)$, i.e.\n",
    "\\[\n",
    "x^T\\mathbf{\\Sigma}^{-1}\\mu_k - \\frac{1}{2}\\mu_k^T \\mathbf{\\Sigma}^{-1}\\mu_k = x^T \\mathbf{\\Sigma}^{-1}\\mu_l - \\frac{1}{2}\\mu_l^T\\mathbf{\\Sigma}^{-1}\\mu_l\n",
    "\\]\n",
    "for $k \\neq l$ (th term $\\pi_k$ has disappeared as all three classes have same number of training observations). \n",
    "\n",
    "Class-specific performance is also important in medicine and biology. Sensitivity is the percentage of true defaulters that are identified. The specificity is the percentage of non-defaulters that are correctly identified. \n",
    "\n",
    "The Bayes classifier, and by extension LDA, uses a threshold of 50\\% for the posterior probability of default in order to assign an observation to the default class. This threshold can be changed!\n",
    "\n",
    "\\begin{enumerate}\n",
    "    \\item False positive rate: type I error, 1 - specificity\n",
    "    \\item True positive rate: 1 - Type II error, power, sensitivity\n",
    "\\end{enumerate}\n",
    "\n",
    "\\subsubsection{Quadratic discriminant analysis}\n",
    "LDA assumes that the observations within each class are drawn from a multivariate Gaussian distribution with a class-specific mean vecotr and a covariance matrix that is common to all $K$ classes. Quadratic discriminant analysis (QDA) assumes that each class has its own covariance matrix. Under this assumption, the Bayes classifier assigns an observation $X = x$ to the class for which\n",
    "\\[\n",
    "\\delta_k(x) = -\\frac{1}{2}(x - \\mu_k)^T \\mathbf{\\Sigma}_k^{-1}(x - \\mu_k) - \\frac{1}{2}\\log |\\mathbf{\\Sigma}_k| + \\log \\pi_k\n",
    "\\]\n",
    "\\[\n",
    "= -\\frac{1}{2}x^T \\mathbf{\\Sigma}_k^{-1}x + x^T \\mathbf{\\Sigma}_k^{-1}\\mu_k - \n",
    "\\frac{1}{2}\\mu_k^T \\mathbf{\\Sigma}_k^{-1}\\mu_k - \\frac{1}{2}\\log|\\mathbf{\\Sigma}_k| + \\log \\pi_k\n",
    "\\]\n",
    "is the largest. \n",
    "\n",
    "Why would one prefer LDA to QDA, or vice versa? The answer lies in the bias-variance trade-off. LDA is much less flexible classifier than QDA, and so has substantially lower variance. LDA can suffer from high bias. LDA tends to be a better bet than QDA if there are relatively few training observations.\n",
    "\n",
    "\\subsection{A comparison of classification methods}\n",
    "We have discussed the K-nearest neighbors method (KNN), logistic regression, LDA and QDA. \n",
    "\n",
    "Both logistic regression and LDA produce linear decision boundaries. KNN is expected to dominate LDA and logistic regression when the decision boundary is highly non-linear.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
